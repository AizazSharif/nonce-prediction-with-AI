{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EthDataset.csv')\n",
    "data['Previous Block Hash'] = data ['Previous Block Hash'].apply(int, base=16)\n",
    "data['Proof'] = data ['Proof'].apply(int, base=16)\n",
    "\n",
    "#data = data.drop('Target (0000)', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Previous Block Hash</th>\n",
       "      <th>Proof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1146441089199153060029496995276330877187321278...</td>\n",
       "      <td>4746932577408162196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2566412050267010276260271432621664016110204312...</td>\n",
       "      <td>3374703753689577614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8529195098143832201007857337380536610916135773...</td>\n",
       "      <td>10079388590392374760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9692593889131966006159283081848168745989525708...</td>\n",
       "      <td>3579112240188546260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5853639850872662295563687245050799779052506231...</td>\n",
       "      <td>13133519388590604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Previous Block Hash                 Proof\n",
       "0  1146441089199153060029496995276330877187321278...   4746932577408162196\n",
       "1  2566412050267010276260271432621664016110204312...   3374703753689577614\n",
       "2  8529195098143832201007857337380536610916135773...  10079388590392374760\n",
       "3  9692593889131966006159283081848168745989525708...   3579112240188546260\n",
       "4  5853639850872662295563687245050799779052506231...     13133519388590604"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Proof']=(data['Proof']-data['Proof'].mean())/data['Proof'].std()\n",
    "data['Previous Block Hash']=(data['Previous Block Hash']-data['Previous Block Hash'].mean())/data['Previous Block Hash'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Previous Block Hash</th>\n",
       "      <th>Proof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.69725</td>\n",
       "      <td>-0.797755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.952501</td>\n",
       "      <td>-1.056966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.823166</td>\n",
       "      <td>0.209538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.16962</td>\n",
       "      <td>-1.018354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0264079</td>\n",
       "      <td>-1.691961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Previous Block Hash     Proof\n",
       "0             1.69725 -0.797755\n",
       "1           -0.952501 -1.056966\n",
       "2            0.823166  0.209538\n",
       "3             1.16962 -1.018354\n",
       "4           0.0264079 -1.691961"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data.loc[:, ['Previous Block Hash']], data['Proof'], test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 20)\n",
    "        self.fc2 = nn.Linear(20, 25)\n",
    "        self.fc3 = nn.Linear(25, 15)\n",
    "        self.fc4 = nn.Linear(15, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Previous Block Hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.124248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.39534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.58139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.56515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Previous Block Hash\n",
       "0           -0.124248\n",
       "1            -1.39534\n",
       "2            -1.58139\n",
       "3             -0.1811\n",
       "4             1.56515"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "x_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.array_split(x_train, 32)\n",
    "label_batch = np.array_split(y_train, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201,)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(x_train, dtype=np.float32)\n",
    "x.shape\n",
    "y = np.array(y_train, dtype=np.float32)\n",
    "y.shape\n",
    "xtest = np.array(x_test, dtype=np.float32)\n",
    "xtest.shape\n",
    "ytest = np.array(y_test, dtype=np.float32)\n",
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)\n",
    "xtest = torch.Tensor(xtest)\n",
    "ytest = torch.Tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = Data.TensorDataset(xtest, ytest)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch_dataset, \n",
    "                                           batch_size=32, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch_dataset, \n",
    "                                          batch_size=32, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5000..  Training Loss: 0.961..  Test Loss: 0.958.. \n",
      "Epoch: 2/5000..  Training Loss: 0.988..  Test Loss: 0.958.. \n",
      "Epoch: 3/5000..  Training Loss: 0.908..  Test Loss: 0.958.. \n",
      "Epoch: 4/5000..  Training Loss: 0.975..  Test Loss: 0.958.. \n",
      "Epoch: 5/5000..  Training Loss: 0.949..  Test Loss: 0.958.. \n",
      "Epoch: 6/5000..  Training Loss: 0.956..  Test Loss: 0.958.. \n",
      "Epoch: 7/5000..  Training Loss: 0.969..  Test Loss: 0.958.. \n",
      "Epoch: 8/5000..  Training Loss: 0.938..  Test Loss: 0.958.. \n",
      "Epoch: 9/5000..  Training Loss: 0.944..  Test Loss: 0.958.. \n",
      "Epoch: 10/5000..  Training Loss: 1.020..  Test Loss: 0.958.. \n",
      "Epoch: 11/5000..  Training Loss: 0.948..  Test Loss: 0.958.. \n",
      "Epoch: 12/5000..  Training Loss: 0.974..  Test Loss: 0.958.. \n",
      "Epoch: 13/5000..  Training Loss: 0.939..  Test Loss: 0.958.. \n",
      "Epoch: 14/5000..  Training Loss: 0.992..  Test Loss: 0.958.. \n",
      "Epoch: 15/5000..  Training Loss: 0.966..  Test Loss: 0.958.. \n",
      "Epoch: 16/5000..  Training Loss: 0.973..  Test Loss: 0.958.. \n",
      "Epoch: 17/5000..  Training Loss: 0.944..  Test Loss: 0.958.. \n",
      "Epoch: 18/5000..  Training Loss: 0.957..  Test Loss: 0.958.. \n",
      "Epoch: 19/5000..  Training Loss: 0.951..  Test Loss: 0.958.. \n",
      "Epoch: 20/5000..  Training Loss: 0.961..  Test Loss: 0.958.. \n",
      "Epoch: 21/5000..  Training Loss: 0.950..  Test Loss: 0.958.. \n",
      "Epoch: 22/5000..  Training Loss: 0.965..  Test Loss: 0.958.. \n",
      "Epoch: 23/5000..  Training Loss: 0.964..  Test Loss: 0.958.. \n",
      "Epoch: 24/5000..  Training Loss: 0.951..  Test Loss: 0.958.. \n",
      "Epoch: 25/5000..  Training Loss: 0.945..  Test Loss: 0.958.. \n",
      "Epoch: 26/5000..  Training Loss: 0.971..  Test Loss: 0.958.. \n",
      "Epoch: 27/5000..  Training Loss: 0.954..  Test Loss: 0.958.. \n",
      "Epoch: 28/5000..  Training Loss: 0.955..  Test Loss: 0.958.. \n",
      "Epoch: 29/5000..  Training Loss: 0.982..  Test Loss: 0.958.. \n",
      "Epoch: 30/5000..  Training Loss: 0.967..  Test Loss: 0.957.. \n",
      "Epoch: 31/5000..  Training Loss: 0.932..  Test Loss: 0.957.. \n",
      "Epoch: 32/5000..  Training Loss: 0.931..  Test Loss: 0.957.. \n",
      "Epoch: 33/5000..  Training Loss: 0.966..  Test Loss: 0.957.. \n",
      "Epoch: 34/5000..  Training Loss: 0.975..  Test Loss: 0.957.. \n",
      "Epoch: 35/5000..  Training Loss: 0.977..  Test Loss: 0.957.. \n",
      "Epoch: 36/5000..  Training Loss: 0.967..  Test Loss: 0.957.. \n",
      "Epoch: 37/5000..  Training Loss: 0.955..  Test Loss: 0.957.. \n",
      "Epoch: 38/5000..  Training Loss: 0.967..  Test Loss: 0.957.. \n",
      "Epoch: 39/5000..  Training Loss: 0.972..  Test Loss: 0.957.. \n",
      "Epoch: 40/5000..  Training Loss: 0.948..  Test Loss: 0.957.. \n",
      "Epoch: 41/5000..  Training Loss: 0.970..  Test Loss: 0.957.. \n",
      "Epoch: 42/5000..  Training Loss: 0.972..  Test Loss: 0.957.. \n",
      "Epoch: 43/5000..  Training Loss: 0.948..  Test Loss: 0.957.. \n",
      "Epoch: 44/5000..  Training Loss: 0.984..  Test Loss: 0.957.. \n",
      "Epoch: 45/5000..  Training Loss: 0.970..  Test Loss: 0.957.. \n",
      "Epoch: 46/5000..  Training Loss: 0.912..  Test Loss: 0.957.. \n",
      "Epoch: 47/5000..  Training Loss: 1.016..  Test Loss: 0.957.. \n",
      "Epoch: 48/5000..  Training Loss: 0.975..  Test Loss: 0.957.. \n",
      "Epoch: 49/5000..  Training Loss: 0.921..  Test Loss: 0.957.. \n",
      "Epoch: 50/5000..  Training Loss: 0.969..  Test Loss: 0.957.. \n",
      "Epoch: 51/5000..  Training Loss: 0.957..  Test Loss: 0.957.. \n",
      "Epoch: 52/5000..  Training Loss: 0.992..  Test Loss: 0.957.. \n",
      "Epoch: 53/5000..  Training Loss: 0.938..  Test Loss: 0.957.. \n",
      "Epoch: 54/5000..  Training Loss: 0.940..  Test Loss: 0.957.. \n",
      "Epoch: 55/5000..  Training Loss: 0.956..  Test Loss: 0.957.. \n",
      "Epoch: 56/5000..  Training Loss: 0.952..  Test Loss: 0.957.. \n",
      "Epoch: 57/5000..  Training Loss: 0.965..  Test Loss: 0.957.. \n",
      "Epoch: 58/5000..  Training Loss: 0.976..  Test Loss: 0.957.. \n",
      "Epoch: 59/5000..  Training Loss: 0.917..  Test Loss: 0.957.. \n",
      "Epoch: 60/5000..  Training Loss: 0.956..  Test Loss: 0.957.. \n",
      "Epoch: 61/5000..  Training Loss: 0.964..  Test Loss: 0.957.. \n",
      "Epoch: 62/5000..  Training Loss: 0.971..  Test Loss: 0.957.. \n",
      "Epoch: 63/5000..  Training Loss: 0.958..  Test Loss: 0.957.. \n",
      "Epoch: 64/5000..  Training Loss: 0.978..  Test Loss: 0.957.. \n",
      "Epoch: 65/5000..  Training Loss: 0.954..  Test Loss: 0.957.. \n",
      "Epoch: 66/5000..  Training Loss: 0.988..  Test Loss: 0.957.. \n",
      "Epoch: 67/5000..  Training Loss: 0.978..  Test Loss: 0.957.. \n",
      "Epoch: 68/5000..  Training Loss: 0.986..  Test Loss: 0.957.. \n",
      "Epoch: 69/5000..  Training Loss: 0.968..  Test Loss: 0.957.. \n",
      "Epoch: 70/5000..  Training Loss: 0.971..  Test Loss: 0.957.. \n",
      "Epoch: 71/5000..  Training Loss: 0.970..  Test Loss: 0.957.. \n",
      "Epoch: 72/5000..  Training Loss: 0.954..  Test Loss: 0.957.. \n",
      "Epoch: 73/5000..  Training Loss: 0.979..  Test Loss: 0.957.. \n",
      "Epoch: 74/5000..  Training Loss: 0.943..  Test Loss: 0.957.. \n",
      "Epoch: 75/5000..  Training Loss: 0.917..  Test Loss: 0.957.. \n",
      "Epoch: 76/5000..  Training Loss: 0.979..  Test Loss: 0.957.. \n",
      "Epoch: 77/5000..  Training Loss: 0.918..  Test Loss: 0.957.. \n",
      "Epoch: 78/5000..  Training Loss: 0.985..  Test Loss: 0.957.. \n",
      "Epoch: 79/5000..  Training Loss: 0.962..  Test Loss: 0.957.. \n",
      "Epoch: 80/5000..  Training Loss: 0.937..  Test Loss: 0.957.. \n",
      "Epoch: 81/5000..  Training Loss: 0.979..  Test Loss: 0.957.. \n",
      "Epoch: 82/5000..  Training Loss: 0.990..  Test Loss: 0.957.. \n",
      "Epoch: 83/5000..  Training Loss: 0.945..  Test Loss: 0.957.. \n",
      "Epoch: 84/5000..  Training Loss: 0.947..  Test Loss: 0.957.. \n",
      "Epoch: 85/5000..  Training Loss: 0.958..  Test Loss: 0.957.. \n",
      "Epoch: 86/5000..  Training Loss: 0.968..  Test Loss: 0.957.. \n",
      "Epoch: 87/5000..  Training Loss: 0.954..  Test Loss: 0.957.. \n",
      "Epoch: 88/5000..  Training Loss: 0.949..  Test Loss: 0.957.. \n",
      "Epoch: 89/5000..  Training Loss: 0.934..  Test Loss: 0.957.. \n",
      "Epoch: 90/5000..  Training Loss: 0.981..  Test Loss: 0.957.. \n",
      "Epoch: 91/5000..  Training Loss: 0.960..  Test Loss: 0.957.. \n",
      "Epoch: 92/5000..  Training Loss: 0.955..  Test Loss: 0.957.. \n",
      "Epoch: 93/5000..  Training Loss: 0.978..  Test Loss: 0.957.. \n",
      "Epoch: 94/5000..  Training Loss: 0.944..  Test Loss: 0.957.. \n",
      "Epoch: 95/5000..  Training Loss: 0.931..  Test Loss: 0.957.. \n",
      "Epoch: 96/5000..  Training Loss: 0.973..  Test Loss: 0.957.. \n",
      "Epoch: 97/5000..  Training Loss: 0.971..  Test Loss: 0.957.. \n",
      "Epoch: 98/5000..  Training Loss: 0.956..  Test Loss: 0.957.. \n",
      "Epoch: 99/5000..  Training Loss: 1.003..  Test Loss: 0.957.. \n",
      "Epoch: 100/5000..  Training Loss: 0.965..  Test Loss: 0.957.. \n",
      "Epoch: 101/5000..  Training Loss: 0.934..  Test Loss: 0.957.. \n",
      "Epoch: 102/5000..  Training Loss: 0.972..  Test Loss: 0.957.. \n",
      "Epoch: 103/5000..  Training Loss: 0.951..  Test Loss: 0.957.. \n",
      "Epoch: 104/5000..  Training Loss: 0.954..  Test Loss: 0.957.. \n",
      "Epoch: 105/5000..  Training Loss: 0.950..  Test Loss: 0.957.. \n",
      "Epoch: 106/5000..  Training Loss: 0.975..  Test Loss: 0.957.. \n",
      "Epoch: 107/5000..  Training Loss: 0.961..  Test Loss: 0.957.. \n",
      "Epoch: 108/5000..  Training Loss: 0.989..  Test Loss: 0.957.. \n",
      "Epoch: 109/5000..  Training Loss: 0.964..  Test Loss: 0.957.. \n",
      "Epoch: 110/5000..  Training Loss: 0.962..  Test Loss: 0.957.. \n",
      "Epoch: 111/5000..  Training Loss: 0.962..  Test Loss: 0.957.. \n",
      "Epoch: 112/5000..  Training Loss: 0.942..  Test Loss: 0.957.. \n",
      "Epoch: 113/5000..  Training Loss: 0.956..  Test Loss: 0.957.. \n",
      "Epoch: 114/5000..  Training Loss: 0.956..  Test Loss: 0.957.. \n",
      "Epoch: 115/5000..  Training Loss: 0.967..  Test Loss: 0.957.. \n",
      "Epoch: 116/5000..  Training Loss: 0.956..  Test Loss: 0.957.. \n",
      "Epoch: 117/5000..  Training Loss: 0.953..  Test Loss: 0.957.. \n",
      "Epoch: 118/5000..  Training Loss: 0.957..  Test Loss: 0.957.. \n",
      "Epoch: 119/5000..  Training Loss: 0.989..  Test Loss: 0.957.. \n",
      "Epoch: 120/5000..  Training Loss: 0.980..  Test Loss: 0.957.. \n",
      "Epoch: 121/5000..  Training Loss: 0.966..  Test Loss: 0.957.. \n",
      "Epoch: 122/5000..  Training Loss: 0.953..  Test Loss: 0.957.. \n",
      "Epoch: 123/5000..  Training Loss: 0.949..  Test Loss: 0.957.. \n",
      "Epoch: 124/5000..  Training Loss: 0.982..  Test Loss: 0.957.. \n",
      "Epoch: 125/5000..  Training Loss: 0.919..  Test Loss: 0.957.. \n",
      "Epoch: 126/5000..  Training Loss: 0.971..  Test Loss: 0.957.. \n",
      "Epoch: 127/5000..  Training Loss: 0.954..  Test Loss: 0.957.. \n",
      "Epoch: 128/5000..  Training Loss: 0.966..  Test Loss: 0.957.. \n",
      "Epoch: 129/5000..  Training Loss: 0.959..  Test Loss: 0.956.. \n",
      "Epoch: 130/5000..  Training Loss: 0.966..  Test Loss: 0.956.. \n",
      "Epoch: 131/5000..  Training Loss: 0.945..  Test Loss: 0.956.. \n",
      "Epoch: 132/5000..  Training Loss: 0.962..  Test Loss: 0.956.. \n",
      "Epoch: 133/5000..  Training Loss: 0.974..  Test Loss: 0.956.. \n",
      "Epoch: 134/5000..  Training Loss: 0.969..  Test Loss: 0.956.. \n",
      "Epoch: 135/5000..  Training Loss: 0.994..  Test Loss: 0.956.. \n",
      "Epoch: 136/5000..  Training Loss: 0.949..  Test Loss: 0.956.. \n",
      "Epoch: 137/5000..  Training Loss: 0.922..  Test Loss: 0.956.. \n",
      "Epoch: 138/5000..  Training Loss: 0.937..  Test Loss: 0.956.. \n",
      "Epoch: 139/5000..  Training Loss: 0.956..  Test Loss: 0.956.. \n",
      "Epoch: 140/5000..  Training Loss: 0.941..  Test Loss: 0.956.. \n",
      "Epoch: 141/5000..  Training Loss: 0.961..  Test Loss: 0.956.. \n",
      "Epoch: 142/5000..  Training Loss: 0.974..  Test Loss: 0.956.. \n",
      "Epoch: 143/5000..  Training Loss: 0.950..  Test Loss: 0.956.. \n",
      "Epoch: 144/5000..  Training Loss: 0.945..  Test Loss: 0.956.. \n",
      "Epoch: 145/5000..  Training Loss: 0.991..  Test Loss: 0.956.. \n",
      "Epoch: 146/5000..  Training Loss: 0.948..  Test Loss: 0.956.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147/5000..  Training Loss: 0.931..  Test Loss: 0.956.. \n",
      "Epoch: 148/5000..  Training Loss: 0.948..  Test Loss: 0.956.. \n",
      "Epoch: 149/5000..  Training Loss: 0.962..  Test Loss: 0.956.. \n",
      "Epoch: 150/5000..  Training Loss: 0.990..  Test Loss: 0.956.. \n",
      "Epoch: 151/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 152/5000..  Training Loss: 0.957..  Test Loss: 0.956.. \n",
      "Epoch: 153/5000..  Training Loss: 0.912..  Test Loss: 0.956.. \n",
      "Epoch: 154/5000..  Training Loss: 0.965..  Test Loss: 0.956.. \n",
      "Epoch: 155/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 156/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 157/5000..  Training Loss: 0.928..  Test Loss: 0.956.. \n",
      "Epoch: 158/5000..  Training Loss: 0.959..  Test Loss: 0.956.. \n",
      "Epoch: 159/5000..  Training Loss: 0.935..  Test Loss: 0.956.. \n",
      "Epoch: 160/5000..  Training Loss: 0.950..  Test Loss: 0.956.. \n",
      "Epoch: 161/5000..  Training Loss: 0.973..  Test Loss: 0.956.. \n",
      "Epoch: 162/5000..  Training Loss: 0.969..  Test Loss: 0.956.. \n",
      "Epoch: 163/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 164/5000..  Training Loss: 0.945..  Test Loss: 0.956.. \n",
      "Epoch: 165/5000..  Training Loss: 0.950..  Test Loss: 0.956.. \n",
      "Epoch: 166/5000..  Training Loss: 0.944..  Test Loss: 0.956.. \n",
      "Epoch: 167/5000..  Training Loss: 0.949..  Test Loss: 0.956.. \n",
      "Epoch: 168/5000..  Training Loss: 0.958..  Test Loss: 0.956.. \n",
      "Epoch: 169/5000..  Training Loss: 0.940..  Test Loss: 0.956.. \n",
      "Epoch: 170/5000..  Training Loss: 0.944..  Test Loss: 0.956.. \n",
      "Epoch: 171/5000..  Training Loss: 0.965..  Test Loss: 0.956.. \n",
      "Epoch: 172/5000..  Training Loss: 0.936..  Test Loss: 0.956.. \n",
      "Epoch: 173/5000..  Training Loss: 0.944..  Test Loss: 0.956.. \n",
      "Epoch: 174/5000..  Training Loss: 0.988..  Test Loss: 0.956.. \n",
      "Epoch: 175/5000..  Training Loss: 0.974..  Test Loss: 0.956.. \n",
      "Epoch: 176/5000..  Training Loss: 0.996..  Test Loss: 0.956.. \n",
      "Epoch: 177/5000..  Training Loss: 0.951..  Test Loss: 0.956.. \n",
      "Epoch: 178/5000..  Training Loss: 0.965..  Test Loss: 0.956.. \n",
      "Epoch: 179/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 180/5000..  Training Loss: 0.967..  Test Loss: 0.956.. \n",
      "Epoch: 181/5000..  Training Loss: 0.943..  Test Loss: 0.956.. \n",
      "Epoch: 182/5000..  Training Loss: 0.957..  Test Loss: 0.956.. \n",
      "Epoch: 183/5000..  Training Loss: 0.929..  Test Loss: 0.956.. \n",
      "Epoch: 184/5000..  Training Loss: 0.941..  Test Loss: 0.956.. \n",
      "Epoch: 185/5000..  Training Loss: 0.975..  Test Loss: 0.956.. \n",
      "Epoch: 186/5000..  Training Loss: 0.989..  Test Loss: 0.956.. \n",
      "Epoch: 187/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 188/5000..  Training Loss: 0.967..  Test Loss: 0.956.. \n",
      "Epoch: 189/5000..  Training Loss: 0.966..  Test Loss: 0.956.. \n",
      "Epoch: 190/5000..  Training Loss: 0.970..  Test Loss: 0.956.. \n",
      "Epoch: 191/5000..  Training Loss: 0.911..  Test Loss: 0.956.. \n",
      "Epoch: 192/5000..  Training Loss: 0.926..  Test Loss: 0.956.. \n",
      "Epoch: 193/5000..  Training Loss: 0.985..  Test Loss: 0.956.. \n",
      "Epoch: 194/5000..  Training Loss: 0.925..  Test Loss: 0.956.. \n",
      "Epoch: 195/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 196/5000..  Training Loss: 0.947..  Test Loss: 0.956.. \n",
      "Epoch: 197/5000..  Training Loss: 0.963..  Test Loss: 0.956.. \n",
      "Epoch: 198/5000..  Training Loss: 0.954..  Test Loss: 0.956.. \n",
      "Epoch: 199/5000..  Training Loss: 0.952..  Test Loss: 0.956.. \n",
      "Epoch: 200/5000..  Training Loss: 0.962..  Test Loss: 0.956.. \n",
      "Epoch: 201/5000..  Training Loss: 0.958..  Test Loss: 0.956.. \n",
      "Epoch: 202/5000..  Training Loss: 0.956..  Test Loss: 0.956.. \n",
      "Epoch: 203/5000..  Training Loss: 0.971..  Test Loss: 0.956.. \n",
      "Epoch: 204/5000..  Training Loss: 0.931..  Test Loss: 0.956.. \n",
      "Epoch: 205/5000..  Training Loss: 0.969..  Test Loss: 0.956.. \n",
      "Epoch: 206/5000..  Training Loss: 0.947..  Test Loss: 0.956.. \n",
      "Epoch: 207/5000..  Training Loss: 0.963..  Test Loss: 0.956.. \n",
      "Epoch: 208/5000..  Training Loss: 0.985..  Test Loss: 0.956.. \n",
      "Epoch: 209/5000..  Training Loss: 0.951..  Test Loss: 0.956.. \n",
      "Epoch: 210/5000..  Training Loss: 0.967..  Test Loss: 0.956.. \n",
      "Epoch: 211/5000..  Training Loss: 0.969..  Test Loss: 0.956.. \n",
      "Epoch: 212/5000..  Training Loss: 0.953..  Test Loss: 0.956.. \n",
      "Epoch: 213/5000..  Training Loss: 0.961..  Test Loss: 0.956.. \n",
      "Epoch: 214/5000..  Training Loss: 0.946..  Test Loss: 0.956.. \n",
      "Epoch: 215/5000..  Training Loss: 1.003..  Test Loss: 0.956.. \n",
      "Epoch: 216/5000..  Training Loss: 0.956..  Test Loss: 0.956.. \n",
      "Epoch: 217/5000..  Training Loss: 0.959..  Test Loss: 0.956.. \n",
      "Epoch: 218/5000..  Training Loss: 0.924..  Test Loss: 0.956.. \n",
      "Epoch: 219/5000..  Training Loss: 0.954..  Test Loss: 0.956.. \n",
      "Epoch: 220/5000..  Training Loss: 0.964..  Test Loss: 0.956.. \n",
      "Epoch: 221/5000..  Training Loss: 0.955..  Test Loss: 0.956.. \n",
      "Epoch: 222/5000..  Training Loss: 0.956..  Test Loss: 0.956.. \n",
      "Epoch: 223/5000..  Training Loss: 0.981..  Test Loss: 0.956.. \n",
      "Epoch: 224/5000..  Training Loss: 0.942..  Test Loss: 0.956.. \n",
      "Epoch: 225/5000..  Training Loss: 0.944..  Test Loss: 0.956.. \n",
      "Epoch: 226/5000..  Training Loss: 0.969..  Test Loss: 0.956.. \n",
      "Epoch: 227/5000..  Training Loss: 0.951..  Test Loss: 0.956.. \n",
      "Epoch: 228/5000..  Training Loss: 0.982..  Test Loss: 0.955.. \n",
      "Epoch: 229/5000..  Training Loss: 0.974..  Test Loss: 0.955.. \n",
      "Epoch: 230/5000..  Training Loss: 0.936..  Test Loss: 0.955.. \n",
      "Epoch: 231/5000..  Training Loss: 0.968..  Test Loss: 0.955.. \n",
      "Epoch: 232/5000..  Training Loss: 0.946..  Test Loss: 0.955.. \n",
      "Epoch: 233/5000..  Training Loss: 0.972..  Test Loss: 0.955.. \n",
      "Epoch: 234/5000..  Training Loss: 0.942..  Test Loss: 0.955.. \n",
      "Epoch: 235/5000..  Training Loss: 0.999..  Test Loss: 0.955.. \n",
      "Epoch: 236/5000..  Training Loss: 0.974..  Test Loss: 0.955.. \n",
      "Epoch: 237/5000..  Training Loss: 1.001..  Test Loss: 0.955.. \n",
      "Epoch: 238/5000..  Training Loss: 0.964..  Test Loss: 0.955.. \n",
      "Epoch: 239/5000..  Training Loss: 0.940..  Test Loss: 0.955.. \n",
      "Epoch: 240/5000..  Training Loss: 0.949..  Test Loss: 0.955.. \n",
      "Epoch: 241/5000..  Training Loss: 0.921..  Test Loss: 0.955.. \n",
      "Epoch: 242/5000..  Training Loss: 0.998..  Test Loss: 0.955.. \n",
      "Epoch: 243/5000..  Training Loss: 0.957..  Test Loss: 0.955.. \n",
      "Epoch: 244/5000..  Training Loss: 0.970..  Test Loss: 0.955.. \n",
      "Epoch: 245/5000..  Training Loss: 0.959..  Test Loss: 0.955.. \n",
      "Epoch: 246/5000..  Training Loss: 0.945..  Test Loss: 0.955.. \n",
      "Epoch: 247/5000..  Training Loss: 0.961..  Test Loss: 0.955.. \n",
      "Epoch: 248/5000..  Training Loss: 0.979..  Test Loss: 0.955.. \n",
      "Epoch: 249/5000..  Training Loss: 0.948..  Test Loss: 0.955.. \n",
      "Epoch: 250/5000..  Training Loss: 0.957..  Test Loss: 0.955.. \n",
      "Epoch: 251/5000..  Training Loss: 0.950..  Test Loss: 0.955.. \n",
      "Epoch: 252/5000..  Training Loss: 0.930..  Test Loss: 0.955.. \n",
      "Epoch: 253/5000..  Training Loss: 0.941..  Test Loss: 0.955.. \n",
      "Epoch: 254/5000..  Training Loss: 0.979..  Test Loss: 0.955.. \n",
      "Epoch: 255/5000..  Training Loss: 0.937..  Test Loss: 0.955.. \n",
      "Epoch: 256/5000..  Training Loss: 0.962..  Test Loss: 0.955.. \n",
      "Epoch: 257/5000..  Training Loss: 0.967..  Test Loss: 0.955.. \n",
      "Epoch: 258/5000..  Training Loss: 0.965..  Test Loss: 0.955.. \n",
      "Epoch: 259/5000..  Training Loss: 0.958..  Test Loss: 0.955.. \n",
      "Epoch: 260/5000..  Training Loss: 0.938..  Test Loss: 0.955.. \n",
      "Epoch: 261/5000..  Training Loss: 0.973..  Test Loss: 0.955.. \n",
      "Epoch: 262/5000..  Training Loss: 0.938..  Test Loss: 0.955.. \n",
      "Epoch: 263/5000..  Training Loss: 0.978..  Test Loss: 0.955.. \n",
      "Epoch: 264/5000..  Training Loss: 0.983..  Test Loss: 0.955.. \n",
      "Epoch: 265/5000..  Training Loss: 0.977..  Test Loss: 0.955.. \n",
      "Epoch: 266/5000..  Training Loss: 0.940..  Test Loss: 0.955.. \n",
      "Epoch: 267/5000..  Training Loss: 0.940..  Test Loss: 0.955.. \n",
      "Epoch: 268/5000..  Training Loss: 0.957..  Test Loss: 0.955.. \n",
      "Epoch: 269/5000..  Training Loss: 0.924..  Test Loss: 0.955.. \n",
      "Epoch: 270/5000..  Training Loss: 0.958..  Test Loss: 0.955.. \n",
      "Epoch: 271/5000..  Training Loss: 0.964..  Test Loss: 0.955.. \n",
      "Epoch: 272/5000..  Training Loss: 0.964..  Test Loss: 0.955.. \n",
      "Epoch: 273/5000..  Training Loss: 0.945..  Test Loss: 0.955.. \n",
      "Epoch: 274/5000..  Training Loss: 0.947..  Test Loss: 0.955.. \n",
      "Epoch: 275/5000..  Training Loss: 0.943..  Test Loss: 0.955.. \n",
      "Epoch: 276/5000..  Training Loss: 0.965..  Test Loss: 0.955.. \n",
      "Epoch: 277/5000..  Training Loss: 0.908..  Test Loss: 0.955.. \n",
      "Epoch: 278/5000..  Training Loss: 0.953..  Test Loss: 0.955.. \n",
      "Epoch: 279/5000..  Training Loss: 0.969..  Test Loss: 0.955.. \n",
      "Epoch: 280/5000..  Training Loss: 0.951..  Test Loss: 0.955.. \n",
      "Epoch: 281/5000..  Training Loss: 0.906..  Test Loss: 0.955.. \n",
      "Epoch: 282/5000..  Training Loss: 0.958..  Test Loss: 0.955.. \n",
      "Epoch: 283/5000..  Training Loss: 0.946..  Test Loss: 0.955.. \n",
      "Epoch: 284/5000..  Training Loss: 0.937..  Test Loss: 0.955.. \n",
      "Epoch: 285/5000..  Training Loss: 0.938..  Test Loss: 0.955.. \n",
      "Epoch: 286/5000..  Training Loss: 0.937..  Test Loss: 0.955.. \n",
      "Epoch: 287/5000..  Training Loss: 0.946..  Test Loss: 0.955.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288/5000..  Training Loss: 0.947..  Test Loss: 0.955.. \n",
      "Epoch: 289/5000..  Training Loss: 0.960..  Test Loss: 0.955.. \n",
      "Epoch: 290/5000..  Training Loss: 0.959..  Test Loss: 0.955.. \n",
      "Epoch: 291/5000..  Training Loss: 0.986..  Test Loss: 0.955.. \n",
      "Epoch: 292/5000..  Training Loss: 0.980..  Test Loss: 0.955.. \n",
      "Epoch: 293/5000..  Training Loss: 0.952..  Test Loss: 0.955.. \n",
      "Epoch: 294/5000..  Training Loss: 0.967..  Test Loss: 0.955.. \n",
      "Epoch: 295/5000..  Training Loss: 0.946..  Test Loss: 0.955.. \n",
      "Epoch: 296/5000..  Training Loss: 0.946..  Test Loss: 0.955.. \n",
      "Epoch: 297/5000..  Training Loss: 0.927..  Test Loss: 0.955.. \n",
      "Epoch: 298/5000..  Training Loss: 0.975..  Test Loss: 0.955.. \n",
      "Epoch: 299/5000..  Training Loss: 0.990..  Test Loss: 0.955.. \n",
      "Epoch: 300/5000..  Training Loss: 0.955..  Test Loss: 0.955.. \n",
      "Epoch: 301/5000..  Training Loss: 0.939..  Test Loss: 0.955.. \n",
      "Epoch: 302/5000..  Training Loss: 0.927..  Test Loss: 0.955.. \n",
      "Epoch: 303/5000..  Training Loss: 0.944..  Test Loss: 0.955.. \n",
      "Epoch: 304/5000..  Training Loss: 0.956..  Test Loss: 0.955.. \n",
      "Epoch: 305/5000..  Training Loss: 0.978..  Test Loss: 0.955.. \n",
      "Epoch: 306/5000..  Training Loss: 0.955..  Test Loss: 0.955.. \n",
      "Epoch: 307/5000..  Training Loss: 0.922..  Test Loss: 0.955.. \n",
      "Epoch: 308/5000..  Training Loss: 0.946..  Test Loss: 0.955.. \n",
      "Epoch: 309/5000..  Training Loss: 0.945..  Test Loss: 0.955.. \n",
      "Epoch: 310/5000..  Training Loss: 0.964..  Test Loss: 0.955.. \n",
      "Epoch: 311/5000..  Training Loss: 0.946..  Test Loss: 0.955.. \n",
      "Epoch: 312/5000..  Training Loss: 0.919..  Test Loss: 0.955.. \n",
      "Epoch: 313/5000..  Training Loss: 0.947..  Test Loss: 0.955.. \n",
      "Epoch: 314/5000..  Training Loss: 0.945..  Test Loss: 0.955.. \n",
      "Epoch: 315/5000..  Training Loss: 0.935..  Test Loss: 0.955.. \n",
      "Epoch: 316/5000..  Training Loss: 0.940..  Test Loss: 0.955.. \n",
      "Epoch: 317/5000..  Training Loss: 0.968..  Test Loss: 0.955.. \n",
      "Epoch: 318/5000..  Training Loss: 0.953..  Test Loss: 0.955.. \n",
      "Epoch: 319/5000..  Training Loss: 0.934..  Test Loss: 0.955.. \n",
      "Epoch: 320/5000..  Training Loss: 0.933..  Test Loss: 0.955.. \n",
      "Epoch: 321/5000..  Training Loss: 0.965..  Test Loss: 0.955.. \n",
      "Epoch: 322/5000..  Training Loss: 0.958..  Test Loss: 0.955.. \n",
      "Epoch: 323/5000..  Training Loss: 0.975..  Test Loss: 0.954.. \n",
      "Epoch: 324/5000..  Training Loss: 0.943..  Test Loss: 0.954.. \n",
      "Epoch: 325/5000..  Training Loss: 0.972..  Test Loss: 0.954.. \n",
      "Epoch: 326/5000..  Training Loss: 0.992..  Test Loss: 0.954.. \n",
      "Epoch: 327/5000..  Training Loss: 0.955..  Test Loss: 0.954.. \n",
      "Epoch: 328/5000..  Training Loss: 0.956..  Test Loss: 0.954.. \n",
      "Epoch: 329/5000..  Training Loss: 0.984..  Test Loss: 0.954.. \n",
      "Epoch: 330/5000..  Training Loss: 0.933..  Test Loss: 0.954.. \n",
      "Epoch: 331/5000..  Training Loss: 0.934..  Test Loss: 0.954.. \n",
      "Epoch: 332/5000..  Training Loss: 0.951..  Test Loss: 0.954.. \n",
      "Epoch: 333/5000..  Training Loss: 0.937..  Test Loss: 0.954.. \n",
      "Epoch: 334/5000..  Training Loss: 0.915..  Test Loss: 0.954.. \n",
      "Epoch: 335/5000..  Training Loss: 0.952..  Test Loss: 0.954.. \n",
      "Epoch: 336/5000..  Training Loss: 0.914..  Test Loss: 0.954.. \n",
      "Epoch: 337/5000..  Training Loss: 0.968..  Test Loss: 0.954.. \n",
      "Epoch: 338/5000..  Training Loss: 0.942..  Test Loss: 0.954.. \n",
      "Epoch: 339/5000..  Training Loss: 0.976..  Test Loss: 0.954.. \n",
      "Epoch: 340/5000..  Training Loss: 0.946..  Test Loss: 0.954.. \n",
      "Epoch: 341/5000..  Training Loss: 0.947..  Test Loss: 0.954.. \n",
      "Epoch: 342/5000..  Training Loss: 0.933..  Test Loss: 0.954.. \n",
      "Epoch: 343/5000..  Training Loss: 0.978..  Test Loss: 0.954.. \n",
      "Epoch: 344/5000..  Training Loss: 0.972..  Test Loss: 0.954.. \n",
      "Epoch: 345/5000..  Training Loss: 0.963..  Test Loss: 0.954.. \n",
      "Epoch: 346/5000..  Training Loss: 0.942..  Test Loss: 0.954.. \n",
      "Epoch: 347/5000..  Training Loss: 0.970..  Test Loss: 0.954.. \n",
      "Epoch: 348/5000..  Training Loss: 0.934..  Test Loss: 0.954.. \n",
      "Epoch: 349/5000..  Training Loss: 0.946..  Test Loss: 0.954.. \n",
      "Epoch: 350/5000..  Training Loss: 0.948..  Test Loss: 0.954.. \n",
      "Epoch: 351/5000..  Training Loss: 0.986..  Test Loss: 0.954.. \n",
      "Epoch: 352/5000..  Training Loss: 0.968..  Test Loss: 0.954.. \n",
      "Epoch: 353/5000..  Training Loss: 0.978..  Test Loss: 0.954.. \n",
      "Epoch: 354/5000..  Training Loss: 0.964..  Test Loss: 0.954.. \n",
      "Epoch: 355/5000..  Training Loss: 0.974..  Test Loss: 0.954.. \n",
      "Epoch: 356/5000..  Training Loss: 0.956..  Test Loss: 0.954.. \n",
      "Epoch: 357/5000..  Training Loss: 0.966..  Test Loss: 0.954.. \n",
      "Epoch: 358/5000..  Training Loss: 0.965..  Test Loss: 0.954.. \n",
      "Epoch: 359/5000..  Training Loss: 0.977..  Test Loss: 0.954.. \n",
      "Epoch: 360/5000..  Training Loss: 0.966..  Test Loss: 0.954.. \n",
      "Epoch: 361/5000..  Training Loss: 0.955..  Test Loss: 0.954.. \n",
      "Epoch: 362/5000..  Training Loss: 0.919..  Test Loss: 0.954.. \n",
      "Epoch: 363/5000..  Training Loss: 0.971..  Test Loss: 0.954.. \n",
      "Epoch: 364/5000..  Training Loss: 0.986..  Test Loss: 0.954.. \n",
      "Epoch: 365/5000..  Training Loss: 0.971..  Test Loss: 0.954.. \n",
      "Epoch: 366/5000..  Training Loss: 0.946..  Test Loss: 0.954.. \n",
      "Epoch: 367/5000..  Training Loss: 0.931..  Test Loss: 0.954.. \n",
      "Epoch: 368/5000..  Training Loss: 0.922..  Test Loss: 0.954.. \n",
      "Epoch: 369/5000..  Training Loss: 0.932..  Test Loss: 0.954.. \n",
      "Epoch: 370/5000..  Training Loss: 0.951..  Test Loss: 0.954.. \n",
      "Epoch: 371/5000..  Training Loss: 0.965..  Test Loss: 0.954.. \n",
      "Epoch: 372/5000..  Training Loss: 0.997..  Test Loss: 0.954.. \n",
      "Epoch: 373/5000..  Training Loss: 0.937..  Test Loss: 0.954.. \n",
      "Epoch: 374/5000..  Training Loss: 0.949..  Test Loss: 0.954.. \n",
      "Epoch: 375/5000..  Training Loss: 0.965..  Test Loss: 0.954.. \n",
      "Epoch: 376/5000..  Training Loss: 0.950..  Test Loss: 0.954.. \n",
      "Epoch: 377/5000..  Training Loss: 0.955..  Test Loss: 0.954.. \n",
      "Epoch: 378/5000..  Training Loss: 0.936..  Test Loss: 0.954.. \n",
      "Epoch: 379/5000..  Training Loss: 0.945..  Test Loss: 0.954.. \n",
      "Epoch: 380/5000..  Training Loss: 0.947..  Test Loss: 0.954.. \n",
      "Epoch: 381/5000..  Training Loss: 0.941..  Test Loss: 0.954.. \n",
      "Epoch: 382/5000..  Training Loss: 0.935..  Test Loss: 0.954.. \n",
      "Epoch: 383/5000..  Training Loss: 0.953..  Test Loss: 0.954.. \n",
      "Epoch: 384/5000..  Training Loss: 0.955..  Test Loss: 0.954.. \n",
      "Epoch: 385/5000..  Training Loss: 0.949..  Test Loss: 0.954.. \n",
      "Epoch: 386/5000..  Training Loss: 0.974..  Test Loss: 0.954.. \n",
      "Epoch: 387/5000..  Training Loss: 0.929..  Test Loss: 0.954.. \n",
      "Epoch: 388/5000..  Training Loss: 0.937..  Test Loss: 0.954.. \n",
      "Epoch: 389/5000..  Training Loss: 0.948..  Test Loss: 0.954.. \n",
      "Epoch: 390/5000..  Training Loss: 0.956..  Test Loss: 0.954.. \n",
      "Epoch: 391/5000..  Training Loss: 0.952..  Test Loss: 0.954.. \n",
      "Epoch: 392/5000..  Training Loss: 0.947..  Test Loss: 0.954.. \n",
      "Epoch: 393/5000..  Training Loss: 0.961..  Test Loss: 0.954.. \n",
      "Epoch: 394/5000..  Training Loss: 0.935..  Test Loss: 0.954.. \n",
      "Epoch: 395/5000..  Training Loss: 0.953..  Test Loss: 0.954.. \n",
      "Epoch: 396/5000..  Training Loss: 0.960..  Test Loss: 0.954.. \n",
      "Epoch: 397/5000..  Training Loss: 0.962..  Test Loss: 0.954.. \n",
      "Epoch: 398/5000..  Training Loss: 0.949..  Test Loss: 0.954.. \n",
      "Epoch: 399/5000..  Training Loss: 0.962..  Test Loss: 0.954.. \n",
      "Epoch: 400/5000..  Training Loss: 0.969..  Test Loss: 0.954.. \n",
      "Epoch: 401/5000..  Training Loss: 0.970..  Test Loss: 0.954.. \n",
      "Epoch: 402/5000..  Training Loss: 0.930..  Test Loss: 0.954.. \n",
      "Epoch: 403/5000..  Training Loss: 0.943..  Test Loss: 0.954.. \n",
      "Epoch: 404/5000..  Training Loss: 0.957..  Test Loss: 0.954.. \n",
      "Epoch: 405/5000..  Training Loss: 0.955..  Test Loss: 0.954.. \n",
      "Epoch: 406/5000..  Training Loss: 0.958..  Test Loss: 0.954.. \n",
      "Epoch: 407/5000..  Training Loss: 0.931..  Test Loss: 0.954.. \n",
      "Epoch: 408/5000..  Training Loss: 0.978..  Test Loss: 0.954.. \n",
      "Epoch: 409/5000..  Training Loss: 0.974..  Test Loss: 0.954.. \n",
      "Epoch: 410/5000..  Training Loss: 0.947..  Test Loss: 0.954.. \n",
      "Epoch: 411/5000..  Training Loss: 0.942..  Test Loss: 0.954.. \n",
      "Epoch: 412/5000..  Training Loss: 0.921..  Test Loss: 0.954.. \n",
      "Epoch: 413/5000..  Training Loss: 0.943..  Test Loss: 0.954.. \n",
      "Epoch: 414/5000..  Training Loss: 0.934..  Test Loss: 0.954.. \n",
      "Epoch: 415/5000..  Training Loss: 0.964..  Test Loss: 0.954.. \n",
      "Epoch: 416/5000..  Training Loss: 0.972..  Test Loss: 0.954.. \n",
      "Epoch: 417/5000..  Training Loss: 0.944..  Test Loss: 0.954.. \n",
      "Epoch: 418/5000..  Training Loss: 0.985..  Test Loss: 0.954.. \n",
      "Epoch: 419/5000..  Training Loss: 0.936..  Test Loss: 0.954.. \n",
      "Epoch: 420/5000..  Training Loss: 0.989..  Test Loss: 0.954.. \n",
      "Epoch: 421/5000..  Training Loss: 0.967..  Test Loss: 0.953.. \n",
      "Epoch: 422/5000..  Training Loss: 0.948..  Test Loss: 0.953.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 423/5000..  Training Loss: 0.905..  Test Loss: 0.953.. \n",
      "Epoch: 424/5000..  Training Loss: 0.967..  Test Loss: 0.953.. \n",
      "Epoch: 425/5000..  Training Loss: 0.928..  Test Loss: 0.953.. \n",
      "Epoch: 426/5000..  Training Loss: 0.972..  Test Loss: 0.953.. \n",
      "Epoch: 427/5000..  Training Loss: 0.950..  Test Loss: 0.953.. \n",
      "Epoch: 428/5000..  Training Loss: 0.945..  Test Loss: 0.953.. \n",
      "Epoch: 429/5000..  Training Loss: 0.958..  Test Loss: 0.953.. \n",
      "Epoch: 430/5000..  Training Loss: 0.942..  Test Loss: 0.953.. \n",
      "Epoch: 431/5000..  Training Loss: 0.904..  Test Loss: 0.953.. \n",
      "Epoch: 432/5000..  Training Loss: 0.951..  Test Loss: 0.953.. \n",
      "Epoch: 433/5000..  Training Loss: 0.952..  Test Loss: 0.953.. \n",
      "Epoch: 434/5000..  Training Loss: 0.947..  Test Loss: 0.953.. \n",
      "Epoch: 435/5000..  Training Loss: 0.959..  Test Loss: 0.953.. \n",
      "Epoch: 436/5000..  Training Loss: 0.946..  Test Loss: 0.953.. \n",
      "Epoch: 437/5000..  Training Loss: 0.933..  Test Loss: 0.953.. \n",
      "Epoch: 438/5000..  Training Loss: 0.958..  Test Loss: 0.953.. \n",
      "Epoch: 439/5000..  Training Loss: 0.951..  Test Loss: 0.953.. \n",
      "Epoch: 440/5000..  Training Loss: 0.941..  Test Loss: 0.953.. \n",
      "Epoch: 441/5000..  Training Loss: 0.953..  Test Loss: 0.953.. \n",
      "Epoch: 442/5000..  Training Loss: 0.992..  Test Loss: 0.953.. \n",
      "Epoch: 443/5000..  Training Loss: 0.921..  Test Loss: 0.953.. \n",
      "Epoch: 444/5000..  Training Loss: 0.952..  Test Loss: 0.953.. \n",
      "Epoch: 445/5000..  Training Loss: 0.961..  Test Loss: 0.953.. \n",
      "Epoch: 446/5000..  Training Loss: 0.977..  Test Loss: 0.953.. \n",
      "Epoch: 447/5000..  Training Loss: 0.954..  Test Loss: 0.953.. \n",
      "Epoch: 448/5000..  Training Loss: 0.951..  Test Loss: 0.953.. \n",
      "Epoch: 449/5000..  Training Loss: 0.973..  Test Loss: 0.953.. \n",
      "Epoch: 450/5000..  Training Loss: 0.977..  Test Loss: 0.953.. \n",
      "Epoch: 451/5000..  Training Loss: 0.979..  Test Loss: 0.953.. \n",
      "Epoch: 452/5000..  Training Loss: 0.976..  Test Loss: 0.953.. \n",
      "Epoch: 453/5000..  Training Loss: 1.007..  Test Loss: 0.953.. \n",
      "Epoch: 454/5000..  Training Loss: 0.973..  Test Loss: 0.953.. \n",
      "Epoch: 455/5000..  Training Loss: 0.977..  Test Loss: 0.953.. \n",
      "Epoch: 456/5000..  Training Loss: 0.950..  Test Loss: 0.953.. \n",
      "Epoch: 457/5000..  Training Loss: 0.958..  Test Loss: 0.953.. \n",
      "Epoch: 458/5000..  Training Loss: 0.928..  Test Loss: 0.953.. \n",
      "Epoch: 459/5000..  Training Loss: 0.964..  Test Loss: 0.953.. \n",
      "Epoch: 460/5000..  Training Loss: 0.966..  Test Loss: 0.953.. \n",
      "Epoch: 461/5000..  Training Loss: 0.945..  Test Loss: 0.953.. \n",
      "Epoch: 462/5000..  Training Loss: 0.944..  Test Loss: 0.953.. \n",
      "Epoch: 463/5000..  Training Loss: 0.973..  Test Loss: 0.953.. \n",
      "Epoch: 464/5000..  Training Loss: 0.948..  Test Loss: 0.953.. \n",
      "Epoch: 465/5000..  Training Loss: 0.933..  Test Loss: 0.953.. \n",
      "Epoch: 466/5000..  Training Loss: 0.934..  Test Loss: 0.953.. \n",
      "Epoch: 467/5000..  Training Loss: 0.950..  Test Loss: 0.953.. \n",
      "Epoch: 468/5000..  Training Loss: 0.967..  Test Loss: 0.953.. \n",
      "Epoch: 469/5000..  Training Loss: 0.927..  Test Loss: 0.953.. \n",
      "Epoch: 470/5000..  Training Loss: 0.973..  Test Loss: 0.953.. \n",
      "Epoch: 471/5000..  Training Loss: 0.929..  Test Loss: 0.953.. \n",
      "Epoch: 472/5000..  Training Loss: 0.941..  Test Loss: 0.953.. \n",
      "Epoch: 473/5000..  Training Loss: 0.980..  Test Loss: 0.953.. \n",
      "Epoch: 474/5000..  Training Loss: 0.950..  Test Loss: 0.953.. \n",
      "Epoch: 475/5000..  Training Loss: 0.935..  Test Loss: 0.953.. \n",
      "Epoch: 476/5000..  Training Loss: 0.943..  Test Loss: 0.953.. \n",
      "Epoch: 477/5000..  Training Loss: 0.940..  Test Loss: 0.953.. \n",
      "Epoch: 478/5000..  Training Loss: 0.987..  Test Loss: 0.953.. \n",
      "Epoch: 479/5000..  Training Loss: 0.993..  Test Loss: 0.953.. \n",
      "Epoch: 480/5000..  Training Loss: 0.958..  Test Loss: 0.953.. \n",
      "Epoch: 481/5000..  Training Loss: 0.954..  Test Loss: 0.953.. \n",
      "Epoch: 482/5000..  Training Loss: 0.943..  Test Loss: 0.953.. \n",
      "Epoch: 483/5000..  Training Loss: 0.940..  Test Loss: 0.953.. \n",
      "Epoch: 484/5000..  Training Loss: 0.982..  Test Loss: 0.953.. \n",
      "Epoch: 485/5000..  Training Loss: 0.975..  Test Loss: 0.953.. \n",
      "Epoch: 486/5000..  Training Loss: 0.962..  Test Loss: 0.953.. \n",
      "Epoch: 487/5000..  Training Loss: 0.938..  Test Loss: 0.953.. \n",
      "Epoch: 488/5000..  Training Loss: 0.953..  Test Loss: 0.953.. \n",
      "Epoch: 489/5000..  Training Loss: 0.948..  Test Loss: 0.953.. \n",
      "Epoch: 490/5000..  Training Loss: 0.932..  Test Loss: 0.953.. \n",
      "Epoch: 491/5000..  Training Loss: 0.952..  Test Loss: 0.953.. \n",
      "Epoch: 492/5000..  Training Loss: 0.941..  Test Loss: 0.953.. \n",
      "Epoch: 493/5000..  Training Loss: 0.986..  Test Loss: 0.953.. \n",
      "Epoch: 494/5000..  Training Loss: 0.950..  Test Loss: 0.953.. \n",
      "Epoch: 495/5000..  Training Loss: 0.980..  Test Loss: 0.953.. \n",
      "Epoch: 496/5000..  Training Loss: 0.947..  Test Loss: 0.953.. \n",
      "Epoch: 497/5000..  Training Loss: 0.978..  Test Loss: 0.953.. \n",
      "Epoch: 498/5000..  Training Loss: 0.964..  Test Loss: 0.953.. \n",
      "Epoch: 499/5000..  Training Loss: 0.964..  Test Loss: 0.953.. \n",
      "Epoch: 500/5000..  Training Loss: 0.904..  Test Loss: 0.953.. \n",
      "Epoch: 501/5000..  Training Loss: 0.952..  Test Loss: 0.953.. \n",
      "Epoch: 502/5000..  Training Loss: 0.968..  Test Loss: 0.953.. \n",
      "Epoch: 503/5000..  Training Loss: 0.950..  Test Loss: 0.953.. \n",
      "Epoch: 504/5000..  Training Loss: 0.946..  Test Loss: 0.953.. \n",
      "Epoch: 505/5000..  Training Loss: 0.959..  Test Loss: 0.953.. \n",
      "Epoch: 506/5000..  Training Loss: 0.947..  Test Loss: 0.953.. \n",
      "Epoch: 507/5000..  Training Loss: 0.940..  Test Loss: 0.953.. \n",
      "Epoch: 508/5000..  Training Loss: 0.962..  Test Loss: 0.953.. \n",
      "Epoch: 509/5000..  Training Loss: 0.962..  Test Loss: 0.953.. \n",
      "Epoch: 510/5000..  Training Loss: 0.965..  Test Loss: 0.953.. \n",
      "Epoch: 511/5000..  Training Loss: 0.942..  Test Loss: 0.953.. \n",
      "Epoch: 512/5000..  Training Loss: 0.985..  Test Loss: 0.953.. \n",
      "Epoch: 513/5000..  Training Loss: 0.947..  Test Loss: 0.953.. \n",
      "Epoch: 514/5000..  Training Loss: 0.924..  Test Loss: 0.953.. \n",
      "Epoch: 515/5000..  Training Loss: 0.989..  Test Loss: 0.953.. \n",
      "Epoch: 516/5000..  Training Loss: 0.961..  Test Loss: 0.953.. \n",
      "Epoch: 517/5000..  Training Loss: 0.925..  Test Loss: 0.953.. \n",
      "Epoch: 518/5000..  Training Loss: 0.962..  Test Loss: 0.953.. \n",
      "Epoch: 519/5000..  Training Loss: 0.938..  Test Loss: 0.953.. \n",
      "Epoch: 520/5000..  Training Loss: 0.949..  Test Loss: 0.953.. \n",
      "Epoch: 521/5000..  Training Loss: 0.990..  Test Loss: 0.953.. \n",
      "Epoch: 522/5000..  Training Loss: 0.952..  Test Loss: 0.953.. \n",
      "Epoch: 523/5000..  Training Loss: 0.979..  Test Loss: 0.953.. \n",
      "Epoch: 524/5000..  Training Loss: 0.957..  Test Loss: 0.953.. \n",
      "Epoch: 525/5000..  Training Loss: 0.905..  Test Loss: 0.952.. \n",
      "Epoch: 526/5000..  Training Loss: 0.955..  Test Loss: 0.952.. \n",
      "Epoch: 527/5000..  Training Loss: 0.948..  Test Loss: 0.952.. \n",
      "Epoch: 528/5000..  Training Loss: 0.934..  Test Loss: 0.952.. \n",
      "Epoch: 529/5000..  Training Loss: 0.942..  Test Loss: 0.952.. \n",
      "Epoch: 530/5000..  Training Loss: 0.943..  Test Loss: 0.952.. \n",
      "Epoch: 531/5000..  Training Loss: 0.982..  Test Loss: 0.952.. \n",
      "Epoch: 532/5000..  Training Loss: 0.927..  Test Loss: 0.952.. \n",
      "Epoch: 533/5000..  Training Loss: 0.954..  Test Loss: 0.952.. \n",
      "Epoch: 534/5000..  Training Loss: 0.931..  Test Loss: 0.952.. \n",
      "Epoch: 535/5000..  Training Loss: 0.966..  Test Loss: 0.952.. \n",
      "Epoch: 536/5000..  Training Loss: 0.952..  Test Loss: 0.952.. \n",
      "Epoch: 537/5000..  Training Loss: 0.955..  Test Loss: 0.952.. \n",
      "Epoch: 538/5000..  Training Loss: 0.960..  Test Loss: 0.952.. \n",
      "Epoch: 539/5000..  Training Loss: 0.963..  Test Loss: 0.952.. \n",
      "Epoch: 540/5000..  Training Loss: 0.939..  Test Loss: 0.952.. \n",
      "Epoch: 541/5000..  Training Loss: 0.983..  Test Loss: 0.952.. \n",
      "Epoch: 542/5000..  Training Loss: 0.943..  Test Loss: 0.952.. \n",
      "Epoch: 543/5000..  Training Loss: 0.943..  Test Loss: 0.952.. \n",
      "Epoch: 544/5000..  Training Loss: 0.997..  Test Loss: 0.952.. \n",
      "Epoch: 545/5000..  Training Loss: 0.960..  Test Loss: 0.952.. \n",
      "Epoch: 546/5000..  Training Loss: 0.958..  Test Loss: 0.952.. \n",
      "Epoch: 547/5000..  Training Loss: 0.942..  Test Loss: 0.952.. \n",
      "Epoch: 548/5000..  Training Loss: 0.948..  Test Loss: 0.952.. \n",
      "Epoch: 549/5000..  Training Loss: 0.931..  Test Loss: 0.952.. \n",
      "Epoch: 550/5000..  Training Loss: 0.942..  Test Loss: 0.952.. \n",
      "Epoch: 551/5000..  Training Loss: 0.955..  Test Loss: 0.952.. \n",
      "Epoch: 552/5000..  Training Loss: 0.972..  Test Loss: 0.952.. \n",
      "Epoch: 553/5000..  Training Loss: 0.930..  Test Loss: 0.952.. \n",
      "Epoch: 554/5000..  Training Loss: 0.979..  Test Loss: 0.952.. \n",
      "Epoch: 555/5000..  Training Loss: 0.965..  Test Loss: 0.952.. \n",
      "Epoch: 556/5000..  Training Loss: 0.931..  Test Loss: 0.952.. \n",
      "Epoch: 557/5000..  Training Loss: 0.937..  Test Loss: 0.952.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 558/5000..  Training Loss: 0.977..  Test Loss: 0.952.. \n",
      "Epoch: 559/5000..  Training Loss: 0.917..  Test Loss: 0.952.. \n",
      "Epoch: 560/5000..  Training Loss: 0.962..  Test Loss: 0.952.. \n",
      "Epoch: 561/5000..  Training Loss: 0.952..  Test Loss: 0.952.. \n",
      "Epoch: 562/5000..  Training Loss: 0.969..  Test Loss: 0.952.. \n",
      "Epoch: 563/5000..  Training Loss: 0.954..  Test Loss: 0.952.. \n",
      "Epoch: 564/5000..  Training Loss: 0.928..  Test Loss: 0.952.. \n",
      "Epoch: 565/5000..  Training Loss: 0.964..  Test Loss: 0.952.. \n",
      "Epoch: 566/5000..  Training Loss: 0.972..  Test Loss: 0.952.. \n",
      "Epoch: 567/5000..  Training Loss: 0.929..  Test Loss: 0.952.. \n",
      "Epoch: 568/5000..  Training Loss: 0.961..  Test Loss: 0.952.. \n",
      "Epoch: 569/5000..  Training Loss: 0.937..  Test Loss: 0.952.. \n",
      "Epoch: 570/5000..  Training Loss: 0.948..  Test Loss: 0.952.. \n",
      "Epoch: 571/5000..  Training Loss: 0.936..  Test Loss: 0.952.. \n",
      "Epoch: 572/5000..  Training Loss: 0.973..  Test Loss: 0.952.. \n",
      "Epoch: 573/5000..  Training Loss: 0.955..  Test Loss: 0.952.. \n",
      "Epoch: 574/5000..  Training Loss: 0.947..  Test Loss: 0.952.. \n",
      "Epoch: 575/5000..  Training Loss: 0.941..  Test Loss: 0.952.. \n",
      "Epoch: 576/5000..  Training Loss: 0.923..  Test Loss: 0.952.. \n",
      "Epoch: 577/5000..  Training Loss: 0.951..  Test Loss: 0.952.. \n",
      "Epoch: 578/5000..  Training Loss: 0.955..  Test Loss: 0.952.. \n",
      "Epoch: 579/5000..  Training Loss: 0.964..  Test Loss: 0.952.. \n",
      "Epoch: 580/5000..  Training Loss: 0.980..  Test Loss: 0.952.. \n",
      "Epoch: 581/5000..  Training Loss: 0.969..  Test Loss: 0.952.. \n",
      "Epoch: 582/5000..  Training Loss: 0.945..  Test Loss: 0.952.. \n",
      "Epoch: 583/5000..  Training Loss: 0.968..  Test Loss: 0.952.. \n",
      "Epoch: 584/5000..  Training Loss: 0.945..  Test Loss: 0.952.. \n",
      "Epoch: 585/5000..  Training Loss: 0.978..  Test Loss: 0.952.. \n",
      "Epoch: 586/5000..  Training Loss: 0.943..  Test Loss: 0.952.. \n",
      "Epoch: 587/5000..  Training Loss: 0.950..  Test Loss: 0.952.. \n",
      "Epoch: 588/5000..  Training Loss: 0.971..  Test Loss: 0.952.. \n",
      "Epoch: 589/5000..  Training Loss: 0.955..  Test Loss: 0.952.. \n",
      "Epoch: 590/5000..  Training Loss: 0.915..  Test Loss: 0.952.. \n",
      "Epoch: 591/5000..  Training Loss: 0.932..  Test Loss: 0.952.. \n",
      "Epoch: 592/5000..  Training Loss: 0.946..  Test Loss: 0.952.. \n",
      "Epoch: 593/5000..  Training Loss: 0.906..  Test Loss: 0.952.. \n",
      "Epoch: 594/5000..  Training Loss: 0.970..  Test Loss: 0.952.. \n",
      "Epoch: 595/5000..  Training Loss: 0.976..  Test Loss: 0.952.. \n",
      "Epoch: 596/5000..  Training Loss: 0.954..  Test Loss: 0.952.. \n",
      "Epoch: 597/5000..  Training Loss: 0.944..  Test Loss: 0.952.. \n",
      "Epoch: 598/5000..  Training Loss: 0.930..  Test Loss: 0.952.. \n",
      "Epoch: 599/5000..  Training Loss: 0.951..  Test Loss: 0.952.. \n",
      "Epoch: 600/5000..  Training Loss: 0.957..  Test Loss: 0.952.. \n",
      "Epoch: 601/5000..  Training Loss: 0.949..  Test Loss: 0.952.. \n",
      "Epoch: 602/5000..  Training Loss: 0.935..  Test Loss: 0.952.. \n",
      "Epoch: 603/5000..  Training Loss: 0.974..  Test Loss: 0.952.. \n",
      "Epoch: 604/5000..  Training Loss: 0.941..  Test Loss: 0.952.. \n",
      "Epoch: 605/5000..  Training Loss: 0.927..  Test Loss: 0.952.. \n",
      "Epoch: 606/5000..  Training Loss: 0.943..  Test Loss: 0.952.. \n",
      "Epoch: 607/5000..  Training Loss: 0.955..  Test Loss: 0.952.. \n",
      "Epoch: 608/5000..  Training Loss: 0.967..  Test Loss: 0.952.. \n",
      "Epoch: 609/5000..  Training Loss: 0.965..  Test Loss: 0.952.. \n",
      "Epoch: 610/5000..  Training Loss: 0.927..  Test Loss: 0.952.. \n",
      "Epoch: 611/5000..  Training Loss: 0.943..  Test Loss: 0.952.. \n",
      "Epoch: 612/5000..  Training Loss: 0.923..  Test Loss: 0.952.. \n",
      "Epoch: 613/5000..  Training Loss: 0.958..  Test Loss: 0.952.. \n",
      "Epoch: 614/5000..  Training Loss: 0.958..  Test Loss: 0.952.. \n",
      "Epoch: 615/5000..  Training Loss: 0.951..  Test Loss: 0.952.. \n",
      "Epoch: 616/5000..  Training Loss: 0.925..  Test Loss: 0.952.. \n",
      "Epoch: 617/5000..  Training Loss: 0.944..  Test Loss: 0.952.. \n",
      "Epoch: 618/5000..  Training Loss: 0.972..  Test Loss: 0.952.. \n",
      "Epoch: 619/5000..  Training Loss: 0.976..  Test Loss: 0.952.. \n",
      "Epoch: 620/5000..  Training Loss: 0.944..  Test Loss: 0.952.. \n",
      "Epoch: 621/5000..  Training Loss: 0.982..  Test Loss: 0.952.. \n",
      "Epoch: 622/5000..  Training Loss: 0.954..  Test Loss: 0.952.. \n",
      "Epoch: 623/5000..  Training Loss: 0.938..  Test Loss: 0.951.. \n",
      "Epoch: 624/5000..  Training Loss: 0.963..  Test Loss: 0.951.. \n",
      "Epoch: 625/5000..  Training Loss: 0.962..  Test Loss: 0.951.. \n",
      "Epoch: 626/5000..  Training Loss: 0.914..  Test Loss: 0.951.. \n",
      "Epoch: 627/5000..  Training Loss: 0.954..  Test Loss: 0.951.. \n",
      "Epoch: 628/5000..  Training Loss: 0.952..  Test Loss: 0.951.. \n",
      "Epoch: 629/5000..  Training Loss: 0.969..  Test Loss: 0.951.. \n",
      "Epoch: 630/5000..  Training Loss: 0.980..  Test Loss: 0.951.. \n",
      "Epoch: 631/5000..  Training Loss: 0.948..  Test Loss: 0.951.. \n",
      "Epoch: 632/5000..  Training Loss: 0.962..  Test Loss: 0.951.. \n",
      "Epoch: 633/5000..  Training Loss: 0.932..  Test Loss: 0.951.. \n",
      "Epoch: 634/5000..  Training Loss: 0.968..  Test Loss: 0.951.. \n",
      "Epoch: 635/5000..  Training Loss: 0.971..  Test Loss: 0.951.. \n",
      "Epoch: 636/5000..  Training Loss: 0.935..  Test Loss: 0.951.. \n",
      "Epoch: 637/5000..  Training Loss: 0.957..  Test Loss: 0.951.. \n",
      "Epoch: 638/5000..  Training Loss: 0.937..  Test Loss: 0.951.. \n",
      "Epoch: 639/5000..  Training Loss: 0.931..  Test Loss: 0.951.. \n",
      "Epoch: 640/5000..  Training Loss: 0.958..  Test Loss: 0.951.. \n",
      "Epoch: 641/5000..  Training Loss: 0.960..  Test Loss: 0.951.. \n",
      "Epoch: 642/5000..  Training Loss: 0.968..  Test Loss: 0.951.. \n",
      "Epoch: 643/5000..  Training Loss: 0.960..  Test Loss: 0.951.. \n",
      "Epoch: 644/5000..  Training Loss: 0.968..  Test Loss: 0.951.. \n",
      "Epoch: 645/5000..  Training Loss: 0.955..  Test Loss: 0.951.. \n",
      "Epoch: 646/5000..  Training Loss: 0.948..  Test Loss: 0.951.. \n",
      "Epoch: 647/5000..  Training Loss: 0.923..  Test Loss: 0.951.. \n",
      "Epoch: 648/5000..  Training Loss: 0.953..  Test Loss: 0.951.. \n",
      "Epoch: 649/5000..  Training Loss: 0.962..  Test Loss: 0.951.. \n",
      "Epoch: 650/5000..  Training Loss: 0.981..  Test Loss: 0.951.. \n",
      "Epoch: 651/5000..  Training Loss: 0.923..  Test Loss: 0.951.. \n",
      "Epoch: 652/5000..  Training Loss: 0.954..  Test Loss: 0.951.. \n",
      "Epoch: 653/5000..  Training Loss: 0.930..  Test Loss: 0.951.. \n",
      "Epoch: 654/5000..  Training Loss: 0.962..  Test Loss: 0.951.. \n",
      "Epoch: 655/5000..  Training Loss: 0.960..  Test Loss: 0.951.. \n",
      "Epoch: 656/5000..  Training Loss: 0.974..  Test Loss: 0.951.. \n",
      "Epoch: 657/5000..  Training Loss: 0.964..  Test Loss: 0.951.. \n",
      "Epoch: 658/5000..  Training Loss: 0.950..  Test Loss: 0.951.. \n",
      "Epoch: 659/5000..  Training Loss: 0.967..  Test Loss: 0.951.. \n",
      "Epoch: 660/5000..  Training Loss: 0.930..  Test Loss: 0.951.. \n",
      "Epoch: 661/5000..  Training Loss: 0.938..  Test Loss: 0.951.. \n",
      "Epoch: 662/5000..  Training Loss: 0.948..  Test Loss: 0.951.. \n",
      "Epoch: 663/5000..  Training Loss: 0.953..  Test Loss: 0.951.. \n",
      "Epoch: 664/5000..  Training Loss: 0.932..  Test Loss: 0.951.. \n",
      "Epoch: 665/5000..  Training Loss: 0.959..  Test Loss: 0.951.. \n",
      "Epoch: 666/5000..  Training Loss: 0.974..  Test Loss: 0.951.. \n",
      "Epoch: 667/5000..  Training Loss: 0.938..  Test Loss: 0.951.. \n",
      "Epoch: 668/5000..  Training Loss: 0.985..  Test Loss: 0.951.. \n",
      "Epoch: 669/5000..  Training Loss: 0.911..  Test Loss: 0.951.. \n",
      "Epoch: 670/5000..  Training Loss: 0.939..  Test Loss: 0.951.. \n",
      "Epoch: 671/5000..  Training Loss: 0.935..  Test Loss: 0.951.. \n",
      "Epoch: 672/5000..  Training Loss: 0.960..  Test Loss: 0.951.. \n",
      "Epoch: 673/5000..  Training Loss: 0.941..  Test Loss: 0.951.. \n",
      "Epoch: 674/5000..  Training Loss: 0.953..  Test Loss: 0.951.. \n",
      "Epoch: 675/5000..  Training Loss: 0.934..  Test Loss: 0.951.. \n",
      "Epoch: 676/5000..  Training Loss: 0.949..  Test Loss: 0.951.. \n",
      "Epoch: 677/5000..  Training Loss: 0.941..  Test Loss: 0.951.. \n",
      "Epoch: 678/5000..  Training Loss: 0.977..  Test Loss: 0.951.. \n",
      "Epoch: 679/5000..  Training Loss: 0.921..  Test Loss: 0.951.. \n",
      "Epoch: 680/5000..  Training Loss: 0.956..  Test Loss: 0.951.. \n",
      "Epoch: 681/5000..  Training Loss: 0.940..  Test Loss: 0.951.. \n",
      "Epoch: 682/5000..  Training Loss: 0.990..  Test Loss: 0.951.. \n",
      "Epoch: 683/5000..  Training Loss: 0.964..  Test Loss: 0.951.. \n",
      "Epoch: 684/5000..  Training Loss: 0.933..  Test Loss: 0.951.. \n",
      "Epoch: 685/5000..  Training Loss: 0.997..  Test Loss: 0.951.. \n",
      "Epoch: 686/5000..  Training Loss: 0.923..  Test Loss: 0.951.. \n",
      "Epoch: 687/5000..  Training Loss: 0.976..  Test Loss: 0.951.. \n",
      "Epoch: 688/5000..  Training Loss: 0.919..  Test Loss: 0.951.. \n",
      "Epoch: 689/5000..  Training Loss: 0.921..  Test Loss: 0.951.. \n",
      "Epoch: 690/5000..  Training Loss: 0.941..  Test Loss: 0.951.. \n",
      "Epoch: 691/5000..  Training Loss: 0.955..  Test Loss: 0.951.. \n",
      "Epoch: 692/5000..  Training Loss: 0.940..  Test Loss: 0.951.. \n",
      "Epoch: 693/5000..  Training Loss: 0.959..  Test Loss: 0.951.. \n",
      "Epoch: 694/5000..  Training Loss: 0.967..  Test Loss: 0.951.. \n",
      "Epoch: 695/5000..  Training Loss: 0.967..  Test Loss: 0.951.. \n",
      "Epoch: 696/5000..  Training Loss: 0.932..  Test Loss: 0.951.. \n",
      "Epoch: 697/5000..  Training Loss: 0.938..  Test Loss: 0.951.. \n",
      "Epoch: 698/5000..  Training Loss: 0.975..  Test Loss: 0.951.. \n",
      "Epoch: 699/5000..  Training Loss: 0.950..  Test Loss: 0.951.. \n",
      "Epoch: 700/5000..  Training Loss: 0.941..  Test Loss: 0.951.. \n",
      "Epoch: 701/5000..  Training Loss: 0.998..  Test Loss: 0.951.. \n",
      "Epoch: 702/5000..  Training Loss: 0.944..  Test Loss: 0.951.. \n",
      "Epoch: 703/5000..  Training Loss: 0.914..  Test Loss: 0.951.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 704/5000..  Training Loss: 0.936..  Test Loss: 0.951.. \n",
      "Epoch: 705/5000..  Training Loss: 0.923..  Test Loss: 0.951.. \n",
      "Epoch: 706/5000..  Training Loss: 0.905..  Test Loss: 0.951.. \n",
      "Epoch: 707/5000..  Training Loss: 0.956..  Test Loss: 0.951.. \n",
      "Epoch: 708/5000..  Training Loss: 0.967..  Test Loss: 0.951.. \n",
      "Epoch: 709/5000..  Training Loss: 0.942..  Test Loss: 0.951.. \n",
      "Epoch: 710/5000..  Training Loss: 0.935..  Test Loss: 0.951.. \n",
      "Epoch: 711/5000..  Training Loss: 0.935..  Test Loss: 0.951.. \n",
      "Epoch: 712/5000..  Training Loss: 0.936..  Test Loss: 0.951.. \n",
      "Epoch: 713/5000..  Training Loss: 0.933..  Test Loss: 0.951.. \n",
      "Epoch: 714/5000..  Training Loss: 0.945..  Test Loss: 0.951.. \n",
      "Epoch: 715/5000..  Training Loss: 0.928..  Test Loss: 0.951.. \n",
      "Epoch: 716/5000..  Training Loss: 0.939..  Test Loss: 0.951.. \n",
      "Epoch: 717/5000..  Training Loss: 0.944..  Test Loss: 0.951.. \n",
      "Epoch: 718/5000..  Training Loss: 0.955..  Test Loss: 0.951.. \n",
      "Epoch: 719/5000..  Training Loss: 0.942..  Test Loss: 0.951.. \n",
      "Epoch: 720/5000..  Training Loss: 0.962..  Test Loss: 0.951.. \n",
      "Epoch: 721/5000..  Training Loss: 0.993..  Test Loss: 0.951.. \n",
      "Epoch: 722/5000..  Training Loss: 0.961..  Test Loss: 0.950.. \n",
      "Epoch: 723/5000..  Training Loss: 0.959..  Test Loss: 0.950.. \n",
      "Epoch: 724/5000..  Training Loss: 0.934..  Test Loss: 0.950.. \n",
      "Epoch: 725/5000..  Training Loss: 0.966..  Test Loss: 0.950.. \n",
      "Epoch: 726/5000..  Training Loss: 0.955..  Test Loss: 0.950.. \n",
      "Epoch: 727/5000..  Training Loss: 0.967..  Test Loss: 0.950.. \n",
      "Epoch: 728/5000..  Training Loss: 0.919..  Test Loss: 0.950.. \n",
      "Epoch: 729/5000..  Training Loss: 0.981..  Test Loss: 0.950.. \n",
      "Epoch: 730/5000..  Training Loss: 0.973..  Test Loss: 0.950.. \n",
      "Epoch: 731/5000..  Training Loss: 0.946..  Test Loss: 0.950.. \n",
      "Epoch: 732/5000..  Training Loss: 0.957..  Test Loss: 0.950.. \n",
      "Epoch: 733/5000..  Training Loss: 0.955..  Test Loss: 0.950.. \n",
      "Epoch: 734/5000..  Training Loss: 0.970..  Test Loss: 0.950.. \n",
      "Epoch: 735/5000..  Training Loss: 0.936..  Test Loss: 0.950.. \n",
      "Epoch: 736/5000..  Training Loss: 0.977..  Test Loss: 0.950.. \n",
      "Epoch: 737/5000..  Training Loss: 0.943..  Test Loss: 0.950.. \n",
      "Epoch: 738/5000..  Training Loss: 0.972..  Test Loss: 0.950.. \n",
      "Epoch: 739/5000..  Training Loss: 0.944..  Test Loss: 0.950.. \n",
      "Epoch: 740/5000..  Training Loss: 0.995..  Test Loss: 0.950.. \n",
      "Epoch: 741/5000..  Training Loss: 0.967..  Test Loss: 0.950.. \n",
      "Epoch: 742/5000..  Training Loss: 0.961..  Test Loss: 0.950.. \n",
      "Epoch: 743/5000..  Training Loss: 0.996..  Test Loss: 0.950.. \n",
      "Epoch: 744/5000..  Training Loss: 0.966..  Test Loss: 0.950.. \n",
      "Epoch: 745/5000..  Training Loss: 0.910..  Test Loss: 0.950.. \n",
      "Epoch: 746/5000..  Training Loss: 0.953..  Test Loss: 0.950.. \n",
      "Epoch: 747/5000..  Training Loss: 0.945..  Test Loss: 0.950.. \n",
      "Epoch: 748/5000..  Training Loss: 0.962..  Test Loss: 0.950.. \n",
      "Epoch: 749/5000..  Training Loss: 0.957..  Test Loss: 0.950.. \n",
      "Epoch: 750/5000..  Training Loss: 0.968..  Test Loss: 0.950.. \n",
      "Epoch: 751/5000..  Training Loss: 0.932..  Test Loss: 0.950.. \n",
      "Epoch: 752/5000..  Training Loss: 0.944..  Test Loss: 0.950.. \n",
      "Epoch: 753/5000..  Training Loss: 0.949..  Test Loss: 0.950.. \n",
      "Epoch: 754/5000..  Training Loss: 0.944..  Test Loss: 0.950.. \n",
      "Epoch: 755/5000..  Training Loss: 0.972..  Test Loss: 0.950.. \n",
      "Epoch: 756/5000..  Training Loss: 0.961..  Test Loss: 0.950.. \n",
      "Epoch: 757/5000..  Training Loss: 0.953..  Test Loss: 0.950.. \n",
      "Epoch: 758/5000..  Training Loss: 0.959..  Test Loss: 0.950.. \n",
      "Epoch: 759/5000..  Training Loss: 0.958..  Test Loss: 0.950.. \n",
      "Epoch: 760/5000..  Training Loss: 0.926..  Test Loss: 0.950.. \n",
      "Epoch: 761/5000..  Training Loss: 0.975..  Test Loss: 0.950.. \n",
      "Epoch: 762/5000..  Training Loss: 0.945..  Test Loss: 0.950.. \n",
      "Epoch: 763/5000..  Training Loss: 0.940..  Test Loss: 0.950.. \n",
      "Epoch: 764/5000..  Training Loss: 0.961..  Test Loss: 0.950.. \n",
      "Epoch: 765/5000..  Training Loss: 0.912..  Test Loss: 0.950.. \n",
      "Epoch: 766/5000..  Training Loss: 0.942..  Test Loss: 0.950.. \n",
      "Epoch: 767/5000..  Training Loss: 0.960..  Test Loss: 0.950.. \n",
      "Epoch: 768/5000..  Training Loss: 0.958..  Test Loss: 0.950.. \n",
      "Epoch: 769/5000..  Training Loss: 0.933..  Test Loss: 0.950.. \n",
      "Epoch: 770/5000..  Training Loss: 0.956..  Test Loss: 0.950.. \n",
      "Epoch: 771/5000..  Training Loss: 0.952..  Test Loss: 0.950.. \n",
      "Epoch: 772/5000..  Training Loss: 0.919..  Test Loss: 0.950.. \n",
      "Epoch: 773/5000..  Training Loss: 0.915..  Test Loss: 0.950.. \n",
      "Epoch: 774/5000..  Training Loss: 0.934..  Test Loss: 0.950.. \n",
      "Epoch: 775/5000..  Training Loss: 0.925..  Test Loss: 0.950.. \n",
      "Epoch: 776/5000..  Training Loss: 0.942..  Test Loss: 0.950.. \n",
      "Epoch: 777/5000..  Training Loss: 0.927..  Test Loss: 0.950.. \n",
      "Epoch: 778/5000..  Training Loss: 0.945..  Test Loss: 0.950.. \n",
      "Epoch: 779/5000..  Training Loss: 0.945..  Test Loss: 0.950.. \n",
      "Epoch: 780/5000..  Training Loss: 0.970..  Test Loss: 0.950.. \n",
      "Epoch: 781/5000..  Training Loss: 0.928..  Test Loss: 0.950.. \n",
      "Epoch: 782/5000..  Training Loss: 0.934..  Test Loss: 0.950.. \n",
      "Epoch: 783/5000..  Training Loss: 0.966..  Test Loss: 0.950.. \n",
      "Epoch: 784/5000..  Training Loss: 0.929..  Test Loss: 0.950.. \n",
      "Epoch: 785/5000..  Training Loss: 0.969..  Test Loss: 0.950.. \n",
      "Epoch: 786/5000..  Training Loss: 0.953..  Test Loss: 0.950.. \n",
      "Epoch: 787/5000..  Training Loss: 0.969..  Test Loss: 0.950.. \n",
      "Epoch: 788/5000..  Training Loss: 0.933..  Test Loss: 0.950.. \n",
      "Epoch: 789/5000..  Training Loss: 0.962..  Test Loss: 0.950.. \n",
      "Epoch: 790/5000..  Training Loss: 0.959..  Test Loss: 0.950.. \n",
      "Epoch: 791/5000..  Training Loss: 0.925..  Test Loss: 0.950.. \n",
      "Epoch: 792/5000..  Training Loss: 0.939..  Test Loss: 0.950.. \n",
      "Epoch: 793/5000..  Training Loss: 0.969..  Test Loss: 0.950.. \n",
      "Epoch: 794/5000..  Training Loss: 0.963..  Test Loss: 0.950.. \n",
      "Epoch: 795/5000..  Training Loss: 0.969..  Test Loss: 0.950.. \n",
      "Epoch: 796/5000..  Training Loss: 0.940..  Test Loss: 0.950.. \n",
      "Epoch: 797/5000..  Training Loss: 0.955..  Test Loss: 0.950.. \n",
      "Epoch: 798/5000..  Training Loss: 0.936..  Test Loss: 0.950.. \n",
      "Epoch: 799/5000..  Training Loss: 0.946..  Test Loss: 0.950.. \n",
      "Epoch: 800/5000..  Training Loss: 0.936..  Test Loss: 0.950.. \n",
      "Epoch: 801/5000..  Training Loss: 0.962..  Test Loss: 0.950.. \n",
      "Epoch: 802/5000..  Training Loss: 0.959..  Test Loss: 0.950.. \n",
      "Epoch: 803/5000..  Training Loss: 0.923..  Test Loss: 0.950.. \n",
      "Epoch: 804/5000..  Training Loss: 0.976..  Test Loss: 0.950.. \n",
      "Epoch: 805/5000..  Training Loss: 0.953..  Test Loss: 0.950.. \n",
      "Epoch: 806/5000..  Training Loss: 0.976..  Test Loss: 0.950.. \n",
      "Epoch: 807/5000..  Training Loss: 0.936..  Test Loss: 0.950.. \n",
      "Epoch: 808/5000..  Training Loss: 0.932..  Test Loss: 0.950.. \n",
      "Epoch: 809/5000..  Training Loss: 0.934..  Test Loss: 0.950.. \n",
      "Epoch: 810/5000..  Training Loss: 0.956..  Test Loss: 0.950.. \n",
      "Epoch: 811/5000..  Training Loss: 0.960..  Test Loss: 0.950.. \n",
      "Epoch: 812/5000..  Training Loss: 0.981..  Test Loss: 0.950.. \n",
      "Epoch: 813/5000..  Training Loss: 0.968..  Test Loss: 0.950.. \n",
      "Epoch: 814/5000..  Training Loss: 0.920..  Test Loss: 0.950.. \n",
      "Epoch: 815/5000..  Training Loss: 0.939..  Test Loss: 0.950.. \n",
      "Epoch: 816/5000..  Training Loss: 0.944..  Test Loss: 0.950.. \n",
      "Epoch: 817/5000..  Training Loss: 0.929..  Test Loss: 0.950.. \n",
      "Epoch: 818/5000..  Training Loss: 0.954..  Test Loss: 0.950.. \n",
      "Epoch: 819/5000..  Training Loss: 0.955..  Test Loss: 0.950.. \n",
      "Epoch: 820/5000..  Training Loss: 0.928..  Test Loss: 0.950.. \n",
      "Epoch: 821/5000..  Training Loss: 0.938..  Test Loss: 0.950.. \n",
      "Epoch: 822/5000..  Training Loss: 0.928..  Test Loss: 0.950.. \n",
      "Epoch: 823/5000..  Training Loss: 0.936..  Test Loss: 0.950.. \n",
      "Epoch: 824/5000..  Training Loss: 0.951..  Test Loss: 0.950.. \n",
      "Epoch: 825/5000..  Training Loss: 0.958..  Test Loss: 0.950.. \n",
      "Epoch: 826/5000..  Training Loss: 0.950..  Test Loss: 0.950.. \n",
      "Epoch: 827/5000..  Training Loss: 0.945..  Test Loss: 0.950.. \n",
      "Epoch: 828/5000..  Training Loss: 0.958..  Test Loss: 0.950.. \n",
      "Epoch: 829/5000..  Training Loss: 0.964..  Test Loss: 0.950.. \n",
      "Epoch: 830/5000..  Training Loss: 0.961..  Test Loss: 0.950.. \n",
      "Epoch: 831/5000..  Training Loss: 0.983..  Test Loss: 0.949.. \n",
      "Epoch: 832/5000..  Training Loss: 0.968..  Test Loss: 0.949.. \n",
      "Epoch: 833/5000..  Training Loss: 0.965..  Test Loss: 0.949.. \n",
      "Epoch: 834/5000..  Training Loss: 0.967..  Test Loss: 0.949.. \n",
      "Epoch: 835/5000..  Training Loss: 0.923..  Test Loss: 0.949.. \n",
      "Epoch: 836/5000..  Training Loss: 0.984..  Test Loss: 0.949.. \n",
      "Epoch: 837/5000..  Training Loss: 0.933..  Test Loss: 0.949.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 838/5000..  Training Loss: 0.934..  Test Loss: 0.949.. \n",
      "Epoch: 839/5000..  Training Loss: 0.937..  Test Loss: 0.949.. \n",
      "Epoch: 840/5000..  Training Loss: 0.925..  Test Loss: 0.949.. \n",
      "Epoch: 841/5000..  Training Loss: 0.977..  Test Loss: 0.949.. \n",
      "Epoch: 842/5000..  Training Loss: 0.940..  Test Loss: 0.949.. \n",
      "Epoch: 843/5000..  Training Loss: 0.932..  Test Loss: 0.949.. \n",
      "Epoch: 844/5000..  Training Loss: 0.966..  Test Loss: 0.949.. \n",
      "Epoch: 845/5000..  Training Loss: 0.931..  Test Loss: 0.949.. \n",
      "Epoch: 846/5000..  Training Loss: 0.965..  Test Loss: 0.949.. \n",
      "Epoch: 847/5000..  Training Loss: 0.961..  Test Loss: 0.949.. \n",
      "Epoch: 848/5000..  Training Loss: 0.971..  Test Loss: 0.949.. \n",
      "Epoch: 849/5000..  Training Loss: 0.923..  Test Loss: 0.949.. \n",
      "Epoch: 850/5000..  Training Loss: 0.934..  Test Loss: 0.949.. \n",
      "Epoch: 851/5000..  Training Loss: 0.930..  Test Loss: 0.949.. \n",
      "Epoch: 852/5000..  Training Loss: 0.943..  Test Loss: 0.949.. \n",
      "Epoch: 853/5000..  Training Loss: 0.959..  Test Loss: 0.949.. \n",
      "Epoch: 854/5000..  Training Loss: 0.950..  Test Loss: 0.949.. \n",
      "Epoch: 855/5000..  Training Loss: 0.946..  Test Loss: 0.949.. \n",
      "Epoch: 856/5000..  Training Loss: 0.958..  Test Loss: 0.949.. \n",
      "Epoch: 857/5000..  Training Loss: 0.955..  Test Loss: 0.949.. \n",
      "Epoch: 858/5000..  Training Loss: 0.922..  Test Loss: 0.949.. \n",
      "Epoch: 859/5000..  Training Loss: 0.933..  Test Loss: 0.949.. \n",
      "Epoch: 860/5000..  Training Loss: 0.955..  Test Loss: 0.949.. \n",
      "Epoch: 861/5000..  Training Loss: 0.987..  Test Loss: 0.949.. \n",
      "Epoch: 862/5000..  Training Loss: 0.967..  Test Loss: 0.949.. \n",
      "Epoch: 863/5000..  Training Loss: 0.959..  Test Loss: 0.949.. \n",
      "Epoch: 864/5000..  Training Loss: 0.928..  Test Loss: 0.949.. \n",
      "Epoch: 865/5000..  Training Loss: 0.924..  Test Loss: 0.949.. \n",
      "Epoch: 866/5000..  Training Loss: 0.925..  Test Loss: 0.949.. \n",
      "Epoch: 867/5000..  Training Loss: 0.931..  Test Loss: 0.949.. \n",
      "Epoch: 868/5000..  Training Loss: 0.947..  Test Loss: 0.949.. \n",
      "Epoch: 869/5000..  Training Loss: 0.908..  Test Loss: 0.949.. \n",
      "Epoch: 870/5000..  Training Loss: 0.940..  Test Loss: 0.949.. \n",
      "Epoch: 871/5000..  Training Loss: 0.972..  Test Loss: 0.949.. \n",
      "Epoch: 872/5000..  Training Loss: 0.958..  Test Loss: 0.949.. \n",
      "Epoch: 873/5000..  Training Loss: 0.940..  Test Loss: 0.949.. \n",
      "Epoch: 874/5000..  Training Loss: 0.955..  Test Loss: 0.949.. \n",
      "Epoch: 875/5000..  Training Loss: 0.963..  Test Loss: 0.949.. \n",
      "Epoch: 876/5000..  Training Loss: 0.962..  Test Loss: 0.949.. \n",
      "Epoch: 877/5000..  Training Loss: 0.950..  Test Loss: 0.949.. \n",
      "Epoch: 878/5000..  Training Loss: 0.941..  Test Loss: 0.949.. \n",
      "Epoch: 879/5000..  Training Loss: 0.959..  Test Loss: 0.949.. \n",
      "Epoch: 880/5000..  Training Loss: 0.946..  Test Loss: 0.949.. \n",
      "Epoch: 881/5000..  Training Loss: 0.910..  Test Loss: 0.949.. \n",
      "Epoch: 882/5000..  Training Loss: 0.965..  Test Loss: 0.949.. \n",
      "Epoch: 883/5000..  Training Loss: 0.925..  Test Loss: 0.949.. \n",
      "Epoch: 884/5000..  Training Loss: 0.944..  Test Loss: 0.949.. \n",
      "Epoch: 885/5000..  Training Loss: 0.935..  Test Loss: 0.949.. \n",
      "Epoch: 886/5000..  Training Loss: 0.940..  Test Loss: 0.949.. \n",
      "Epoch: 887/5000..  Training Loss: 0.950..  Test Loss: 0.949.. \n",
      "Epoch: 888/5000..  Training Loss: 0.939..  Test Loss: 0.949.. \n",
      "Epoch: 889/5000..  Training Loss: 0.985..  Test Loss: 0.949.. \n",
      "Epoch: 890/5000..  Training Loss: 0.977..  Test Loss: 0.949.. \n",
      "Epoch: 891/5000..  Training Loss: 0.956..  Test Loss: 0.949.. \n",
      "Epoch: 892/5000..  Training Loss: 0.924..  Test Loss: 0.949.. \n",
      "Epoch: 893/5000..  Training Loss: 0.920..  Test Loss: 0.949.. \n",
      "Epoch: 894/5000..  Training Loss: 0.935..  Test Loss: 0.949.. \n",
      "Epoch: 895/5000..  Training Loss: 0.949..  Test Loss: 0.949.. \n",
      "Epoch: 896/5000..  Training Loss: 0.948..  Test Loss: 0.949.. \n",
      "Epoch: 897/5000..  Training Loss: 0.966..  Test Loss: 0.949.. \n",
      "Epoch: 898/5000..  Training Loss: 0.938..  Test Loss: 0.949.. \n",
      "Epoch: 899/5000..  Training Loss: 0.961..  Test Loss: 0.949.. \n",
      "Epoch: 900/5000..  Training Loss: 0.984..  Test Loss: 0.949.. \n",
      "Epoch: 901/5000..  Training Loss: 0.935..  Test Loss: 0.949.. \n",
      "Epoch: 902/5000..  Training Loss: 0.953..  Test Loss: 0.949.. \n",
      "Epoch: 903/5000..  Training Loss: 0.961..  Test Loss: 0.949.. \n",
      "Epoch: 904/5000..  Training Loss: 0.960..  Test Loss: 0.949.. \n",
      "Epoch: 905/5000..  Training Loss: 0.927..  Test Loss: 0.949.. \n",
      "Epoch: 906/5000..  Training Loss: 0.935..  Test Loss: 0.949.. \n",
      "Epoch: 907/5000..  Training Loss: 0.926..  Test Loss: 0.949.. \n",
      "Epoch: 908/5000..  Training Loss: 0.926..  Test Loss: 0.949.. \n",
      "Epoch: 909/5000..  Training Loss: 0.938..  Test Loss: 0.949.. \n",
      "Epoch: 910/5000..  Training Loss: 0.940..  Test Loss: 0.949.. \n",
      "Epoch: 911/5000..  Training Loss: 0.952..  Test Loss: 0.949.. \n",
      "Epoch: 912/5000..  Training Loss: 0.950..  Test Loss: 0.949.. \n",
      "Epoch: 913/5000..  Training Loss: 0.961..  Test Loss: 0.949.. \n",
      "Epoch: 914/5000..  Training Loss: 0.992..  Test Loss: 0.949.. \n",
      "Epoch: 915/5000..  Training Loss: 0.951..  Test Loss: 0.949.. \n",
      "Epoch: 916/5000..  Training Loss: 0.947..  Test Loss: 0.949.. \n",
      "Epoch: 917/5000..  Training Loss: 0.955..  Test Loss: 0.949.. \n",
      "Epoch: 918/5000..  Training Loss: 0.927..  Test Loss: 0.949.. \n",
      "Epoch: 919/5000..  Training Loss: 0.973..  Test Loss: 0.949.. \n",
      "Epoch: 920/5000..  Training Loss: 0.926..  Test Loss: 0.949.. \n",
      "Epoch: 921/5000..  Training Loss: 0.932..  Test Loss: 0.949.. \n",
      "Epoch: 922/5000..  Training Loss: 0.928..  Test Loss: 0.949.. \n",
      "Epoch: 923/5000..  Training Loss: 0.953..  Test Loss: 0.949.. \n",
      "Epoch: 924/5000..  Training Loss: 0.978..  Test Loss: 0.949.. \n",
      "Epoch: 925/5000..  Training Loss: 0.931..  Test Loss: 0.949.. \n",
      "Epoch: 926/5000..  Training Loss: 0.934..  Test Loss: 0.949.. \n",
      "Epoch: 927/5000..  Training Loss: 0.942..  Test Loss: 0.949.. \n",
      "Epoch: 928/5000..  Training Loss: 0.973..  Test Loss: 0.949.. \n",
      "Epoch: 929/5000..  Training Loss: 0.922..  Test Loss: 0.949.. \n",
      "Epoch: 930/5000..  Training Loss: 0.932..  Test Loss: 0.949.. \n",
      "Epoch: 931/5000..  Training Loss: 0.962..  Test Loss: 0.949.. \n",
      "Epoch: 932/5000..  Training Loss: 0.914..  Test Loss: 0.949.. \n",
      "Epoch: 933/5000..  Training Loss: 0.928..  Test Loss: 0.949.. \n",
      "Epoch: 934/5000..  Training Loss: 0.916..  Test Loss: 0.949.. \n",
      "Epoch: 935/5000..  Training Loss: 0.905..  Test Loss: 0.949.. \n",
      "Epoch: 936/5000..  Training Loss: 0.936..  Test Loss: 0.948.. \n",
      "Epoch: 937/5000..  Training Loss: 0.946..  Test Loss: 0.948.. \n",
      "Epoch: 938/5000..  Training Loss: 1.010..  Test Loss: 0.948.. \n",
      "Epoch: 939/5000..  Training Loss: 0.948..  Test Loss: 0.948.. \n",
      "Epoch: 940/5000..  Training Loss: 0.995..  Test Loss: 0.948.. \n",
      "Epoch: 941/5000..  Training Loss: 0.935..  Test Loss: 0.948.. \n",
      "Epoch: 942/5000..  Training Loss: 0.911..  Test Loss: 0.948.. \n",
      "Epoch: 943/5000..  Training Loss: 0.916..  Test Loss: 0.948.. \n",
      "Epoch: 944/5000..  Training Loss: 0.936..  Test Loss: 0.948.. \n",
      "Epoch: 945/5000..  Training Loss: 0.949..  Test Loss: 0.948.. \n",
      "Epoch: 946/5000..  Training Loss: 0.955..  Test Loss: 0.948.. \n",
      "Epoch: 947/5000..  Training Loss: 0.962..  Test Loss: 0.948.. \n",
      "Epoch: 948/5000..  Training Loss: 0.951..  Test Loss: 0.948.. \n",
      "Epoch: 949/5000..  Training Loss: 0.976..  Test Loss: 0.948.. \n",
      "Epoch: 950/5000..  Training Loss: 0.935..  Test Loss: 0.948.. \n",
      "Epoch: 951/5000..  Training Loss: 0.956..  Test Loss: 0.948.. \n",
      "Epoch: 952/5000..  Training Loss: 0.961..  Test Loss: 0.948.. \n",
      "Epoch: 953/5000..  Training Loss: 0.935..  Test Loss: 0.948.. \n",
      "Epoch: 954/5000..  Training Loss: 0.952..  Test Loss: 0.948.. \n",
      "Epoch: 955/5000..  Training Loss: 0.950..  Test Loss: 0.948.. \n",
      "Epoch: 956/5000..  Training Loss: 0.959..  Test Loss: 0.948.. \n",
      "Epoch: 957/5000..  Training Loss: 0.968..  Test Loss: 0.948.. \n",
      "Epoch: 958/5000..  Training Loss: 0.968..  Test Loss: 0.948.. \n",
      "Epoch: 959/5000..  Training Loss: 0.967..  Test Loss: 0.948.. \n",
      "Epoch: 960/5000..  Training Loss: 0.945..  Test Loss: 0.948.. \n",
      "Epoch: 961/5000..  Training Loss: 0.939..  Test Loss: 0.948.. \n",
      "Epoch: 962/5000..  Training Loss: 0.972..  Test Loss: 0.948.. \n",
      "Epoch: 963/5000..  Training Loss: 0.960..  Test Loss: 0.948.. \n",
      "Epoch: 964/5000..  Training Loss: 0.963..  Test Loss: 0.948.. \n",
      "Epoch: 965/5000..  Training Loss: 0.968..  Test Loss: 0.948.. \n",
      "Epoch: 966/5000..  Training Loss: 0.947..  Test Loss: 0.948.. \n",
      "Epoch: 967/5000..  Training Loss: 0.980..  Test Loss: 0.948.. \n",
      "Epoch: 968/5000..  Training Loss: 0.948..  Test Loss: 0.948.. \n",
      "Epoch: 969/5000..  Training Loss: 0.970..  Test Loss: 0.948.. \n",
      "Epoch: 970/5000..  Training Loss: 0.958..  Test Loss: 0.948.. \n",
      "Epoch: 971/5000..  Training Loss: 0.921..  Test Loss: 0.948.. \n",
      "Epoch: 972/5000..  Training Loss: 0.931..  Test Loss: 0.948.. \n",
      "Epoch: 973/5000..  Training Loss: 0.954..  Test Loss: 0.948.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 974/5000..  Training Loss: 0.931..  Test Loss: 0.948.. \n",
      "Epoch: 975/5000..  Training Loss: 0.934..  Test Loss: 0.948.. \n",
      "Epoch: 976/5000..  Training Loss: 0.971..  Test Loss: 0.948.. \n",
      "Epoch: 977/5000..  Training Loss: 0.966..  Test Loss: 0.948.. \n",
      "Epoch: 978/5000..  Training Loss: 0.959..  Test Loss: 0.948.. \n",
      "Epoch: 979/5000..  Training Loss: 0.936..  Test Loss: 0.948.. \n",
      "Epoch: 980/5000..  Training Loss: 0.981..  Test Loss: 0.948.. \n",
      "Epoch: 981/5000..  Training Loss: 0.948..  Test Loss: 0.948.. \n",
      "Epoch: 982/5000..  Training Loss: 0.922..  Test Loss: 0.948.. \n",
      "Epoch: 983/5000..  Training Loss: 0.967..  Test Loss: 0.948.. \n",
      "Epoch: 984/5000..  Training Loss: 0.946..  Test Loss: 0.948.. \n",
      "Epoch: 985/5000..  Training Loss: 0.961..  Test Loss: 0.948.. \n",
      "Epoch: 986/5000..  Training Loss: 0.964..  Test Loss: 0.948.. \n",
      "Epoch: 987/5000..  Training Loss: 0.959..  Test Loss: 0.948.. \n",
      "Epoch: 988/5000..  Training Loss: 0.969..  Test Loss: 0.948.. \n",
      "Epoch: 989/5000..  Training Loss: 0.972..  Test Loss: 0.948.. \n",
      "Epoch: 990/5000..  Training Loss: 0.935..  Test Loss: 0.948.. \n",
      "Epoch: 991/5000..  Training Loss: 0.933..  Test Loss: 0.948.. \n",
      "Epoch: 992/5000..  Training Loss: 0.972..  Test Loss: 0.948.. \n",
      "Epoch: 993/5000..  Training Loss: 0.941..  Test Loss: 0.948.. \n",
      "Epoch: 994/5000..  Training Loss: 0.986..  Test Loss: 0.948.. \n",
      "Epoch: 995/5000..  Training Loss: 0.913..  Test Loss: 0.948.. \n",
      "Epoch: 996/5000..  Training Loss: 0.965..  Test Loss: 0.948.. \n",
      "Epoch: 997/5000..  Training Loss: 0.936..  Test Loss: 0.948.. \n",
      "Epoch: 998/5000..  Training Loss: 0.974..  Test Loss: 0.948.. \n",
      "Epoch: 999/5000..  Training Loss: 0.931..  Test Loss: 0.948.. \n",
      "Epoch: 1000/5000..  Training Loss: 0.958..  Test Loss: 0.948.. \n",
      "Epoch: 1001/5000..  Training Loss: 0.971..  Test Loss: 0.948.. \n",
      "Epoch: 1002/5000..  Training Loss: 0.943..  Test Loss: 0.948.. \n",
      "Epoch: 1003/5000..  Training Loss: 0.969..  Test Loss: 0.948.. \n",
      "Epoch: 1004/5000..  Training Loss: 0.930..  Test Loss: 0.948.. \n",
      "Epoch: 1005/5000..  Training Loss: 0.937..  Test Loss: 0.948.. \n",
      "Epoch: 1006/5000..  Training Loss: 0.956..  Test Loss: 0.948.. \n",
      "Epoch: 1007/5000..  Training Loss: 0.949..  Test Loss: 0.948.. \n",
      "Epoch: 1008/5000..  Training Loss: 0.901..  Test Loss: 0.948.. \n",
      "Epoch: 1009/5000..  Training Loss: 0.941..  Test Loss: 0.948.. \n",
      "Epoch: 1010/5000..  Training Loss: 0.970..  Test Loss: 0.948.. \n",
      "Epoch: 1011/5000..  Training Loss: 0.943..  Test Loss: 0.948.. \n",
      "Epoch: 1012/5000..  Training Loss: 0.966..  Test Loss: 0.948.. \n",
      "Epoch: 1013/5000..  Training Loss: 0.962..  Test Loss: 0.948.. \n",
      "Epoch: 1014/5000..  Training Loss: 0.928..  Test Loss: 0.948.. \n",
      "Epoch: 1015/5000..  Training Loss: 0.937..  Test Loss: 0.948.. \n",
      "Epoch: 1016/5000..  Training Loss: 0.922..  Test Loss: 0.948.. \n",
      "Epoch: 1017/5000..  Training Loss: 0.932..  Test Loss: 0.948.. \n",
      "Epoch: 1018/5000..  Training Loss: 0.949..  Test Loss: 0.948.. \n",
      "Epoch: 1019/5000..  Training Loss: 0.951..  Test Loss: 0.948.. \n",
      "Epoch: 1020/5000..  Training Loss: 0.917..  Test Loss: 0.948.. \n",
      "Epoch: 1021/5000..  Training Loss: 0.941..  Test Loss: 0.948.. \n",
      "Epoch: 1022/5000..  Training Loss: 0.977..  Test Loss: 0.948.. \n",
      "Epoch: 1023/5000..  Training Loss: 0.979..  Test Loss: 0.948.. \n",
      "Epoch: 1024/5000..  Training Loss: 0.981..  Test Loss: 0.948.. \n",
      "Epoch: 1025/5000..  Training Loss: 0.919..  Test Loss: 0.948.. \n",
      "Epoch: 1026/5000..  Training Loss: 0.931..  Test Loss: 0.948.. \n",
      "Epoch: 1027/5000..  Training Loss: 0.914..  Test Loss: 0.948.. \n",
      "Epoch: 1028/5000..  Training Loss: 0.942..  Test Loss: 0.948.. \n",
      "Epoch: 1029/5000..  Training Loss: 0.981..  Test Loss: 0.948.. \n",
      "Epoch: 1030/5000..  Training Loss: 0.927..  Test Loss: 0.948.. \n",
      "Epoch: 1031/5000..  Training Loss: 0.955..  Test Loss: 0.948.. \n",
      "Epoch: 1032/5000..  Training Loss: 0.917..  Test Loss: 0.948.. \n",
      "Epoch: 1033/5000..  Training Loss: 0.958..  Test Loss: 0.948.. \n",
      "Epoch: 1034/5000..  Training Loss: 0.931..  Test Loss: 0.948.. \n",
      "Epoch: 1035/5000..  Training Loss: 0.954..  Test Loss: 0.948.. \n",
      "Epoch: 1036/5000..  Training Loss: 0.933..  Test Loss: 0.948.. \n",
      "Epoch: 1037/5000..  Training Loss: 0.927..  Test Loss: 0.948.. \n",
      "Epoch: 1038/5000..  Training Loss: 0.909..  Test Loss: 0.948.. \n",
      "Epoch: 1039/5000..  Training Loss: 0.966..  Test Loss: 0.948.. \n",
      "Epoch: 1040/5000..  Training Loss: 0.958..  Test Loss: 0.948.. \n",
      "Epoch: 1041/5000..  Training Loss: 0.942..  Test Loss: 0.948.. \n",
      "Epoch: 1042/5000..  Training Loss: 0.909..  Test Loss: 0.948.. \n",
      "Epoch: 1043/5000..  Training Loss: 0.949..  Test Loss: 0.948.. \n",
      "Epoch: 1044/5000..  Training Loss: 0.924..  Test Loss: 0.948.. \n",
      "Epoch: 1045/5000..  Training Loss: 0.967..  Test Loss: 0.948.. \n",
      "Epoch: 1046/5000..  Training Loss: 0.951..  Test Loss: 0.947.. \n",
      "Epoch: 1047/5000..  Training Loss: 0.934..  Test Loss: 0.947.. \n",
      "Epoch: 1048/5000..  Training Loss: 0.932..  Test Loss: 0.947.. \n",
      "Epoch: 1049/5000..  Training Loss: 0.919..  Test Loss: 0.947.. \n",
      "Epoch: 1050/5000..  Training Loss: 0.937..  Test Loss: 0.947.. \n",
      "Epoch: 1051/5000..  Training Loss: 0.963..  Test Loss: 0.947.. \n",
      "Epoch: 1052/5000..  Training Loss: 0.924..  Test Loss: 0.947.. \n",
      "Epoch: 1053/5000..  Training Loss: 0.932..  Test Loss: 0.947.. \n",
      "Epoch: 1054/5000..  Training Loss: 0.946..  Test Loss: 0.947.. \n",
      "Epoch: 1055/5000..  Training Loss: 0.957..  Test Loss: 0.947.. \n",
      "Epoch: 1056/5000..  Training Loss: 0.921..  Test Loss: 0.947.. \n",
      "Epoch: 1057/5000..  Training Loss: 0.981..  Test Loss: 0.947.. \n",
      "Epoch: 1058/5000..  Training Loss: 0.970..  Test Loss: 0.947.. \n",
      "Epoch: 1059/5000..  Training Loss: 0.934..  Test Loss: 0.947.. \n",
      "Epoch: 1060/5000..  Training Loss: 0.957..  Test Loss: 0.947.. \n",
      "Epoch: 1061/5000..  Training Loss: 0.937..  Test Loss: 0.947.. \n",
      "Epoch: 1062/5000..  Training Loss: 0.943..  Test Loss: 0.947.. \n",
      "Epoch: 1063/5000..  Training Loss: 0.955..  Test Loss: 0.947.. \n",
      "Epoch: 1064/5000..  Training Loss: 0.948..  Test Loss: 0.947.. \n",
      "Epoch: 1065/5000..  Training Loss: 0.987..  Test Loss: 0.947.. \n",
      "Epoch: 1066/5000..  Training Loss: 0.956..  Test Loss: 0.947.. \n",
      "Epoch: 1067/5000..  Training Loss: 0.953..  Test Loss: 0.947.. \n",
      "Epoch: 1068/5000..  Training Loss: 0.975..  Test Loss: 0.947.. \n",
      "Epoch: 1069/5000..  Training Loss: 0.920..  Test Loss: 0.947.. \n",
      "Epoch: 1070/5000..  Training Loss: 0.942..  Test Loss: 0.947.. \n",
      "Epoch: 1071/5000..  Training Loss: 0.935..  Test Loss: 0.947.. \n",
      "Epoch: 1072/5000..  Training Loss: 0.951..  Test Loss: 0.947.. \n",
      "Epoch: 1073/5000..  Training Loss: 0.957..  Test Loss: 0.947.. \n",
      "Epoch: 1074/5000..  Training Loss: 0.948..  Test Loss: 0.947.. \n",
      "Epoch: 1075/5000..  Training Loss: 0.919..  Test Loss: 0.947.. \n",
      "Epoch: 1076/5000..  Training Loss: 0.940..  Test Loss: 0.947.. \n",
      "Epoch: 1077/5000..  Training Loss: 0.947..  Test Loss: 0.947.. \n",
      "Epoch: 1078/5000..  Training Loss: 0.926..  Test Loss: 0.947.. \n",
      "Epoch: 1079/5000..  Training Loss: 0.960..  Test Loss: 0.947.. \n",
      "Epoch: 1080/5000..  Training Loss: 0.947..  Test Loss: 0.947.. \n",
      "Epoch: 1081/5000..  Training Loss: 0.962..  Test Loss: 0.947.. \n",
      "Epoch: 1082/5000..  Training Loss: 0.985..  Test Loss: 0.947.. \n",
      "Epoch: 1083/5000..  Training Loss: 0.949..  Test Loss: 0.947.. \n",
      "Epoch: 1084/5000..  Training Loss: 0.961..  Test Loss: 0.947.. \n",
      "Epoch: 1085/5000..  Training Loss: 0.939..  Test Loss: 0.947.. \n",
      "Epoch: 1086/5000..  Training Loss: 0.925..  Test Loss: 0.947.. \n",
      "Epoch: 1087/5000..  Training Loss: 0.961..  Test Loss: 0.947.. \n",
      "Epoch: 1088/5000..  Training Loss: 0.948..  Test Loss: 0.947.. \n",
      "Epoch: 1089/5000..  Training Loss: 0.948..  Test Loss: 0.947.. \n",
      "Epoch: 1090/5000..  Training Loss: 0.937..  Test Loss: 0.947.. \n",
      "Epoch: 1091/5000..  Training Loss: 0.912..  Test Loss: 0.947.. \n",
      "Epoch: 1092/5000..  Training Loss: 0.935..  Test Loss: 0.947.. \n",
      "Epoch: 1093/5000..  Training Loss: 0.956..  Test Loss: 0.947.. \n",
      "Epoch: 1094/5000..  Training Loss: 0.955..  Test Loss: 0.947.. \n",
      "Epoch: 1095/5000..  Training Loss: 0.950..  Test Loss: 0.947.. \n",
      "Epoch: 1096/5000..  Training Loss: 0.896..  Test Loss: 0.947.. \n",
      "Epoch: 1097/5000..  Training Loss: 0.978..  Test Loss: 0.947.. \n",
      "Epoch: 1098/5000..  Training Loss: 0.950..  Test Loss: 0.947.. \n",
      "Epoch: 1099/5000..  Training Loss: 0.952..  Test Loss: 0.947.. \n",
      "Epoch: 1100/5000..  Training Loss: 0.930..  Test Loss: 0.947.. \n",
      "Epoch: 1101/5000..  Training Loss: 0.961..  Test Loss: 0.947.. \n",
      "Epoch: 1102/5000..  Training Loss: 0.935..  Test Loss: 0.947.. \n",
      "Epoch: 1103/5000..  Training Loss: 0.921..  Test Loss: 0.947.. \n",
      "Epoch: 1104/5000..  Training Loss: 0.938..  Test Loss: 0.947.. \n",
      "Epoch: 1105/5000..  Training Loss: 0.967..  Test Loss: 0.947.. \n",
      "Epoch: 1106/5000..  Training Loss: 0.955..  Test Loss: 0.947.. \n",
      "Epoch: 1107/5000..  Training Loss: 0.950..  Test Loss: 0.947.. \n",
      "Epoch: 1108/5000..  Training Loss: 0.997..  Test Loss: 0.947.. \n",
      "Epoch: 1109/5000..  Training Loss: 0.929..  Test Loss: 0.947.. \n",
      "Epoch: 1110/5000..  Training Loss: 0.916..  Test Loss: 0.947.. \n",
      "Epoch: 1111/5000..  Training Loss: 0.941..  Test Loss: 0.947.. \n",
      "Epoch: 1112/5000..  Training Loss: 0.959..  Test Loss: 0.947.. \n",
      "Epoch: 1113/5000..  Training Loss: 0.946..  Test Loss: 0.947.. \n",
      "Epoch: 1114/5000..  Training Loss: 0.940..  Test Loss: 0.947.. \n",
      "Epoch: 1115/5000..  Training Loss: 0.972..  Test Loss: 0.947.. \n",
      "Epoch: 1116/5000..  Training Loss: 0.938..  Test Loss: 0.947.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1117/5000..  Training Loss: 0.929..  Test Loss: 0.947.. \n",
      "Epoch: 1118/5000..  Training Loss: 0.961..  Test Loss: 0.947.. \n",
      "Epoch: 1119/5000..  Training Loss: 0.914..  Test Loss: 0.947.. \n",
      "Epoch: 1120/5000..  Training Loss: 0.924..  Test Loss: 0.947.. \n",
      "Epoch: 1121/5000..  Training Loss: 0.932..  Test Loss: 0.947.. \n",
      "Epoch: 1122/5000..  Training Loss: 0.927..  Test Loss: 0.947.. \n",
      "Epoch: 1123/5000..  Training Loss: 0.921..  Test Loss: 0.947.. \n",
      "Epoch: 1124/5000..  Training Loss: 0.953..  Test Loss: 0.947.. \n",
      "Epoch: 1125/5000..  Training Loss: 0.971..  Test Loss: 0.947.. \n",
      "Epoch: 1126/5000..  Training Loss: 0.944..  Test Loss: 0.947.. \n",
      "Epoch: 1127/5000..  Training Loss: 0.970..  Test Loss: 0.947.. \n",
      "Epoch: 1128/5000..  Training Loss: 0.917..  Test Loss: 0.947.. \n",
      "Epoch: 1129/5000..  Training Loss: 0.925..  Test Loss: 0.947.. \n",
      "Epoch: 1130/5000..  Training Loss: 0.921..  Test Loss: 0.947.. \n",
      "Epoch: 1131/5000..  Training Loss: 0.934..  Test Loss: 0.947.. \n",
      "Epoch: 1132/5000..  Training Loss: 0.953..  Test Loss: 0.947.. \n",
      "Epoch: 1133/5000..  Training Loss: 0.922..  Test Loss: 0.947.. \n",
      "Epoch: 1134/5000..  Training Loss: 0.950..  Test Loss: 0.947.. \n",
      "Epoch: 1135/5000..  Training Loss: 0.963..  Test Loss: 0.947.. \n",
      "Epoch: 1136/5000..  Training Loss: 0.971..  Test Loss: 0.947.. \n",
      "Epoch: 1137/5000..  Training Loss: 0.946..  Test Loss: 0.947.. \n",
      "Epoch: 1138/5000..  Training Loss: 0.937..  Test Loss: 0.947.. \n",
      "Epoch: 1139/5000..  Training Loss: 0.953..  Test Loss: 0.947.. \n",
      "Epoch: 1140/5000..  Training Loss: 0.954..  Test Loss: 0.947.. \n",
      "Epoch: 1141/5000..  Training Loss: 0.907..  Test Loss: 0.947.. \n",
      "Epoch: 1142/5000..  Training Loss: 0.904..  Test Loss: 0.947.. \n",
      "Epoch: 1143/5000..  Training Loss: 0.927..  Test Loss: 0.947.. \n",
      "Epoch: 1144/5000..  Training Loss: 0.994..  Test Loss: 0.947.. \n",
      "Epoch: 1145/5000..  Training Loss: 0.941..  Test Loss: 0.947.. \n",
      "Epoch: 1146/5000..  Training Loss: 0.978..  Test Loss: 0.947.. \n",
      "Epoch: 1147/5000..  Training Loss: 0.976..  Test Loss: 0.947.. \n",
      "Epoch: 1148/5000..  Training Loss: 0.926..  Test Loss: 0.946.. \n",
      "Epoch: 1149/5000..  Training Loss: 0.916..  Test Loss: 0.946.. \n",
      "Epoch: 1150/5000..  Training Loss: 0.974..  Test Loss: 0.946.. \n",
      "Epoch: 1151/5000..  Training Loss: 0.926..  Test Loss: 0.946.. \n",
      "Epoch: 1152/5000..  Training Loss: 0.972..  Test Loss: 0.946.. \n",
      "Epoch: 1153/5000..  Training Loss: 0.930..  Test Loss: 0.946.. \n",
      "Epoch: 1154/5000..  Training Loss: 0.970..  Test Loss: 0.946.. \n",
      "Epoch: 1155/5000..  Training Loss: 0.950..  Test Loss: 0.946.. \n",
      "Epoch: 1156/5000..  Training Loss: 0.947..  Test Loss: 0.946.. \n",
      "Epoch: 1157/5000..  Training Loss: 0.929..  Test Loss: 0.946.. \n",
      "Epoch: 1158/5000..  Training Loss: 0.952..  Test Loss: 0.946.. \n",
      "Epoch: 1159/5000..  Training Loss: 0.935..  Test Loss: 0.946.. \n",
      "Epoch: 1160/5000..  Training Loss: 0.949..  Test Loss: 0.946.. \n",
      "Epoch: 1161/5000..  Training Loss: 0.933..  Test Loss: 0.946.. \n",
      "Epoch: 1162/5000..  Training Loss: 0.958..  Test Loss: 0.946.. \n",
      "Epoch: 1163/5000..  Training Loss: 0.976..  Test Loss: 0.946.. \n",
      "Epoch: 1164/5000..  Training Loss: 0.931..  Test Loss: 0.946.. \n",
      "Epoch: 1165/5000..  Training Loss: 0.945..  Test Loss: 0.946.. \n",
      "Epoch: 1166/5000..  Training Loss: 0.938..  Test Loss: 0.946.. \n",
      "Epoch: 1167/5000..  Training Loss: 0.925..  Test Loss: 0.946.. \n",
      "Epoch: 1168/5000..  Training Loss: 0.935..  Test Loss: 0.946.. \n",
      "Epoch: 1169/5000..  Training Loss: 0.956..  Test Loss: 0.946.. \n",
      "Epoch: 1170/5000..  Training Loss: 0.944..  Test Loss: 0.946.. \n",
      "Epoch: 1171/5000..  Training Loss: 0.973..  Test Loss: 0.946.. \n",
      "Epoch: 1172/5000..  Training Loss: 0.954..  Test Loss: 0.946.. \n",
      "Epoch: 1173/5000..  Training Loss: 0.945..  Test Loss: 0.946.. \n",
      "Epoch: 1174/5000..  Training Loss: 0.920..  Test Loss: 0.946.. \n",
      "Epoch: 1175/5000..  Training Loss: 0.954..  Test Loss: 0.946.. \n",
      "Epoch: 1176/5000..  Training Loss: 0.933..  Test Loss: 0.946.. \n",
      "Epoch: 1177/5000..  Training Loss: 0.954..  Test Loss: 0.946.. \n",
      "Epoch: 1178/5000..  Training Loss: 0.941..  Test Loss: 0.946.. \n",
      "Epoch: 1179/5000..  Training Loss: 0.919..  Test Loss: 0.946.. \n",
      "Epoch: 1180/5000..  Training Loss: 0.963..  Test Loss: 0.946.. \n",
      "Epoch: 1181/5000..  Training Loss: 0.953..  Test Loss: 0.946.. \n",
      "Epoch: 1182/5000..  Training Loss: 0.950..  Test Loss: 0.946.. \n",
      "Epoch: 1183/5000..  Training Loss: 0.965..  Test Loss: 0.946.. \n",
      "Epoch: 1184/5000..  Training Loss: 0.962..  Test Loss: 0.946.. \n",
      "Epoch: 1185/5000..  Training Loss: 0.953..  Test Loss: 0.946.. \n",
      "Epoch: 1186/5000..  Training Loss: 0.944..  Test Loss: 0.946.. \n",
      "Epoch: 1187/5000..  Training Loss: 0.969..  Test Loss: 0.946.. \n",
      "Epoch: 1188/5000..  Training Loss: 0.963..  Test Loss: 0.946.. \n",
      "Epoch: 1189/5000..  Training Loss: 0.965..  Test Loss: 0.946.. \n",
      "Epoch: 1190/5000..  Training Loss: 0.936..  Test Loss: 0.946.. \n",
      "Epoch: 1191/5000..  Training Loss: 0.910..  Test Loss: 0.946.. \n",
      "Epoch: 1192/5000..  Training Loss: 0.941..  Test Loss: 0.946.. \n",
      "Epoch: 1193/5000..  Training Loss: 0.949..  Test Loss: 0.946.. \n",
      "Epoch: 1194/5000..  Training Loss: 0.975..  Test Loss: 0.946.. \n",
      "Epoch: 1195/5000..  Training Loss: 0.939..  Test Loss: 0.946.. \n",
      "Epoch: 1196/5000..  Training Loss: 0.969..  Test Loss: 0.946.. \n",
      "Epoch: 1197/5000..  Training Loss: 0.914..  Test Loss: 0.946.. \n",
      "Epoch: 1198/5000..  Training Loss: 0.930..  Test Loss: 0.946.. \n",
      "Epoch: 1199/5000..  Training Loss: 0.949..  Test Loss: 0.946.. \n",
      "Epoch: 1200/5000..  Training Loss: 0.969..  Test Loss: 0.946.. \n",
      "Epoch: 1201/5000..  Training Loss: 0.961..  Test Loss: 0.946.. \n",
      "Epoch: 1202/5000..  Training Loss: 0.920..  Test Loss: 0.946.. \n",
      "Epoch: 1203/5000..  Training Loss: 0.952..  Test Loss: 0.946.. \n",
      "Epoch: 1204/5000..  Training Loss: 0.931..  Test Loss: 0.946.. \n",
      "Epoch: 1205/5000..  Training Loss: 0.927..  Test Loss: 0.946.. \n",
      "Epoch: 1206/5000..  Training Loss: 0.942..  Test Loss: 0.946.. \n",
      "Epoch: 1207/5000..  Training Loss: 0.948..  Test Loss: 0.946.. \n",
      "Epoch: 1208/5000..  Training Loss: 0.954..  Test Loss: 0.946.. \n",
      "Epoch: 1209/5000..  Training Loss: 0.930..  Test Loss: 0.946.. \n",
      "Epoch: 1210/5000..  Training Loss: 0.977..  Test Loss: 0.946.. \n",
      "Epoch: 1211/5000..  Training Loss: 0.952..  Test Loss: 0.946.. \n",
      "Epoch: 1212/5000..  Training Loss: 0.941..  Test Loss: 0.946.. \n",
      "Epoch: 1213/5000..  Training Loss: 0.935..  Test Loss: 0.946.. \n",
      "Epoch: 1214/5000..  Training Loss: 0.941..  Test Loss: 0.946.. \n",
      "Epoch: 1215/5000..  Training Loss: 0.955..  Test Loss: 0.946.. \n",
      "Epoch: 1216/5000..  Training Loss: 0.910..  Test Loss: 0.946.. \n",
      "Epoch: 1217/5000..  Training Loss: 0.944..  Test Loss: 0.946.. \n",
      "Epoch: 1218/5000..  Training Loss: 0.967..  Test Loss: 0.946.. \n",
      "Epoch: 1219/5000..  Training Loss: 0.947..  Test Loss: 0.946.. \n",
      "Epoch: 1220/5000..  Training Loss: 0.954..  Test Loss: 0.946.. \n",
      "Epoch: 1221/5000..  Training Loss: 0.951..  Test Loss: 0.946.. \n",
      "Epoch: 1222/5000..  Training Loss: 0.929..  Test Loss: 0.946.. \n",
      "Epoch: 1223/5000..  Training Loss: 0.944..  Test Loss: 0.946.. \n",
      "Epoch: 1224/5000..  Training Loss: 0.931..  Test Loss: 0.946.. \n",
      "Epoch: 1225/5000..  Training Loss: 0.957..  Test Loss: 0.946.. \n",
      "Epoch: 1226/5000..  Training Loss: 0.967..  Test Loss: 0.946.. \n",
      "Epoch: 1227/5000..  Training Loss: 0.939..  Test Loss: 0.946.. \n",
      "Epoch: 1228/5000..  Training Loss: 0.911..  Test Loss: 0.946.. \n",
      "Epoch: 1229/5000..  Training Loss: 0.947..  Test Loss: 0.946.. \n",
      "Epoch: 1230/5000..  Training Loss: 0.968..  Test Loss: 0.946.. \n",
      "Epoch: 1231/5000..  Training Loss: 0.922..  Test Loss: 0.946.. \n",
      "Epoch: 1232/5000..  Training Loss: 0.920..  Test Loss: 0.946.. \n",
      "Epoch: 1233/5000..  Training Loss: 0.960..  Test Loss: 0.946.. \n",
      "Epoch: 1234/5000..  Training Loss: 0.955..  Test Loss: 0.946.. \n",
      "Epoch: 1235/5000..  Training Loss: 0.928..  Test Loss: 0.946.. \n",
      "Epoch: 1236/5000..  Training Loss: 0.935..  Test Loss: 0.946.. \n",
      "Epoch: 1237/5000..  Training Loss: 0.956..  Test Loss: 0.946.. \n",
      "Epoch: 1238/5000..  Training Loss: 0.943..  Test Loss: 0.946.. \n",
      "Epoch: 1239/5000..  Training Loss: 0.941..  Test Loss: 0.946.. \n",
      "Epoch: 1240/5000..  Training Loss: 0.977..  Test Loss: 0.946.. \n",
      "Epoch: 1241/5000..  Training Loss: 0.944..  Test Loss: 0.946.. \n",
      "Epoch: 1242/5000..  Training Loss: 0.953..  Test Loss: 0.946.. \n",
      "Epoch: 1243/5000..  Training Loss: 0.966..  Test Loss: 0.946.. \n",
      "Epoch: 1244/5000..  Training Loss: 0.947..  Test Loss: 0.946.. \n",
      "Epoch: 1245/5000..  Training Loss: 0.925..  Test Loss: 0.946.. \n",
      "Epoch: 1246/5000..  Training Loss: 0.978..  Test Loss: 0.946.. \n",
      "Epoch: 1247/5000..  Training Loss: 0.965..  Test Loss: 0.946.. \n",
      "Epoch: 1248/5000..  Training Loss: 0.966..  Test Loss: 0.946.. \n",
      "Epoch: 1249/5000..  Training Loss: 0.980..  Test Loss: 0.946.. \n",
      "Epoch: 1250/5000..  Training Loss: 0.921..  Test Loss: 0.946.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1251/5000..  Training Loss: 0.920..  Test Loss: 0.946.. \n",
      "Epoch: 1252/5000..  Training Loss: 0.939..  Test Loss: 0.945.. \n",
      "Epoch: 1253/5000..  Training Loss: 0.977..  Test Loss: 0.945.. \n",
      "Epoch: 1254/5000..  Training Loss: 0.895..  Test Loss: 0.945.. \n",
      "Epoch: 1255/5000..  Training Loss: 0.963..  Test Loss: 0.945.. \n",
      "Epoch: 1256/5000..  Training Loss: 0.960..  Test Loss: 0.945.. \n",
      "Epoch: 1257/5000..  Training Loss: 0.942..  Test Loss: 0.945.. \n",
      "Epoch: 1258/5000..  Training Loss: 0.959..  Test Loss: 0.945.. \n",
      "Epoch: 1259/5000..  Training Loss: 0.975..  Test Loss: 0.945.. \n",
      "Epoch: 1260/5000..  Training Loss: 0.936..  Test Loss: 0.945.. \n",
      "Epoch: 1261/5000..  Training Loss: 0.954..  Test Loss: 0.945.. \n",
      "Epoch: 1262/5000..  Training Loss: 0.965..  Test Loss: 0.945.. \n",
      "Epoch: 1263/5000..  Training Loss: 0.935..  Test Loss: 0.945.. \n",
      "Epoch: 1264/5000..  Training Loss: 0.943..  Test Loss: 0.945.. \n",
      "Epoch: 1265/5000..  Training Loss: 0.924..  Test Loss: 0.945.. \n",
      "Epoch: 1266/5000..  Training Loss: 0.936..  Test Loss: 0.945.. \n",
      "Epoch: 1267/5000..  Training Loss: 0.951..  Test Loss: 0.945.. \n",
      "Epoch: 1268/5000..  Training Loss: 0.947..  Test Loss: 0.945.. \n",
      "Epoch: 1269/5000..  Training Loss: 0.937..  Test Loss: 0.945.. \n",
      "Epoch: 1270/5000..  Training Loss: 0.944..  Test Loss: 0.945.. \n",
      "Epoch: 1271/5000..  Training Loss: 0.936..  Test Loss: 0.945.. \n",
      "Epoch: 1272/5000..  Training Loss: 0.957..  Test Loss: 0.945.. \n",
      "Epoch: 1273/5000..  Training Loss: 0.947..  Test Loss: 0.945.. \n",
      "Epoch: 1274/5000..  Training Loss: 0.931..  Test Loss: 0.945.. \n",
      "Epoch: 1275/5000..  Training Loss: 0.944..  Test Loss: 0.945.. \n",
      "Epoch: 1276/5000..  Training Loss: 0.966..  Test Loss: 0.945.. \n",
      "Epoch: 1277/5000..  Training Loss: 0.912..  Test Loss: 0.945.. \n",
      "Epoch: 1278/5000..  Training Loss: 0.955..  Test Loss: 0.945.. \n",
      "Epoch: 1279/5000..  Training Loss: 0.937..  Test Loss: 0.945.. \n",
      "Epoch: 1280/5000..  Training Loss: 0.951..  Test Loss: 0.945.. \n",
      "Epoch: 1281/5000..  Training Loss: 0.954..  Test Loss: 0.945.. \n",
      "Epoch: 1282/5000..  Training Loss: 0.946..  Test Loss: 0.945.. \n",
      "Epoch: 1283/5000..  Training Loss: 0.975..  Test Loss: 0.945.. \n",
      "Epoch: 1284/5000..  Training Loss: 0.931..  Test Loss: 0.945.. \n",
      "Epoch: 1285/5000..  Training Loss: 0.955..  Test Loss: 0.945.. \n",
      "Epoch: 1286/5000..  Training Loss: 0.963..  Test Loss: 0.945.. \n",
      "Epoch: 1287/5000..  Training Loss: 0.899..  Test Loss: 0.945.. \n",
      "Epoch: 1288/5000..  Training Loss: 0.963..  Test Loss: 0.945.. \n",
      "Epoch: 1289/5000..  Training Loss: 0.920..  Test Loss: 0.945.. \n",
      "Epoch: 1290/5000..  Training Loss: 0.976..  Test Loss: 0.945.. \n",
      "Epoch: 1291/5000..  Training Loss: 0.900..  Test Loss: 0.945.. \n",
      "Epoch: 1292/5000..  Training Loss: 0.960..  Test Loss: 0.945.. \n",
      "Epoch: 1293/5000..  Training Loss: 0.944..  Test Loss: 0.945.. \n",
      "Epoch: 1294/5000..  Training Loss: 0.956..  Test Loss: 0.945.. \n",
      "Epoch: 1295/5000..  Training Loss: 0.942..  Test Loss: 0.945.. \n",
      "Epoch: 1296/5000..  Training Loss: 0.935..  Test Loss: 0.945.. \n",
      "Epoch: 1297/5000..  Training Loss: 0.957..  Test Loss: 0.945.. \n",
      "Epoch: 1298/5000..  Training Loss: 0.901..  Test Loss: 0.945.. \n",
      "Epoch: 1299/5000..  Training Loss: 0.958..  Test Loss: 0.945.. \n",
      "Epoch: 1300/5000..  Training Loss: 0.966..  Test Loss: 0.945.. \n",
      "Epoch: 1301/5000..  Training Loss: 0.975..  Test Loss: 0.945.. \n",
      "Epoch: 1302/5000..  Training Loss: 0.926..  Test Loss: 0.945.. \n",
      "Epoch: 1303/5000..  Training Loss: 0.977..  Test Loss: 0.945.. \n",
      "Epoch: 1304/5000..  Training Loss: 0.939..  Test Loss: 0.945.. \n",
      "Epoch: 1305/5000..  Training Loss: 0.958..  Test Loss: 0.945.. \n",
      "Epoch: 1306/5000..  Training Loss: 0.966..  Test Loss: 0.945.. \n",
      "Epoch: 1307/5000..  Training Loss: 0.935..  Test Loss: 0.945.. \n",
      "Epoch: 1308/5000..  Training Loss: 0.944..  Test Loss: 0.945.. \n",
      "Epoch: 1309/5000..  Training Loss: 0.980..  Test Loss: 0.945.. \n",
      "Epoch: 1310/5000..  Training Loss: 0.953..  Test Loss: 0.945.. \n",
      "Epoch: 1311/5000..  Training Loss: 0.933..  Test Loss: 0.945.. \n",
      "Epoch: 1312/5000..  Training Loss: 0.916..  Test Loss: 0.945.. \n",
      "Epoch: 1313/5000..  Training Loss: 0.947..  Test Loss: 0.945.. \n",
      "Epoch: 1314/5000..  Training Loss: 0.927..  Test Loss: 0.945.. \n",
      "Epoch: 1315/5000..  Training Loss: 0.918..  Test Loss: 0.945.. \n",
      "Epoch: 1316/5000..  Training Loss: 0.955..  Test Loss: 0.945.. \n",
      "Epoch: 1317/5000..  Training Loss: 0.940..  Test Loss: 0.945.. \n",
      "Epoch: 1318/5000..  Training Loss: 0.976..  Test Loss: 0.945.. \n",
      "Epoch: 1319/5000..  Training Loss: 0.933..  Test Loss: 0.945.. \n",
      "Epoch: 1320/5000..  Training Loss: 0.959..  Test Loss: 0.945.. \n",
      "Epoch: 1321/5000..  Training Loss: 0.957..  Test Loss: 0.945.. \n",
      "Epoch: 1322/5000..  Training Loss: 0.925..  Test Loss: 0.945.. \n",
      "Epoch: 1323/5000..  Training Loss: 0.913..  Test Loss: 0.945.. \n",
      "Epoch: 1324/5000..  Training Loss: 0.953..  Test Loss: 0.945.. \n",
      "Epoch: 1325/5000..  Training Loss: 0.936..  Test Loss: 0.945.. \n",
      "Epoch: 1326/5000..  Training Loss: 0.908..  Test Loss: 0.945.. \n",
      "Epoch: 1327/5000..  Training Loss: 0.967..  Test Loss: 0.945.. \n",
      "Epoch: 1328/5000..  Training Loss: 0.944..  Test Loss: 0.945.. \n",
      "Epoch: 1329/5000..  Training Loss: 0.940..  Test Loss: 0.945.. \n",
      "Epoch: 1330/5000..  Training Loss: 0.914..  Test Loss: 0.945.. \n",
      "Epoch: 1331/5000..  Training Loss: 0.958..  Test Loss: 0.945.. \n",
      "Epoch: 1332/5000..  Training Loss: 0.947..  Test Loss: 0.945.. \n",
      "Epoch: 1333/5000..  Training Loss: 0.928..  Test Loss: 0.945.. \n",
      "Epoch: 1334/5000..  Training Loss: 0.974..  Test Loss: 0.945.. \n",
      "Epoch: 1335/5000..  Training Loss: 0.923..  Test Loss: 0.945.. \n",
      "Epoch: 1336/5000..  Training Loss: 0.949..  Test Loss: 0.945.. \n",
      "Epoch: 1337/5000..  Training Loss: 0.914..  Test Loss: 0.945.. \n",
      "Epoch: 1338/5000..  Training Loss: 0.919..  Test Loss: 0.945.. \n",
      "Epoch: 1339/5000..  Training Loss: 0.989..  Test Loss: 0.945.. \n",
      "Epoch: 1340/5000..  Training Loss: 0.947..  Test Loss: 0.945.. \n",
      "Epoch: 1341/5000..  Training Loss: 0.935..  Test Loss: 0.945.. \n",
      "Epoch: 1342/5000..  Training Loss: 0.945..  Test Loss: 0.945.. \n",
      "Epoch: 1343/5000..  Training Loss: 0.941..  Test Loss: 0.945.. \n",
      "Epoch: 1344/5000..  Training Loss: 0.950..  Test Loss: 0.945.. \n",
      "Epoch: 1345/5000..  Training Loss: 0.938..  Test Loss: 0.945.. \n",
      "Epoch: 1346/5000..  Training Loss: 0.945..  Test Loss: 0.945.. \n",
      "Epoch: 1347/5000..  Training Loss: 0.924..  Test Loss: 0.945.. \n",
      "Epoch: 1348/5000..  Training Loss: 0.943..  Test Loss: 0.945.. \n",
      "Epoch: 1349/5000..  Training Loss: 0.963..  Test Loss: 0.945.. \n",
      "Epoch: 1350/5000..  Training Loss: 0.945..  Test Loss: 0.945.. \n",
      "Epoch: 1351/5000..  Training Loss: 0.932..  Test Loss: 0.945.. \n",
      "Epoch: 1352/5000..  Training Loss: 0.914..  Test Loss: 0.945.. \n",
      "Epoch: 1353/5000..  Training Loss: 0.982..  Test Loss: 0.945.. \n",
      "Epoch: 1354/5000..  Training Loss: 0.937..  Test Loss: 0.944.. \n",
      "Epoch: 1355/5000..  Training Loss: 0.945..  Test Loss: 0.944.. \n",
      "Epoch: 1356/5000..  Training Loss: 0.937..  Test Loss: 0.944.. \n",
      "Epoch: 1357/5000..  Training Loss: 0.948..  Test Loss: 0.944.. \n",
      "Epoch: 1358/5000..  Training Loss: 0.946..  Test Loss: 0.944.. \n",
      "Epoch: 1359/5000..  Training Loss: 0.955..  Test Loss: 0.944.. \n",
      "Epoch: 1360/5000..  Training Loss: 0.928..  Test Loss: 0.944.. \n",
      "Epoch: 1361/5000..  Training Loss: 0.922..  Test Loss: 0.944.. \n",
      "Epoch: 1362/5000..  Training Loss: 0.947..  Test Loss: 0.944.. \n",
      "Epoch: 1363/5000..  Training Loss: 0.943..  Test Loss: 0.944.. \n",
      "Epoch: 1364/5000..  Training Loss: 0.926..  Test Loss: 0.944.. \n",
      "Epoch: 1365/5000..  Training Loss: 0.919..  Test Loss: 0.944.. \n",
      "Epoch: 1366/5000..  Training Loss: 0.916..  Test Loss: 0.944.. \n",
      "Epoch: 1367/5000..  Training Loss: 0.888..  Test Loss: 0.944.. \n",
      "Epoch: 1368/5000..  Training Loss: 0.918..  Test Loss: 0.944.. \n",
      "Epoch: 1369/5000..  Training Loss: 0.966..  Test Loss: 0.944.. \n",
      "Epoch: 1370/5000..  Training Loss: 0.943..  Test Loss: 0.944.. \n",
      "Epoch: 1371/5000..  Training Loss: 0.941..  Test Loss: 0.944.. \n",
      "Epoch: 1372/5000..  Training Loss: 0.921..  Test Loss: 0.944.. \n",
      "Epoch: 1373/5000..  Training Loss: 0.921..  Test Loss: 0.944.. \n",
      "Epoch: 1374/5000..  Training Loss: 0.926..  Test Loss: 0.944.. \n",
      "Epoch: 1375/5000..  Training Loss: 0.952..  Test Loss: 0.944.. \n",
      "Epoch: 1376/5000..  Training Loss: 0.942..  Test Loss: 0.944.. \n",
      "Epoch: 1377/5000..  Training Loss: 0.943..  Test Loss: 0.944.. \n",
      "Epoch: 1378/5000..  Training Loss: 0.948..  Test Loss: 0.944.. \n",
      "Epoch: 1379/5000..  Training Loss: 0.947..  Test Loss: 0.944.. \n",
      "Epoch: 1380/5000..  Training Loss: 0.925..  Test Loss: 0.944.. \n",
      "Epoch: 1381/5000..  Training Loss: 0.971..  Test Loss: 0.944.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1382/5000..  Training Loss: 0.908..  Test Loss: 0.944.. \n",
      "Epoch: 1383/5000..  Training Loss: 0.961..  Test Loss: 0.944.. \n",
      "Epoch: 1384/5000..  Training Loss: 0.944..  Test Loss: 0.944.. \n",
      "Epoch: 1385/5000..  Training Loss: 0.927..  Test Loss: 0.944.. \n",
      "Epoch: 1386/5000..  Training Loss: 0.948..  Test Loss: 0.944.. \n",
      "Epoch: 1387/5000..  Training Loss: 0.921..  Test Loss: 0.944.. \n",
      "Epoch: 1388/5000..  Training Loss: 0.930..  Test Loss: 0.944.. \n",
      "Epoch: 1389/5000..  Training Loss: 0.920..  Test Loss: 0.944.. \n",
      "Epoch: 1390/5000..  Training Loss: 0.907..  Test Loss: 0.944.. \n",
      "Epoch: 1391/5000..  Training Loss: 0.920..  Test Loss: 0.944.. \n",
      "Epoch: 1392/5000..  Training Loss: 0.935..  Test Loss: 0.944.. \n",
      "Epoch: 1393/5000..  Training Loss: 0.962..  Test Loss: 0.944.. \n",
      "Epoch: 1394/5000..  Training Loss: 0.936..  Test Loss: 0.944.. \n",
      "Epoch: 1395/5000..  Training Loss: 0.938..  Test Loss: 0.944.. \n",
      "Epoch: 1396/5000..  Training Loss: 0.928..  Test Loss: 0.944.. \n",
      "Epoch: 1397/5000..  Training Loss: 0.930..  Test Loss: 0.944.. \n",
      "Epoch: 1398/5000..  Training Loss: 0.951..  Test Loss: 0.944.. \n",
      "Epoch: 1399/5000..  Training Loss: 0.974..  Test Loss: 0.944.. \n",
      "Epoch: 1400/5000..  Training Loss: 0.916..  Test Loss: 0.944.. \n",
      "Epoch: 1401/5000..  Training Loss: 0.945..  Test Loss: 0.944.. \n",
      "Epoch: 1402/5000..  Training Loss: 0.949..  Test Loss: 0.944.. \n",
      "Epoch: 1403/5000..  Training Loss: 0.937..  Test Loss: 0.944.. \n",
      "Epoch: 1404/5000..  Training Loss: 0.956..  Test Loss: 0.944.. \n",
      "Epoch: 1405/5000..  Training Loss: 0.923..  Test Loss: 0.944.. \n",
      "Epoch: 1406/5000..  Training Loss: 0.919..  Test Loss: 0.944.. \n",
      "Epoch: 1407/5000..  Training Loss: 0.962..  Test Loss: 0.944.. \n",
      "Epoch: 1408/5000..  Training Loss: 0.965..  Test Loss: 0.944.. \n",
      "Epoch: 1409/5000..  Training Loss: 0.972..  Test Loss: 0.944.. \n",
      "Epoch: 1410/5000..  Training Loss: 0.943..  Test Loss: 0.944.. \n",
      "Epoch: 1411/5000..  Training Loss: 0.943..  Test Loss: 0.944.. \n",
      "Epoch: 1412/5000..  Training Loss: 0.928..  Test Loss: 0.944.. \n",
      "Epoch: 1413/5000..  Training Loss: 0.958..  Test Loss: 0.944.. \n",
      "Epoch: 1414/5000..  Training Loss: 0.903..  Test Loss: 0.944.. \n",
      "Epoch: 1415/5000..  Training Loss: 0.955..  Test Loss: 0.944.. \n",
      "Epoch: 1416/5000..  Training Loss: 0.949..  Test Loss: 0.944.. \n",
      "Epoch: 1417/5000..  Training Loss: 0.938..  Test Loss: 0.944.. \n",
      "Epoch: 1418/5000..  Training Loss: 0.908..  Test Loss: 0.944.. \n",
      "Epoch: 1419/5000..  Training Loss: 0.940..  Test Loss: 0.944.. \n",
      "Epoch: 1420/5000..  Training Loss: 0.954..  Test Loss: 0.944.. \n",
      "Epoch: 1421/5000..  Training Loss: 0.971..  Test Loss: 0.944.. \n",
      "Epoch: 1422/5000..  Training Loss: 0.972..  Test Loss: 0.944.. \n",
      "Epoch: 1423/5000..  Training Loss: 0.981..  Test Loss: 0.944.. \n",
      "Epoch: 1424/5000..  Training Loss: 0.946..  Test Loss: 0.944.. \n",
      "Epoch: 1425/5000..  Training Loss: 0.956..  Test Loss: 0.944.. \n",
      "Epoch: 1426/5000..  Training Loss: 0.951..  Test Loss: 0.944.. \n",
      "Epoch: 1427/5000..  Training Loss: 0.956..  Test Loss: 0.944.. \n",
      "Epoch: 1428/5000..  Training Loss: 0.950..  Test Loss: 0.944.. \n",
      "Epoch: 1429/5000..  Training Loss: 0.922..  Test Loss: 0.944.. \n",
      "Epoch: 1430/5000..  Training Loss: 0.948..  Test Loss: 0.944.. \n",
      "Epoch: 1431/5000..  Training Loss: 0.940..  Test Loss: 0.944.. \n",
      "Epoch: 1432/5000..  Training Loss: 0.946..  Test Loss: 0.944.. \n",
      "Epoch: 1433/5000..  Training Loss: 0.929..  Test Loss: 0.944.. \n",
      "Epoch: 1434/5000..  Training Loss: 0.898..  Test Loss: 0.944.. \n",
      "Epoch: 1435/5000..  Training Loss: 0.957..  Test Loss: 0.944.. \n",
      "Epoch: 1436/5000..  Training Loss: 0.923..  Test Loss: 0.944.. \n",
      "Epoch: 1437/5000..  Training Loss: 0.931..  Test Loss: 0.944.. \n",
      "Epoch: 1438/5000..  Training Loss: 0.928..  Test Loss: 0.944.. \n",
      "Epoch: 1439/5000..  Training Loss: 0.947..  Test Loss: 0.944.. \n",
      "Epoch: 1440/5000..  Training Loss: 0.937..  Test Loss: 0.944.. \n",
      "Epoch: 1441/5000..  Training Loss: 0.923..  Test Loss: 0.944.. \n",
      "Epoch: 1442/5000..  Training Loss: 0.954..  Test Loss: 0.944.. \n",
      "Epoch: 1443/5000..  Training Loss: 0.960..  Test Loss: 0.944.. \n",
      "Epoch: 1444/5000..  Training Loss: 0.964..  Test Loss: 0.944.. \n",
      "Epoch: 1445/5000..  Training Loss: 0.951..  Test Loss: 0.944.. \n",
      "Epoch: 1446/5000..  Training Loss: 0.963..  Test Loss: 0.944.. \n",
      "Epoch: 1447/5000..  Training Loss: 0.979..  Test Loss: 0.944.. \n",
      "Epoch: 1448/5000..  Training Loss: 0.952..  Test Loss: 0.944.. \n",
      "Epoch: 1449/5000..  Training Loss: 0.980..  Test Loss: 0.944.. \n",
      "Epoch: 1450/5000..  Training Loss: 0.948..  Test Loss: 0.944.. \n",
      "Epoch: 1451/5000..  Training Loss: 0.959..  Test Loss: 0.944.. \n",
      "Epoch: 1452/5000..  Training Loss: 0.970..  Test Loss: 0.944.. \n",
      "Epoch: 1453/5000..  Training Loss: 0.961..  Test Loss: 0.944.. \n",
      "Epoch: 1454/5000..  Training Loss: 0.977..  Test Loss: 0.944.. \n",
      "Epoch: 1455/5000..  Training Loss: 0.927..  Test Loss: 0.944.. \n",
      "Epoch: 1456/5000..  Training Loss: 0.940..  Test Loss: 0.944.. \n",
      "Epoch: 1457/5000..  Training Loss: 0.927..  Test Loss: 0.944.. \n",
      "Epoch: 1458/5000..  Training Loss: 0.911..  Test Loss: 0.943.. \n",
      "Epoch: 1459/5000..  Training Loss: 0.971..  Test Loss: 0.943.. \n",
      "Epoch: 1460/5000..  Training Loss: 0.940..  Test Loss: 0.943.. \n",
      "Epoch: 1461/5000..  Training Loss: 0.923..  Test Loss: 0.943.. \n",
      "Epoch: 1462/5000..  Training Loss: 0.917..  Test Loss: 0.943.. \n",
      "Epoch: 1463/5000..  Training Loss: 0.938..  Test Loss: 0.943.. \n",
      "Epoch: 1464/5000..  Training Loss: 0.930..  Test Loss: 0.943.. \n",
      "Epoch: 1465/5000..  Training Loss: 0.946..  Test Loss: 0.943.. \n",
      "Epoch: 1466/5000..  Training Loss: 0.917..  Test Loss: 0.943.. \n",
      "Epoch: 1467/5000..  Training Loss: 0.968..  Test Loss: 0.943.. \n",
      "Epoch: 1468/5000..  Training Loss: 0.948..  Test Loss: 0.943.. \n",
      "Epoch: 1469/5000..  Training Loss: 0.943..  Test Loss: 0.943.. \n",
      "Epoch: 1470/5000..  Training Loss: 0.940..  Test Loss: 0.943.. \n",
      "Epoch: 1471/5000..  Training Loss: 0.944..  Test Loss: 0.943.. \n",
      "Epoch: 1472/5000..  Training Loss: 0.963..  Test Loss: 0.943.. \n",
      "Epoch: 1473/5000..  Training Loss: 0.934..  Test Loss: 0.943.. \n",
      "Epoch: 1474/5000..  Training Loss: 0.947..  Test Loss: 0.943.. \n",
      "Epoch: 1475/5000..  Training Loss: 0.977..  Test Loss: 0.943.. \n",
      "Epoch: 1476/5000..  Training Loss: 0.930..  Test Loss: 0.943.. \n",
      "Epoch: 1477/5000..  Training Loss: 0.964..  Test Loss: 0.943.. \n",
      "Epoch: 1478/5000..  Training Loss: 0.960..  Test Loss: 0.943.. \n",
      "Epoch: 1479/5000..  Training Loss: 0.955..  Test Loss: 0.943.. \n",
      "Epoch: 1480/5000..  Training Loss: 0.962..  Test Loss: 0.943.. \n",
      "Epoch: 1481/5000..  Training Loss: 0.969..  Test Loss: 0.943.. \n",
      "Epoch: 1482/5000..  Training Loss: 0.924..  Test Loss: 0.943.. \n",
      "Epoch: 1483/5000..  Training Loss: 0.908..  Test Loss: 0.943.. \n",
      "Epoch: 1484/5000..  Training Loss: 0.942..  Test Loss: 0.943.. \n",
      "Epoch: 1485/5000..  Training Loss: 0.943..  Test Loss: 0.943.. \n",
      "Epoch: 1486/5000..  Training Loss: 0.998..  Test Loss: 0.943.. \n",
      "Epoch: 1487/5000..  Training Loss: 0.930..  Test Loss: 0.943.. \n",
      "Epoch: 1488/5000..  Training Loss: 0.946..  Test Loss: 0.943.. \n",
      "Epoch: 1489/5000..  Training Loss: 0.904..  Test Loss: 0.943.. \n",
      "Epoch: 1490/5000..  Training Loss: 0.945..  Test Loss: 0.943.. \n",
      "Epoch: 1491/5000..  Training Loss: 0.921..  Test Loss: 0.943.. \n",
      "Epoch: 1492/5000..  Training Loss: 0.940..  Test Loss: 0.943.. \n",
      "Epoch: 1493/5000..  Training Loss: 0.939..  Test Loss: 0.943.. \n",
      "Epoch: 1494/5000..  Training Loss: 0.948..  Test Loss: 0.943.. \n",
      "Epoch: 1495/5000..  Training Loss: 0.945..  Test Loss: 0.943.. \n",
      "Epoch: 1496/5000..  Training Loss: 0.955..  Test Loss: 0.943.. \n",
      "Epoch: 1497/5000..  Training Loss: 0.941..  Test Loss: 0.943.. \n",
      "Epoch: 1498/5000..  Training Loss: 0.936..  Test Loss: 0.943.. \n",
      "Epoch: 1499/5000..  Training Loss: 0.965..  Test Loss: 0.943.. \n",
      "Epoch: 1500/5000..  Training Loss: 0.950..  Test Loss: 0.943.. \n",
      "Epoch: 1501/5000..  Training Loss: 0.922..  Test Loss: 0.943.. \n",
      "Epoch: 1502/5000..  Training Loss: 0.954..  Test Loss: 0.943.. \n",
      "Epoch: 1503/5000..  Training Loss: 0.948..  Test Loss: 0.943.. \n",
      "Epoch: 1504/5000..  Training Loss: 0.965..  Test Loss: 0.943.. \n",
      "Epoch: 1505/5000..  Training Loss: 0.916..  Test Loss: 0.943.. \n",
      "Epoch: 1506/5000..  Training Loss: 0.942..  Test Loss: 0.943.. \n",
      "Epoch: 1507/5000..  Training Loss: 0.910..  Test Loss: 0.943.. \n",
      "Epoch: 1508/5000..  Training Loss: 0.976..  Test Loss: 0.943.. \n",
      "Epoch: 1509/5000..  Training Loss: 0.944..  Test Loss: 0.943.. \n",
      "Epoch: 1510/5000..  Training Loss: 0.965..  Test Loss: 0.943.. \n",
      "Epoch: 1511/5000..  Training Loss: 0.919..  Test Loss: 0.943.. \n",
      "Epoch: 1512/5000..  Training Loss: 0.915..  Test Loss: 0.943.. \n",
      "Epoch: 1513/5000..  Training Loss: 0.932..  Test Loss: 0.943.. \n",
      "Epoch: 1514/5000..  Training Loss: 0.955..  Test Loss: 0.943.. \n",
      "Epoch: 1515/5000..  Training Loss: 0.917..  Test Loss: 0.943.. \n",
      "Epoch: 1516/5000..  Training Loss: 0.920..  Test Loss: 0.943.. \n",
      "Epoch: 1517/5000..  Training Loss: 0.927..  Test Loss: 0.943.. \n",
      "Epoch: 1518/5000..  Training Loss: 0.965..  Test Loss: 0.943.. \n",
      "Epoch: 1519/5000..  Training Loss: 0.926..  Test Loss: 0.943.. \n",
      "Epoch: 1520/5000..  Training Loss: 0.962..  Test Loss: 0.943.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1521/5000..  Training Loss: 0.934..  Test Loss: 0.943.. \n",
      "Epoch: 1522/5000..  Training Loss: 0.959..  Test Loss: 0.943.. \n",
      "Epoch: 1523/5000..  Training Loss: 0.933..  Test Loss: 0.943.. \n",
      "Epoch: 1524/5000..  Training Loss: 0.949..  Test Loss: 0.943.. \n",
      "Epoch: 1525/5000..  Training Loss: 0.965..  Test Loss: 0.943.. \n",
      "Epoch: 1526/5000..  Training Loss: 0.968..  Test Loss: 0.943.. \n",
      "Epoch: 1527/5000..  Training Loss: 0.940..  Test Loss: 0.943.. \n",
      "Epoch: 1528/5000..  Training Loss: 0.935..  Test Loss: 0.943.. \n",
      "Epoch: 1529/5000..  Training Loss: 0.937..  Test Loss: 0.943.. \n",
      "Epoch: 1530/5000..  Training Loss: 0.924..  Test Loss: 0.943.. \n",
      "Epoch: 1531/5000..  Training Loss: 0.958..  Test Loss: 0.943.. \n",
      "Epoch: 1532/5000..  Training Loss: 0.912..  Test Loss: 0.943.. \n",
      "Epoch: 1533/5000..  Training Loss: 0.912..  Test Loss: 0.943.. \n",
      "Epoch: 1534/5000..  Training Loss: 0.929..  Test Loss: 0.943.. \n",
      "Epoch: 1535/5000..  Training Loss: 0.952..  Test Loss: 0.943.. \n",
      "Epoch: 1536/5000..  Training Loss: 0.939..  Test Loss: 0.943.. \n",
      "Epoch: 1537/5000..  Training Loss: 0.947..  Test Loss: 0.943.. \n",
      "Epoch: 1538/5000..  Training Loss: 0.967..  Test Loss: 0.943.. \n",
      "Epoch: 1539/5000..  Training Loss: 0.925..  Test Loss: 0.943.. \n",
      "Epoch: 1540/5000..  Training Loss: 0.946..  Test Loss: 0.943.. \n",
      "Epoch: 1541/5000..  Training Loss: 0.976..  Test Loss: 0.943.. \n",
      "Epoch: 1542/5000..  Training Loss: 0.920..  Test Loss: 0.943.. \n",
      "Epoch: 1543/5000..  Training Loss: 0.953..  Test Loss: 0.943.. \n",
      "Epoch: 1544/5000..  Training Loss: 0.940..  Test Loss: 0.943.. \n",
      "Epoch: 1545/5000..  Training Loss: 0.927..  Test Loss: 0.943.. \n",
      "Epoch: 1546/5000..  Training Loss: 0.968..  Test Loss: 0.943.. \n",
      "Epoch: 1547/5000..  Training Loss: 0.907..  Test Loss: 0.943.. \n",
      "Epoch: 1548/5000..  Training Loss: 0.928..  Test Loss: 0.943.. \n",
      "Epoch: 1549/5000..  Training Loss: 0.950..  Test Loss: 0.943.. \n",
      "Epoch: 1550/5000..  Training Loss: 0.938..  Test Loss: 0.943.. \n",
      "Epoch: 1551/5000..  Training Loss: 0.956..  Test Loss: 0.943.. \n",
      "Epoch: 1552/5000..  Training Loss: 0.963..  Test Loss: 0.943.. \n",
      "Epoch: 1553/5000..  Training Loss: 0.938..  Test Loss: 0.943.. \n",
      "Epoch: 1554/5000..  Training Loss: 0.934..  Test Loss: 0.943.. \n",
      "Epoch: 1555/5000..  Training Loss: 0.920..  Test Loss: 0.943.. \n",
      "Epoch: 1556/5000..  Training Loss: 0.959..  Test Loss: 0.943.. \n",
      "Epoch: 1557/5000..  Training Loss: 0.931..  Test Loss: 0.943.. \n",
      "Epoch: 1558/5000..  Training Loss: 0.924..  Test Loss: 0.943.. \n",
      "Epoch: 1559/5000..  Training Loss: 0.947..  Test Loss: 0.943.. \n",
      "Epoch: 1560/5000..  Training Loss: 0.927..  Test Loss: 0.943.. \n",
      "Epoch: 1561/5000..  Training Loss: 0.955..  Test Loss: 0.942.. \n",
      "Epoch: 1562/5000..  Training Loss: 0.932..  Test Loss: 0.942.. \n",
      "Epoch: 1563/5000..  Training Loss: 0.947..  Test Loss: 0.942.. \n",
      "Epoch: 1564/5000..  Training Loss: 0.951..  Test Loss: 0.942.. \n",
      "Epoch: 1565/5000..  Training Loss: 0.925..  Test Loss: 0.942.. \n",
      "Epoch: 1566/5000..  Training Loss: 0.959..  Test Loss: 0.942.. \n",
      "Epoch: 1567/5000..  Training Loss: 0.948..  Test Loss: 0.942.. \n",
      "Epoch: 1568/5000..  Training Loss: 0.938..  Test Loss: 0.942.. \n",
      "Epoch: 1569/5000..  Training Loss: 0.946..  Test Loss: 0.942.. \n",
      "Epoch: 1570/5000..  Training Loss: 0.970..  Test Loss: 0.942.. \n",
      "Epoch: 1571/5000..  Training Loss: 0.938..  Test Loss: 0.942.. \n",
      "Epoch: 1572/5000..  Training Loss: 0.927..  Test Loss: 0.942.. \n",
      "Epoch: 1573/5000..  Training Loss: 0.954..  Test Loss: 0.942.. \n",
      "Epoch: 1574/5000..  Training Loss: 0.927..  Test Loss: 0.942.. \n",
      "Epoch: 1575/5000..  Training Loss: 0.941..  Test Loss: 0.942.. \n",
      "Epoch: 1576/5000..  Training Loss: 0.939..  Test Loss: 0.942.. \n",
      "Epoch: 1577/5000..  Training Loss: 0.955..  Test Loss: 0.942.. \n",
      "Epoch: 1578/5000..  Training Loss: 0.935..  Test Loss: 0.942.. \n",
      "Epoch: 1579/5000..  Training Loss: 0.925..  Test Loss: 0.942.. \n",
      "Epoch: 1580/5000..  Training Loss: 0.943..  Test Loss: 0.942.. \n",
      "Epoch: 1581/5000..  Training Loss: 0.931..  Test Loss: 0.942.. \n",
      "Epoch: 1582/5000..  Training Loss: 0.948..  Test Loss: 0.942.. \n",
      "Epoch: 1583/5000..  Training Loss: 0.935..  Test Loss: 0.942.. \n",
      "Epoch: 1584/5000..  Training Loss: 0.957..  Test Loss: 0.942.. \n",
      "Epoch: 1585/5000..  Training Loss: 0.943..  Test Loss: 0.942.. \n",
      "Epoch: 1586/5000..  Training Loss: 0.941..  Test Loss: 0.942.. \n",
      "Epoch: 1587/5000..  Training Loss: 0.931..  Test Loss: 0.942.. \n",
      "Epoch: 1588/5000..  Training Loss: 0.952..  Test Loss: 0.942.. \n",
      "Epoch: 1589/5000..  Training Loss: 0.967..  Test Loss: 0.942.. \n",
      "Epoch: 1590/5000..  Training Loss: 0.894..  Test Loss: 0.942.. \n",
      "Epoch: 1591/5000..  Training Loss: 0.984..  Test Loss: 0.942.. \n",
      "Epoch: 1592/5000..  Training Loss: 0.926..  Test Loss: 0.942.. \n",
      "Epoch: 1593/5000..  Training Loss: 0.977..  Test Loss: 0.942.. \n",
      "Epoch: 1594/5000..  Training Loss: 0.963..  Test Loss: 0.942.. \n",
      "Epoch: 1595/5000..  Training Loss: 0.946..  Test Loss: 0.942.. \n",
      "Epoch: 1596/5000..  Training Loss: 0.976..  Test Loss: 0.942.. \n",
      "Epoch: 1597/5000..  Training Loss: 0.917..  Test Loss: 0.942.. \n",
      "Epoch: 1598/5000..  Training Loss: 0.964..  Test Loss: 0.942.. \n",
      "Epoch: 1599/5000..  Training Loss: 0.918..  Test Loss: 0.942.. \n",
      "Epoch: 1600/5000..  Training Loss: 0.966..  Test Loss: 0.942.. \n",
      "Epoch: 1601/5000..  Training Loss: 0.948..  Test Loss: 0.942.. \n",
      "Epoch: 1602/5000..  Training Loss: 0.934..  Test Loss: 0.942.. \n",
      "Epoch: 1603/5000..  Training Loss: 0.974..  Test Loss: 0.942.. \n",
      "Epoch: 1604/5000..  Training Loss: 0.988..  Test Loss: 0.942.. \n",
      "Epoch: 1605/5000..  Training Loss: 0.941..  Test Loss: 0.942.. \n",
      "Epoch: 1606/5000..  Training Loss: 0.954..  Test Loss: 0.942.. \n",
      "Epoch: 1607/5000..  Training Loss: 0.943..  Test Loss: 0.942.. \n",
      "Epoch: 1608/5000..  Training Loss: 0.962..  Test Loss: 0.942.. \n",
      "Epoch: 1609/5000..  Training Loss: 0.930..  Test Loss: 0.942.. \n",
      "Epoch: 1610/5000..  Training Loss: 0.940..  Test Loss: 0.942.. \n",
      "Epoch: 1611/5000..  Training Loss: 0.967..  Test Loss: 0.942.. \n",
      "Epoch: 1612/5000..  Training Loss: 0.926..  Test Loss: 0.942.. \n",
      "Epoch: 1613/5000..  Training Loss: 0.945..  Test Loss: 0.942.. \n",
      "Epoch: 1614/5000..  Training Loss: 0.938..  Test Loss: 0.942.. \n",
      "Epoch: 1615/5000..  Training Loss: 0.933..  Test Loss: 0.942.. \n",
      "Epoch: 1616/5000..  Training Loss: 0.929..  Test Loss: 0.942.. \n",
      "Epoch: 1617/5000..  Training Loss: 0.914..  Test Loss: 0.942.. \n",
      "Epoch: 1618/5000..  Training Loss: 0.966..  Test Loss: 0.942.. \n",
      "Epoch: 1619/5000..  Training Loss: 0.907..  Test Loss: 0.942.. \n",
      "Epoch: 1620/5000..  Training Loss: 0.942..  Test Loss: 0.942.. \n",
      "Epoch: 1621/5000..  Training Loss: 0.929..  Test Loss: 0.942.. \n",
      "Epoch: 1622/5000..  Training Loss: 0.996..  Test Loss: 0.942.. \n",
      "Epoch: 1623/5000..  Training Loss: 0.947..  Test Loss: 0.942.. \n",
      "Epoch: 1624/5000..  Training Loss: 0.907..  Test Loss: 0.942.. \n",
      "Epoch: 1625/5000..  Training Loss: 0.966..  Test Loss: 0.942.. \n",
      "Epoch: 1626/5000..  Training Loss: 0.920..  Test Loss: 0.942.. \n",
      "Epoch: 1627/5000..  Training Loss: 0.912..  Test Loss: 0.942.. \n",
      "Epoch: 1628/5000..  Training Loss: 0.965..  Test Loss: 0.942.. \n",
      "Epoch: 1629/5000..  Training Loss: 0.967..  Test Loss: 0.942.. \n",
      "Epoch: 1630/5000..  Training Loss: 0.995..  Test Loss: 0.942.. \n",
      "Epoch: 1631/5000..  Training Loss: 0.933..  Test Loss: 0.942.. \n",
      "Epoch: 1632/5000..  Training Loss: 0.921..  Test Loss: 0.942.. \n",
      "Epoch: 1633/5000..  Training Loss: 0.966..  Test Loss: 0.942.. \n",
      "Epoch: 1634/5000..  Training Loss: 0.962..  Test Loss: 0.942.. \n",
      "Epoch: 1635/5000..  Training Loss: 0.959..  Test Loss: 0.942.. \n",
      "Epoch: 1636/5000..  Training Loss: 0.925..  Test Loss: 0.942.. \n",
      "Epoch: 1637/5000..  Training Loss: 0.951..  Test Loss: 0.942.. \n",
      "Epoch: 1638/5000..  Training Loss: 0.966..  Test Loss: 0.942.. \n",
      "Epoch: 1639/5000..  Training Loss: 0.935..  Test Loss: 0.942.. \n",
      "Epoch: 1640/5000..  Training Loss: 0.939..  Test Loss: 0.942.. \n",
      "Epoch: 1641/5000..  Training Loss: 0.959..  Test Loss: 0.942.. \n",
      "Epoch: 1642/5000..  Training Loss: 0.928..  Test Loss: 0.942.. \n",
      "Epoch: 1643/5000..  Training Loss: 0.974..  Test Loss: 0.942.. \n",
      "Epoch: 1644/5000..  Training Loss: 0.961..  Test Loss: 0.942.. \n",
      "Epoch: 1645/5000..  Training Loss: 0.909..  Test Loss: 0.942.. \n",
      "Epoch: 1646/5000..  Training Loss: 0.957..  Test Loss: 0.942.. \n",
      "Epoch: 1647/5000..  Training Loss: 0.949..  Test Loss: 0.942.. \n",
      "Epoch: 1648/5000..  Training Loss: 0.934..  Test Loss: 0.942.. \n",
      "Epoch: 1649/5000..  Training Loss: 0.971..  Test Loss: 0.942.. \n",
      "Epoch: 1650/5000..  Training Loss: 0.939..  Test Loss: 0.942.. \n",
      "Epoch: 1651/5000..  Training Loss: 0.966..  Test Loss: 0.942.. \n",
      "Epoch: 1652/5000..  Training Loss: 0.958..  Test Loss: 0.942.. \n",
      "Epoch: 1653/5000..  Training Loss: 0.937..  Test Loss: 0.942.. \n",
      "Epoch: 1654/5000..  Training Loss: 0.956..  Test Loss: 0.942.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1655/5000..  Training Loss: 0.943..  Test Loss: 0.942.. \n",
      "Epoch: 1656/5000..  Training Loss: 0.941..  Test Loss: 0.942.. \n",
      "Epoch: 1657/5000..  Training Loss: 0.921..  Test Loss: 0.942.. \n",
      "Epoch: 1658/5000..  Training Loss: 0.935..  Test Loss: 0.942.. \n",
      "Epoch: 1659/5000..  Training Loss: 0.917..  Test Loss: 0.942.. \n",
      "Epoch: 1660/5000..  Training Loss: 0.933..  Test Loss: 0.942.. \n",
      "Epoch: 1661/5000..  Training Loss: 0.954..  Test Loss: 0.942.. \n",
      "Epoch: 1662/5000..  Training Loss: 0.961..  Test Loss: 0.942.. \n",
      "Epoch: 1663/5000..  Training Loss: 0.976..  Test Loss: 0.942.. \n",
      "Epoch: 1664/5000..  Training Loss: 0.925..  Test Loss: 0.942.. \n",
      "Epoch: 1665/5000..  Training Loss: 0.973..  Test Loss: 0.942.. \n",
      "Epoch: 1666/5000..  Training Loss: 0.950..  Test Loss: 0.942.. \n",
      "Epoch: 1667/5000..  Training Loss: 0.918..  Test Loss: 0.942.. \n",
      "Epoch: 1668/5000..  Training Loss: 0.937..  Test Loss: 0.942.. \n",
      "Epoch: 1669/5000..  Training Loss: 0.950..  Test Loss: 0.942.. \n",
      "Epoch: 1670/5000..  Training Loss: 0.937..  Test Loss: 0.942.. \n",
      "Epoch: 1671/5000..  Training Loss: 0.933..  Test Loss: 0.942.. \n",
      "Epoch: 1672/5000..  Training Loss: 0.933..  Test Loss: 0.942.. \n",
      "Epoch: 1673/5000..  Training Loss: 0.908..  Test Loss: 0.941.. \n",
      "Epoch: 1674/5000..  Training Loss: 0.945..  Test Loss: 0.941.. \n",
      "Epoch: 1675/5000..  Training Loss: 0.906..  Test Loss: 0.941.. \n",
      "Epoch: 1676/5000..  Training Loss: 0.982..  Test Loss: 0.941.. \n",
      "Epoch: 1677/5000..  Training Loss: 0.957..  Test Loss: 0.941.. \n",
      "Epoch: 1678/5000..  Training Loss: 0.966..  Test Loss: 0.941.. \n",
      "Epoch: 1679/5000..  Training Loss: 0.962..  Test Loss: 0.941.. \n",
      "Epoch: 1680/5000..  Training Loss: 0.935..  Test Loss: 0.941.. \n",
      "Epoch: 1681/5000..  Training Loss: 0.935..  Test Loss: 0.941.. \n",
      "Epoch: 1682/5000..  Training Loss: 0.925..  Test Loss: 0.941.. \n",
      "Epoch: 1683/5000..  Training Loss: 0.934..  Test Loss: 0.941.. \n",
      "Epoch: 1684/5000..  Training Loss: 0.966..  Test Loss: 0.941.. \n",
      "Epoch: 1685/5000..  Training Loss: 0.930..  Test Loss: 0.941.. \n",
      "Epoch: 1686/5000..  Training Loss: 0.921..  Test Loss: 0.941.. \n",
      "Epoch: 1687/5000..  Training Loss: 0.937..  Test Loss: 0.941.. \n",
      "Epoch: 1688/5000..  Training Loss: 0.969..  Test Loss: 0.941.. \n",
      "Epoch: 1689/5000..  Training Loss: 0.947..  Test Loss: 0.941.. \n",
      "Epoch: 1690/5000..  Training Loss: 0.968..  Test Loss: 0.941.. \n",
      "Epoch: 1691/5000..  Training Loss: 0.962..  Test Loss: 0.941.. \n",
      "Epoch: 1692/5000..  Training Loss: 0.963..  Test Loss: 0.941.. \n",
      "Epoch: 1693/5000..  Training Loss: 0.955..  Test Loss: 0.941.. \n",
      "Epoch: 1694/5000..  Training Loss: 0.952..  Test Loss: 0.941.. \n",
      "Epoch: 1695/5000..  Training Loss: 0.909..  Test Loss: 0.941.. \n",
      "Epoch: 1696/5000..  Training Loss: 0.959..  Test Loss: 0.941.. \n",
      "Epoch: 1697/5000..  Training Loss: 0.940..  Test Loss: 0.941.. \n",
      "Epoch: 1698/5000..  Training Loss: 0.981..  Test Loss: 0.941.. \n",
      "Epoch: 1699/5000..  Training Loss: 0.941..  Test Loss: 0.941.. \n",
      "Epoch: 1700/5000..  Training Loss: 0.944..  Test Loss: 0.941.. \n",
      "Epoch: 1701/5000..  Training Loss: 0.947..  Test Loss: 0.941.. \n",
      "Epoch: 1702/5000..  Training Loss: 0.964..  Test Loss: 0.941.. \n",
      "Epoch: 1703/5000..  Training Loss: 0.904..  Test Loss: 0.941.. \n",
      "Epoch: 1704/5000..  Training Loss: 0.941..  Test Loss: 0.941.. \n",
      "Epoch: 1705/5000..  Training Loss: 0.947..  Test Loss: 0.941.. \n",
      "Epoch: 1706/5000..  Training Loss: 0.902..  Test Loss: 0.941.. \n",
      "Epoch: 1707/5000..  Training Loss: 0.937..  Test Loss: 0.941.. \n",
      "Epoch: 1708/5000..  Training Loss: 0.948..  Test Loss: 0.941.. \n",
      "Epoch: 1709/5000..  Training Loss: 0.925..  Test Loss: 0.941.. \n",
      "Epoch: 1710/5000..  Training Loss: 0.927..  Test Loss: 0.941.. \n",
      "Epoch: 1711/5000..  Training Loss: 0.931..  Test Loss: 0.941.. \n",
      "Epoch: 1712/5000..  Training Loss: 0.962..  Test Loss: 0.941.. \n",
      "Epoch: 1713/5000..  Training Loss: 0.948..  Test Loss: 0.941.. \n",
      "Epoch: 1714/5000..  Training Loss: 0.937..  Test Loss: 0.941.. \n",
      "Epoch: 1715/5000..  Training Loss: 0.956..  Test Loss: 0.941.. \n",
      "Epoch: 1716/5000..  Training Loss: 0.920..  Test Loss: 0.941.. \n",
      "Epoch: 1717/5000..  Training Loss: 0.943..  Test Loss: 0.941.. \n",
      "Epoch: 1718/5000..  Training Loss: 0.958..  Test Loss: 0.941.. \n",
      "Epoch: 1719/5000..  Training Loss: 0.931..  Test Loss: 0.941.. \n",
      "Epoch: 1720/5000..  Training Loss: 0.927..  Test Loss: 0.941.. \n",
      "Epoch: 1721/5000..  Training Loss: 0.917..  Test Loss: 0.941.. \n",
      "Epoch: 1722/5000..  Training Loss: 0.934..  Test Loss: 0.941.. \n",
      "Epoch: 1723/5000..  Training Loss: 0.935..  Test Loss: 0.941.. \n",
      "Epoch: 1724/5000..  Training Loss: 0.923..  Test Loss: 0.941.. \n",
      "Epoch: 1725/5000..  Training Loss: 0.935..  Test Loss: 0.941.. \n",
      "Epoch: 1726/5000..  Training Loss: 0.994..  Test Loss: 0.941.. \n",
      "Epoch: 1727/5000..  Training Loss: 0.960..  Test Loss: 0.941.. \n",
      "Epoch: 1728/5000..  Training Loss: 0.964..  Test Loss: 0.941.. \n",
      "Epoch: 1729/5000..  Training Loss: 0.939..  Test Loss: 0.941.. \n",
      "Epoch: 1730/5000..  Training Loss: 0.953..  Test Loss: 0.941.. \n",
      "Epoch: 1731/5000..  Training Loss: 0.944..  Test Loss: 0.941.. \n",
      "Epoch: 1732/5000..  Training Loss: 0.921..  Test Loss: 0.941.. \n",
      "Epoch: 1733/5000..  Training Loss: 0.972..  Test Loss: 0.941.. \n",
      "Epoch: 1734/5000..  Training Loss: 0.939..  Test Loss: 0.941.. \n",
      "Epoch: 1735/5000..  Training Loss: 0.907..  Test Loss: 0.941.. \n",
      "Epoch: 1736/5000..  Training Loss: 0.913..  Test Loss: 0.941.. \n",
      "Epoch: 1737/5000..  Training Loss: 0.949..  Test Loss: 0.941.. \n",
      "Epoch: 1738/5000..  Training Loss: 0.935..  Test Loss: 0.941.. \n",
      "Epoch: 1739/5000..  Training Loss: 0.932..  Test Loss: 0.941.. \n",
      "Epoch: 1740/5000..  Training Loss: 0.953..  Test Loss: 0.941.. \n",
      "Epoch: 1741/5000..  Training Loss: 0.944..  Test Loss: 0.941.. \n",
      "Epoch: 1742/5000..  Training Loss: 0.933..  Test Loss: 0.941.. \n",
      "Epoch: 1743/5000..  Training Loss: 0.974..  Test Loss: 0.941.. \n",
      "Epoch: 1744/5000..  Training Loss: 0.965..  Test Loss: 0.941.. \n",
      "Epoch: 1745/5000..  Training Loss: 0.890..  Test Loss: 0.941.. \n",
      "Epoch: 1746/5000..  Training Loss: 0.943..  Test Loss: 0.941.. \n",
      "Epoch: 1747/5000..  Training Loss: 0.985..  Test Loss: 0.941.. \n",
      "Epoch: 1748/5000..  Training Loss: 0.922..  Test Loss: 0.941.. \n",
      "Epoch: 1749/5000..  Training Loss: 0.971..  Test Loss: 0.941.. \n",
      "Epoch: 1750/5000..  Training Loss: 0.903..  Test Loss: 0.941.. \n",
      "Epoch: 1751/5000..  Training Loss: 0.932..  Test Loss: 0.941.. \n",
      "Epoch: 1752/5000..  Training Loss: 0.923..  Test Loss: 0.941.. \n",
      "Epoch: 1753/5000..  Training Loss: 0.923..  Test Loss: 0.941.. \n",
      "Epoch: 1754/5000..  Training Loss: 0.975..  Test Loss: 0.941.. \n",
      "Epoch: 1755/5000..  Training Loss: 0.919..  Test Loss: 0.941.. \n",
      "Epoch: 1756/5000..  Training Loss: 0.939..  Test Loss: 0.941.. \n",
      "Epoch: 1757/5000..  Training Loss: 0.987..  Test Loss: 0.941.. \n",
      "Epoch: 1758/5000..  Training Loss: 0.951..  Test Loss: 0.941.. \n",
      "Epoch: 1759/5000..  Training Loss: 0.923..  Test Loss: 0.941.. \n",
      "Epoch: 1760/5000..  Training Loss: 0.946..  Test Loss: 0.941.. \n",
      "Epoch: 1761/5000..  Training Loss: 0.925..  Test Loss: 0.941.. \n",
      "Epoch: 1762/5000..  Training Loss: 0.942..  Test Loss: 0.941.. \n",
      "Epoch: 1763/5000..  Training Loss: 0.938..  Test Loss: 0.941.. \n",
      "Epoch: 1764/5000..  Training Loss: 0.935..  Test Loss: 0.941.. \n",
      "Epoch: 1765/5000..  Training Loss: 0.980..  Test Loss: 0.941.. \n",
      "Epoch: 1766/5000..  Training Loss: 0.923..  Test Loss: 0.941.. \n",
      "Epoch: 1767/5000..  Training Loss: 0.956..  Test Loss: 0.941.. \n",
      "Epoch: 1768/5000..  Training Loss: 0.932..  Test Loss: 0.941.. \n",
      "Epoch: 1769/5000..  Training Loss: 0.938..  Test Loss: 0.941.. \n",
      "Epoch: 1770/5000..  Training Loss: 0.966..  Test Loss: 0.941.. \n",
      "Epoch: 1771/5000..  Training Loss: 0.910..  Test Loss: 0.941.. \n",
      "Epoch: 1772/5000..  Training Loss: 0.937..  Test Loss: 0.941.. \n",
      "Epoch: 1773/5000..  Training Loss: 0.944..  Test Loss: 0.941.. \n",
      "Epoch: 1774/5000..  Training Loss: 0.948..  Test Loss: 0.941.. \n",
      "Epoch: 1775/5000..  Training Loss: 0.921..  Test Loss: 0.941.. \n",
      "Epoch: 1776/5000..  Training Loss: 0.960..  Test Loss: 0.941.. \n",
      "Epoch: 1777/5000..  Training Loss: 0.940..  Test Loss: 0.941.. \n",
      "Epoch: 1778/5000..  Training Loss: 0.945..  Test Loss: 0.941.. \n",
      "Epoch: 1779/5000..  Training Loss: 0.968..  Test Loss: 0.941.. \n",
      "Epoch: 1780/5000..  Training Loss: 0.967..  Test Loss: 0.941.. \n",
      "Epoch: 1781/5000..  Training Loss: 0.961..  Test Loss: 0.941.. \n",
      "Epoch: 1782/5000..  Training Loss: 0.923..  Test Loss: 0.941.. \n",
      "Epoch: 1783/5000..  Training Loss: 0.923..  Test Loss: 0.941.. \n",
      "Epoch: 1784/5000..  Training Loss: 0.934..  Test Loss: 0.941.. \n",
      "Epoch: 1785/5000..  Training Loss: 0.916..  Test Loss: 0.941.. \n",
      "Epoch: 1786/5000..  Training Loss: 0.976..  Test Loss: 0.941.. \n",
      "Epoch: 1787/5000..  Training Loss: 0.962..  Test Loss: 0.941.. \n",
      "Epoch: 1788/5000..  Training Loss: 0.930..  Test Loss: 0.940.. \n",
      "Epoch: 1789/5000..  Training Loss: 0.922..  Test Loss: 0.940.. \n",
      "Epoch: 1790/5000..  Training Loss: 0.951..  Test Loss: 0.940.. \n",
      "Epoch: 1791/5000..  Training Loss: 0.933..  Test Loss: 0.940.. \n",
      "Epoch: 1792/5000..  Training Loss: 0.937..  Test Loss: 0.940.. \n",
      "Epoch: 1793/5000..  Training Loss: 0.967..  Test Loss: 0.940.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1794/5000..  Training Loss: 0.953..  Test Loss: 0.940.. \n",
      "Epoch: 1795/5000..  Training Loss: 0.954..  Test Loss: 0.940.. \n",
      "Epoch: 1796/5000..  Training Loss: 0.960..  Test Loss: 0.940.. \n",
      "Epoch: 1797/5000..  Training Loss: 0.948..  Test Loss: 0.940.. \n",
      "Epoch: 1798/5000..  Training Loss: 0.972..  Test Loss: 0.940.. \n",
      "Epoch: 1799/5000..  Training Loss: 0.940..  Test Loss: 0.940.. \n",
      "Epoch: 1800/5000..  Training Loss: 0.937..  Test Loss: 0.940.. \n",
      "Epoch: 1801/5000..  Training Loss: 0.938..  Test Loss: 0.940.. \n",
      "Epoch: 1802/5000..  Training Loss: 0.969..  Test Loss: 0.940.. \n",
      "Epoch: 1803/5000..  Training Loss: 0.943..  Test Loss: 0.940.. \n",
      "Epoch: 1804/5000..  Training Loss: 0.969..  Test Loss: 0.940.. \n",
      "Epoch: 1805/5000..  Training Loss: 0.933..  Test Loss: 0.940.. \n",
      "Epoch: 1806/5000..  Training Loss: 0.951..  Test Loss: 0.940.. \n",
      "Epoch: 1807/5000..  Training Loss: 0.980..  Test Loss: 0.940.. \n",
      "Epoch: 1808/5000..  Training Loss: 0.921..  Test Loss: 0.940.. \n",
      "Epoch: 1809/5000..  Training Loss: 0.937..  Test Loss: 0.940.. \n",
      "Epoch: 1810/5000..  Training Loss: 0.926..  Test Loss: 0.940.. \n",
      "Epoch: 1811/5000..  Training Loss: 0.906..  Test Loss: 0.940.. \n",
      "Epoch: 1812/5000..  Training Loss: 0.937..  Test Loss: 0.940.. \n",
      "Epoch: 1813/5000..  Training Loss: 0.946..  Test Loss: 0.940.. \n",
      "Epoch: 1814/5000..  Training Loss: 0.919..  Test Loss: 0.940.. \n",
      "Epoch: 1815/5000..  Training Loss: 0.914..  Test Loss: 0.940.. \n",
      "Epoch: 1816/5000..  Training Loss: 0.918..  Test Loss: 0.940.. \n",
      "Epoch: 1817/5000..  Training Loss: 0.956..  Test Loss: 0.940.. \n",
      "Epoch: 1818/5000..  Training Loss: 0.939..  Test Loss: 0.940.. \n",
      "Epoch: 1819/5000..  Training Loss: 0.970..  Test Loss: 0.940.. \n",
      "Epoch: 1820/5000..  Training Loss: 0.930..  Test Loss: 0.940.. \n",
      "Epoch: 1821/5000..  Training Loss: 0.950..  Test Loss: 0.940.. \n",
      "Epoch: 1822/5000..  Training Loss: 0.943..  Test Loss: 0.940.. \n",
      "Epoch: 1823/5000..  Training Loss: 0.951..  Test Loss: 0.940.. \n",
      "Epoch: 1824/5000..  Training Loss: 0.924..  Test Loss: 0.940.. \n",
      "Epoch: 1825/5000..  Training Loss: 0.943..  Test Loss: 0.940.. \n",
      "Epoch: 1826/5000..  Training Loss: 0.935..  Test Loss: 0.940.. \n",
      "Epoch: 1827/5000..  Training Loss: 0.920..  Test Loss: 0.940.. \n",
      "Epoch: 1828/5000..  Training Loss: 0.973..  Test Loss: 0.940.. \n",
      "Epoch: 1829/5000..  Training Loss: 0.961..  Test Loss: 0.940.. \n",
      "Epoch: 1830/5000..  Training Loss: 0.931..  Test Loss: 0.940.. \n",
      "Epoch: 1831/5000..  Training Loss: 0.940..  Test Loss: 0.940.. \n",
      "Epoch: 1832/5000..  Training Loss: 0.934..  Test Loss: 0.940.. \n",
      "Epoch: 1833/5000..  Training Loss: 0.937..  Test Loss: 0.940.. \n",
      "Epoch: 1834/5000..  Training Loss: 0.940..  Test Loss: 0.940.. \n",
      "Epoch: 1835/5000..  Training Loss: 0.938..  Test Loss: 0.940.. \n",
      "Epoch: 1836/5000..  Training Loss: 0.940..  Test Loss: 0.940.. \n",
      "Epoch: 1837/5000..  Training Loss: 0.932..  Test Loss: 0.940.. \n",
      "Epoch: 1838/5000..  Training Loss: 0.958..  Test Loss: 0.940.. \n",
      "Epoch: 1839/5000..  Training Loss: 0.952..  Test Loss: 0.940.. \n",
      "Epoch: 1840/5000..  Training Loss: 0.938..  Test Loss: 0.940.. \n",
      "Epoch: 1841/5000..  Training Loss: 0.954..  Test Loss: 0.940.. \n",
      "Epoch: 1842/5000..  Training Loss: 0.935..  Test Loss: 0.940.. \n",
      "Epoch: 1843/5000..  Training Loss: 0.933..  Test Loss: 0.940.. \n",
      "Epoch: 1844/5000..  Training Loss: 0.948..  Test Loss: 0.940.. \n",
      "Epoch: 1845/5000..  Training Loss: 0.940..  Test Loss: 0.940.. \n",
      "Epoch: 1846/5000..  Training Loss: 0.908..  Test Loss: 0.940.. \n",
      "Epoch: 1847/5000..  Training Loss: 0.933..  Test Loss: 0.940.. \n",
      "Epoch: 1848/5000..  Training Loss: 0.910..  Test Loss: 0.940.. \n",
      "Epoch: 1849/5000..  Training Loss: 0.940..  Test Loss: 0.940.. \n",
      "Epoch: 1850/5000..  Training Loss: 0.934..  Test Loss: 0.940.. \n",
      "Epoch: 1851/5000..  Training Loss: 0.946..  Test Loss: 0.940.. \n",
      "Epoch: 1852/5000..  Training Loss: 0.958..  Test Loss: 0.940.. \n",
      "Epoch: 1853/5000..  Training Loss: 0.974..  Test Loss: 0.940.. \n",
      "Epoch: 1854/5000..  Training Loss: 0.923..  Test Loss: 0.940.. \n",
      "Epoch: 1855/5000..  Training Loss: 0.949..  Test Loss: 0.940.. \n",
      "Epoch: 1856/5000..  Training Loss: 0.936..  Test Loss: 0.940.. \n",
      "Epoch: 1857/5000..  Training Loss: 0.939..  Test Loss: 0.940.. \n",
      "Epoch: 1858/5000..  Training Loss: 0.971..  Test Loss: 0.940.. \n",
      "Epoch: 1859/5000..  Training Loss: 0.937..  Test Loss: 0.940.. \n",
      "Epoch: 1860/5000..  Training Loss: 0.958..  Test Loss: 0.940.. \n",
      "Epoch: 1861/5000..  Training Loss: 0.932..  Test Loss: 0.940.. \n",
      "Epoch: 1862/5000..  Training Loss: 0.952..  Test Loss: 0.940.. \n",
      "Epoch: 1863/5000..  Training Loss: 0.943..  Test Loss: 0.940.. \n",
      "Epoch: 1864/5000..  Training Loss: 0.932..  Test Loss: 0.940.. \n",
      "Epoch: 1865/5000..  Training Loss: 0.958..  Test Loss: 0.940.. \n",
      "Epoch: 1866/5000..  Training Loss: 0.925..  Test Loss: 0.940.. \n",
      "Epoch: 1867/5000..  Training Loss: 0.925..  Test Loss: 0.940.. \n",
      "Epoch: 1868/5000..  Training Loss: 0.931..  Test Loss: 0.940.. \n",
      "Epoch: 1869/5000..  Training Loss: 0.933..  Test Loss: 0.940.. \n",
      "Epoch: 1870/5000..  Training Loss: 0.957..  Test Loss: 0.940.. \n",
      "Epoch: 1871/5000..  Training Loss: 0.962..  Test Loss: 0.940.. \n",
      "Epoch: 1872/5000..  Training Loss: 0.947..  Test Loss: 0.940.. \n",
      "Epoch: 1873/5000..  Training Loss: 0.944..  Test Loss: 0.940.. \n",
      "Epoch: 1874/5000..  Training Loss: 0.941..  Test Loss: 0.940.. \n",
      "Epoch: 1875/5000..  Training Loss: 0.941..  Test Loss: 0.940.. \n",
      "Epoch: 1876/5000..  Training Loss: 0.924..  Test Loss: 0.940.. \n",
      "Epoch: 1877/5000..  Training Loss: 0.917..  Test Loss: 0.940.. \n",
      "Epoch: 1878/5000..  Training Loss: 0.939..  Test Loss: 0.940.. \n",
      "Epoch: 1879/5000..  Training Loss: 0.924..  Test Loss: 0.940.. \n",
      "Epoch: 1880/5000..  Training Loss: 0.919..  Test Loss: 0.940.. \n",
      "Epoch: 1881/5000..  Training Loss: 0.966..  Test Loss: 0.940.. \n",
      "Epoch: 1882/5000..  Training Loss: 0.937..  Test Loss: 0.940.. \n",
      "Epoch: 1883/5000..  Training Loss: 0.943..  Test Loss: 0.940.. \n",
      "Epoch: 1884/5000..  Training Loss: 0.970..  Test Loss: 0.940.. \n",
      "Epoch: 1885/5000..  Training Loss: 0.946..  Test Loss: 0.940.. \n",
      "Epoch: 1886/5000..  Training Loss: 0.918..  Test Loss: 0.940.. \n",
      "Epoch: 1887/5000..  Training Loss: 0.918..  Test Loss: 0.940.. \n",
      "Epoch: 1888/5000..  Training Loss: 0.940..  Test Loss: 0.940.. \n",
      "Epoch: 1889/5000..  Training Loss: 0.929..  Test Loss: 0.940.. \n",
      "Epoch: 1890/5000..  Training Loss: 0.925..  Test Loss: 0.940.. \n",
      "Epoch: 1891/5000..  Training Loss: 0.972..  Test Loss: 0.940.. \n",
      "Epoch: 1892/5000..  Training Loss: 0.915..  Test Loss: 0.940.. \n",
      "Epoch: 1893/5000..  Training Loss: 0.936..  Test Loss: 0.940.. \n",
      "Epoch: 1894/5000..  Training Loss: 0.926..  Test Loss: 0.940.. \n",
      "Epoch: 1895/5000..  Training Loss: 0.950..  Test Loss: 0.940.. \n",
      "Epoch: 1896/5000..  Training Loss: 0.948..  Test Loss: 0.940.. \n",
      "Epoch: 1897/5000..  Training Loss: 0.950..  Test Loss: 0.940.. \n",
      "Epoch: 1898/5000..  Training Loss: 0.940..  Test Loss: 0.939.. \n",
      "Epoch: 1899/5000..  Training Loss: 0.969..  Test Loss: 0.939.. \n",
      "Epoch: 1900/5000..  Training Loss: 0.913..  Test Loss: 0.939.. \n",
      "Epoch: 1901/5000..  Training Loss: 0.985..  Test Loss: 0.939.. \n",
      "Epoch: 1902/5000..  Training Loss: 0.951..  Test Loss: 0.939.. \n",
      "Epoch: 1903/5000..  Training Loss: 0.945..  Test Loss: 0.939.. \n",
      "Epoch: 1904/5000..  Training Loss: 0.932..  Test Loss: 0.939.. \n",
      "Epoch: 1905/5000..  Training Loss: 0.972..  Test Loss: 0.939.. \n",
      "Epoch: 1906/5000..  Training Loss: 0.949..  Test Loss: 0.939.. \n",
      "Epoch: 1907/5000..  Training Loss: 0.944..  Test Loss: 0.939.. \n",
      "Epoch: 1908/5000..  Training Loss: 0.940..  Test Loss: 0.939.. \n",
      "Epoch: 1909/5000..  Training Loss: 0.946..  Test Loss: 0.939.. \n",
      "Epoch: 1910/5000..  Training Loss: 0.931..  Test Loss: 0.939.. \n",
      "Epoch: 1911/5000..  Training Loss: 0.951..  Test Loss: 0.939.. \n",
      "Epoch: 1912/5000..  Training Loss: 0.956..  Test Loss: 0.939.. \n",
      "Epoch: 1913/5000..  Training Loss: 0.889..  Test Loss: 0.939.. \n",
      "Epoch: 1914/5000..  Training Loss: 0.980..  Test Loss: 0.939.. \n",
      "Epoch: 1915/5000..  Training Loss: 0.907..  Test Loss: 0.939.. \n",
      "Epoch: 1916/5000..  Training Loss: 0.967..  Test Loss: 0.939.. \n",
      "Epoch: 1917/5000..  Training Loss: 0.938..  Test Loss: 0.939.. \n",
      "Epoch: 1918/5000..  Training Loss: 0.946..  Test Loss: 0.939.. \n",
      "Epoch: 1919/5000..  Training Loss: 0.928..  Test Loss: 0.939.. \n",
      "Epoch: 1920/5000..  Training Loss: 0.929..  Test Loss: 0.939.. \n",
      "Epoch: 1921/5000..  Training Loss: 0.908..  Test Loss: 0.939.. \n",
      "Epoch: 1922/5000..  Training Loss: 0.946..  Test Loss: 0.939.. \n",
      "Epoch: 1923/5000..  Training Loss: 0.931..  Test Loss: 0.939.. \n",
      "Epoch: 1924/5000..  Training Loss: 0.927..  Test Loss: 0.939.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1925/5000..  Training Loss: 0.940..  Test Loss: 0.939.. \n",
      "Epoch: 1926/5000..  Training Loss: 0.923..  Test Loss: 0.939.. \n",
      "Epoch: 1927/5000..  Training Loss: 0.936..  Test Loss: 0.939.. \n",
      "Epoch: 1928/5000..  Training Loss: 0.947..  Test Loss: 0.939.. \n",
      "Epoch: 1929/5000..  Training Loss: 0.956..  Test Loss: 0.939.. \n",
      "Epoch: 1930/5000..  Training Loss: 0.960..  Test Loss: 0.939.. \n",
      "Epoch: 1931/5000..  Training Loss: 0.928..  Test Loss: 0.939.. \n",
      "Epoch: 1932/5000..  Training Loss: 0.966..  Test Loss: 0.939.. \n",
      "Epoch: 1933/5000..  Training Loss: 0.958..  Test Loss: 0.939.. \n",
      "Epoch: 1934/5000..  Training Loss: 0.937..  Test Loss: 0.939.. \n",
      "Epoch: 1935/5000..  Training Loss: 0.913..  Test Loss: 0.939.. \n",
      "Epoch: 1936/5000..  Training Loss: 0.941..  Test Loss: 0.939.. \n",
      "Epoch: 1937/5000..  Training Loss: 0.934..  Test Loss: 0.939.. \n",
      "Epoch: 1938/5000..  Training Loss: 0.935..  Test Loss: 0.939.. \n",
      "Epoch: 1939/5000..  Training Loss: 0.969..  Test Loss: 0.939.. \n",
      "Epoch: 1940/5000..  Training Loss: 0.986..  Test Loss: 0.939.. \n",
      "Epoch: 1941/5000..  Training Loss: 0.960..  Test Loss: 0.939.. \n",
      "Epoch: 1942/5000..  Training Loss: 0.944..  Test Loss: 0.939.. \n",
      "Epoch: 1943/5000..  Training Loss: 0.949..  Test Loss: 0.939.. \n",
      "Epoch: 1944/5000..  Training Loss: 0.965..  Test Loss: 0.939.. \n",
      "Epoch: 1945/5000..  Training Loss: 0.933..  Test Loss: 0.939.. \n",
      "Epoch: 1946/5000..  Training Loss: 0.941..  Test Loss: 0.939.. \n",
      "Epoch: 1947/5000..  Training Loss: 0.954..  Test Loss: 0.939.. \n",
      "Epoch: 1948/5000..  Training Loss: 0.950..  Test Loss: 0.939.. \n",
      "Epoch: 1949/5000..  Training Loss: 0.938..  Test Loss: 0.939.. \n",
      "Epoch: 1950/5000..  Training Loss: 0.948..  Test Loss: 0.939.. \n",
      "Epoch: 1951/5000..  Training Loss: 0.934..  Test Loss: 0.939.. \n",
      "Epoch: 1952/5000..  Training Loss: 0.931..  Test Loss: 0.939.. \n",
      "Epoch: 1953/5000..  Training Loss: 0.919..  Test Loss: 0.939.. \n",
      "Epoch: 1954/5000..  Training Loss: 0.953..  Test Loss: 0.939.. \n",
      "Epoch: 1955/5000..  Training Loss: 0.973..  Test Loss: 0.939.. \n",
      "Epoch: 1956/5000..  Training Loss: 0.961..  Test Loss: 0.939.. \n",
      "Epoch: 1957/5000..  Training Loss: 0.942..  Test Loss: 0.939.. \n",
      "Epoch: 1958/5000..  Training Loss: 0.934..  Test Loss: 0.939.. \n",
      "Epoch: 1959/5000..  Training Loss: 0.944..  Test Loss: 0.939.. \n",
      "Epoch: 1960/5000..  Training Loss: 0.915..  Test Loss: 0.939.. \n",
      "Epoch: 1961/5000..  Training Loss: 0.954..  Test Loss: 0.939.. \n",
      "Epoch: 1962/5000..  Training Loss: 0.964..  Test Loss: 0.939.. \n",
      "Epoch: 1963/5000..  Training Loss: 0.944..  Test Loss: 0.939.. \n",
      "Epoch: 1964/5000..  Training Loss: 0.959..  Test Loss: 0.939.. \n",
      "Epoch: 1965/5000..  Training Loss: 0.938..  Test Loss: 0.939.. \n",
      "Epoch: 1966/5000..  Training Loss: 0.984..  Test Loss: 0.939.. \n",
      "Epoch: 1967/5000..  Training Loss: 0.953..  Test Loss: 0.939.. \n",
      "Epoch: 1968/5000..  Training Loss: 0.922..  Test Loss: 0.939.. \n",
      "Epoch: 1969/5000..  Training Loss: 0.939..  Test Loss: 0.939.. \n",
      "Epoch: 1970/5000..  Training Loss: 0.959..  Test Loss: 0.939.. \n",
      "Epoch: 1971/5000..  Training Loss: 0.922..  Test Loss: 0.939.. \n",
      "Epoch: 1972/5000..  Training Loss: 0.930..  Test Loss: 0.939.. \n",
      "Epoch: 1973/5000..  Training Loss: 0.922..  Test Loss: 0.939.. \n",
      "Epoch: 1974/5000..  Training Loss: 0.965..  Test Loss: 0.939.. \n",
      "Epoch: 1975/5000..  Training Loss: 0.957..  Test Loss: 0.939.. \n",
      "Epoch: 1976/5000..  Training Loss: 0.935..  Test Loss: 0.939.. \n",
      "Epoch: 1977/5000..  Training Loss: 0.915..  Test Loss: 0.939.. \n",
      "Epoch: 1978/5000..  Training Loss: 0.948..  Test Loss: 0.939.. \n",
      "Epoch: 1979/5000..  Training Loss: 0.920..  Test Loss: 0.939.. \n",
      "Epoch: 1980/5000..  Training Loss: 0.951..  Test Loss: 0.939.. \n",
      "Epoch: 1981/5000..  Training Loss: 0.957..  Test Loss: 0.939.. \n",
      "Epoch: 1982/5000..  Training Loss: 0.945..  Test Loss: 0.939.. \n",
      "Epoch: 1983/5000..  Training Loss: 0.948..  Test Loss: 0.939.. \n",
      "Epoch: 1984/5000..  Training Loss: 0.941..  Test Loss: 0.939.. \n",
      "Epoch: 1985/5000..  Training Loss: 0.915..  Test Loss: 0.939.. \n",
      "Epoch: 1986/5000..  Training Loss: 0.951..  Test Loss: 0.939.. \n",
      "Epoch: 1987/5000..  Training Loss: 0.959..  Test Loss: 0.939.. \n",
      "Epoch: 1988/5000..  Training Loss: 0.925..  Test Loss: 0.939.. \n",
      "Epoch: 1989/5000..  Training Loss: 0.940..  Test Loss: 0.939.. \n",
      "Epoch: 1990/5000..  Training Loss: 0.968..  Test Loss: 0.939.. \n",
      "Epoch: 1991/5000..  Training Loss: 0.928..  Test Loss: 0.939.. \n",
      "Epoch: 1992/5000..  Training Loss: 0.951..  Test Loss: 0.939.. \n",
      "Epoch: 1993/5000..  Training Loss: 0.973..  Test Loss: 0.939.. \n",
      "Epoch: 1994/5000..  Training Loss: 0.944..  Test Loss: 0.939.. \n",
      "Epoch: 1995/5000..  Training Loss: 0.910..  Test Loss: 0.939.. \n",
      "Epoch: 1996/5000..  Training Loss: 0.926..  Test Loss: 0.939.. \n",
      "Epoch: 1997/5000..  Training Loss: 0.910..  Test Loss: 0.939.. \n",
      "Epoch: 1998/5000..  Training Loss: 0.957..  Test Loss: 0.939.. \n",
      "Epoch: 1999/5000..  Training Loss: 0.932..  Test Loss: 0.939.. \n",
      "Epoch: 2000/5000..  Training Loss: 0.940..  Test Loss: 0.939.. \n",
      "Epoch: 2001/5000..  Training Loss: 0.934..  Test Loss: 0.939.. \n",
      "Epoch: 2002/5000..  Training Loss: 0.922..  Test Loss: 0.939.. \n",
      "Epoch: 2003/5000..  Training Loss: 0.963..  Test Loss: 0.939.. \n",
      "Epoch: 2004/5000..  Training Loss: 0.968..  Test Loss: 0.939.. \n",
      "Epoch: 2005/5000..  Training Loss: 0.952..  Test Loss: 0.939.. \n",
      "Epoch: 2006/5000..  Training Loss: 0.951..  Test Loss: 0.939.. \n",
      "Epoch: 2007/5000..  Training Loss: 0.939..  Test Loss: 0.939.. \n",
      "Epoch: 2008/5000..  Training Loss: 0.957..  Test Loss: 0.938.. \n",
      "Epoch: 2009/5000..  Training Loss: 0.921..  Test Loss: 0.938.. \n",
      "Epoch: 2010/5000..  Training Loss: 0.950..  Test Loss: 0.938.. \n",
      "Epoch: 2011/5000..  Training Loss: 0.944..  Test Loss: 0.938.. \n",
      "Epoch: 2012/5000..  Training Loss: 0.898..  Test Loss: 0.938.. \n",
      "Epoch: 2013/5000..  Training Loss: 0.952..  Test Loss: 0.938.. \n",
      "Epoch: 2014/5000..  Training Loss: 0.931..  Test Loss: 0.938.. \n",
      "Epoch: 2015/5000..  Training Loss: 0.954..  Test Loss: 0.938.. \n",
      "Epoch: 2016/5000..  Training Loss: 0.936..  Test Loss: 0.938.. \n",
      "Epoch: 2017/5000..  Training Loss: 0.944..  Test Loss: 0.938.. \n",
      "Epoch: 2018/5000..  Training Loss: 0.941..  Test Loss: 0.938.. \n",
      "Epoch: 2019/5000..  Training Loss: 0.945..  Test Loss: 0.938.. \n",
      "Epoch: 2020/5000..  Training Loss: 0.943..  Test Loss: 0.938.. \n",
      "Epoch: 2021/5000..  Training Loss: 0.959..  Test Loss: 0.938.. \n",
      "Epoch: 2022/5000..  Training Loss: 0.930..  Test Loss: 0.938.. \n",
      "Epoch: 2023/5000..  Training Loss: 0.924..  Test Loss: 0.938.. \n",
      "Epoch: 2024/5000..  Training Loss: 0.931..  Test Loss: 0.938.. \n",
      "Epoch: 2025/5000..  Training Loss: 0.886..  Test Loss: 0.938.. \n",
      "Epoch: 2026/5000..  Training Loss: 0.952..  Test Loss: 0.938.. \n",
      "Epoch: 2027/5000..  Training Loss: 0.966..  Test Loss: 0.938.. \n",
      "Epoch: 2028/5000..  Training Loss: 0.898..  Test Loss: 0.938.. \n",
      "Epoch: 2029/5000..  Training Loss: 0.932..  Test Loss: 0.938.. \n",
      "Epoch: 2030/5000..  Training Loss: 0.932..  Test Loss: 0.938.. \n",
      "Epoch: 2031/5000..  Training Loss: 0.938..  Test Loss: 0.938.. \n",
      "Epoch: 2032/5000..  Training Loss: 0.901..  Test Loss: 0.938.. \n",
      "Epoch: 2033/5000..  Training Loss: 0.958..  Test Loss: 0.938.. \n",
      "Epoch: 2034/5000..  Training Loss: 0.924..  Test Loss: 0.938.. \n",
      "Epoch: 2035/5000..  Training Loss: 0.973..  Test Loss: 0.938.. \n",
      "Epoch: 2036/5000..  Training Loss: 0.935..  Test Loss: 0.938.. \n",
      "Epoch: 2037/5000..  Training Loss: 0.950..  Test Loss: 0.938.. \n",
      "Epoch: 2038/5000..  Training Loss: 0.934..  Test Loss: 0.938.. \n",
      "Epoch: 2039/5000..  Training Loss: 0.899..  Test Loss: 0.938.. \n",
      "Epoch: 2040/5000..  Training Loss: 0.935..  Test Loss: 0.938.. \n",
      "Epoch: 2041/5000..  Training Loss: 0.908..  Test Loss: 0.938.. \n",
      "Epoch: 2042/5000..  Training Loss: 0.961..  Test Loss: 0.938.. \n",
      "Epoch: 2043/5000..  Training Loss: 0.936..  Test Loss: 0.938.. \n",
      "Epoch: 2044/5000..  Training Loss: 0.949..  Test Loss: 0.938.. \n",
      "Epoch: 2045/5000..  Training Loss: 0.944..  Test Loss: 0.938.. \n",
      "Epoch: 2046/5000..  Training Loss: 0.959..  Test Loss: 0.938.. \n",
      "Epoch: 2047/5000..  Training Loss: 0.923..  Test Loss: 0.938.. \n",
      "Epoch: 2048/5000..  Training Loss: 0.918..  Test Loss: 0.938.. \n",
      "Epoch: 2049/5000..  Training Loss: 0.902..  Test Loss: 0.938.. \n",
      "Epoch: 2050/5000..  Training Loss: 0.915..  Test Loss: 0.938.. \n",
      "Epoch: 2051/5000..  Training Loss: 0.907..  Test Loss: 0.938.. \n",
      "Epoch: 2052/5000..  Training Loss: 0.963..  Test Loss: 0.938.. \n",
      "Epoch: 2053/5000..  Training Loss: 0.938..  Test Loss: 0.938.. \n",
      "Epoch: 2054/5000..  Training Loss: 0.908..  Test Loss: 0.938.. \n",
      "Epoch: 2055/5000..  Training Loss: 0.964..  Test Loss: 0.938.. \n",
      "Epoch: 2056/5000..  Training Loss: 0.915..  Test Loss: 0.938.. \n",
      "Epoch: 2057/5000..  Training Loss: 0.961..  Test Loss: 0.938.. \n",
      "Epoch: 2058/5000..  Training Loss: 0.918..  Test Loss: 0.938.. \n",
      "Epoch: 2059/5000..  Training Loss: 0.964..  Test Loss: 0.938.. \n",
      "Epoch: 2060/5000..  Training Loss: 0.918..  Test Loss: 0.938.. \n",
      "Epoch: 2061/5000..  Training Loss: 0.919..  Test Loss: 0.938.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2062/5000..  Training Loss: 0.923..  Test Loss: 0.938.. \n",
      "Epoch: 2063/5000..  Training Loss: 0.959..  Test Loss: 0.938.. \n",
      "Epoch: 2064/5000..  Training Loss: 0.936..  Test Loss: 0.938.. \n",
      "Epoch: 2065/5000..  Training Loss: 0.933..  Test Loss: 0.938.. \n",
      "Epoch: 2066/5000..  Training Loss: 0.924..  Test Loss: 0.938.. \n",
      "Epoch: 2067/5000..  Training Loss: 0.925..  Test Loss: 0.938.. \n",
      "Epoch: 2068/5000..  Training Loss: 0.987..  Test Loss: 0.938.. \n",
      "Epoch: 2069/5000..  Training Loss: 0.944..  Test Loss: 0.938.. \n",
      "Epoch: 2070/5000..  Training Loss: 0.940..  Test Loss: 0.938.. \n",
      "Epoch: 2071/5000..  Training Loss: 0.949..  Test Loss: 0.938.. \n",
      "Epoch: 2072/5000..  Training Loss: 0.957..  Test Loss: 0.938.. \n",
      "Epoch: 2073/5000..  Training Loss: 0.915..  Test Loss: 0.938.. \n",
      "Epoch: 2074/5000..  Training Loss: 0.926..  Test Loss: 0.938.. \n",
      "Epoch: 2075/5000..  Training Loss: 0.916..  Test Loss: 0.938.. \n",
      "Epoch: 2076/5000..  Training Loss: 0.947..  Test Loss: 0.938.. \n",
      "Epoch: 2077/5000..  Training Loss: 0.969..  Test Loss: 0.938.. \n",
      "Epoch: 2078/5000..  Training Loss: 0.923..  Test Loss: 0.938.. \n",
      "Epoch: 2079/5000..  Training Loss: 0.921..  Test Loss: 0.938.. \n",
      "Epoch: 2080/5000..  Training Loss: 0.923..  Test Loss: 0.938.. \n",
      "Epoch: 2081/5000..  Training Loss: 0.910..  Test Loss: 0.938.. \n",
      "Epoch: 2082/5000..  Training Loss: 0.932..  Test Loss: 0.938.. \n",
      "Epoch: 2083/5000..  Training Loss: 0.941..  Test Loss: 0.938.. \n",
      "Epoch: 2084/5000..  Training Loss: 0.927..  Test Loss: 0.938.. \n",
      "Epoch: 2085/5000..  Training Loss: 0.940..  Test Loss: 0.938.. \n",
      "Epoch: 2086/5000..  Training Loss: 0.911..  Test Loss: 0.938.. \n",
      "Epoch: 2087/5000..  Training Loss: 0.933..  Test Loss: 0.938.. \n",
      "Epoch: 2088/5000..  Training Loss: 0.941..  Test Loss: 0.938.. \n",
      "Epoch: 2089/5000..  Training Loss: 0.923..  Test Loss: 0.938.. \n",
      "Epoch: 2090/5000..  Training Loss: 0.936..  Test Loss: 0.938.. \n",
      "Epoch: 2091/5000..  Training Loss: 0.952..  Test Loss: 0.938.. \n",
      "Epoch: 2092/5000..  Training Loss: 0.923..  Test Loss: 0.938.. \n",
      "Epoch: 2093/5000..  Training Loss: 0.920..  Test Loss: 0.938.. \n",
      "Epoch: 2094/5000..  Training Loss: 0.927..  Test Loss: 0.938.. \n",
      "Epoch: 2095/5000..  Training Loss: 0.893..  Test Loss: 0.938.. \n",
      "Epoch: 2096/5000..  Training Loss: 0.894..  Test Loss: 0.938.. \n",
      "Epoch: 2097/5000..  Training Loss: 0.940..  Test Loss: 0.938.. \n",
      "Epoch: 2098/5000..  Training Loss: 0.943..  Test Loss: 0.938.. \n",
      "Epoch: 2099/5000..  Training Loss: 0.953..  Test Loss: 0.938.. \n",
      "Epoch: 2100/5000..  Training Loss: 0.942..  Test Loss: 0.938.. \n",
      "Epoch: 2101/5000..  Training Loss: 0.953..  Test Loss: 0.938.. \n",
      "Epoch: 2102/5000..  Training Loss: 0.922..  Test Loss: 0.938.. \n",
      "Epoch: 2103/5000..  Training Loss: 0.965..  Test Loss: 0.938.. \n",
      "Epoch: 2104/5000..  Training Loss: 0.916..  Test Loss: 0.938.. \n",
      "Epoch: 2105/5000..  Training Loss: 0.944..  Test Loss: 0.938.. \n",
      "Epoch: 2106/5000..  Training Loss: 0.944..  Test Loss: 0.938.. \n",
      "Epoch: 2107/5000..  Training Loss: 0.924..  Test Loss: 0.938.. \n",
      "Epoch: 2108/5000..  Training Loss: 0.919..  Test Loss: 0.938.. \n",
      "Epoch: 2109/5000..  Training Loss: 0.966..  Test Loss: 0.938.. \n",
      "Epoch: 2110/5000..  Training Loss: 0.951..  Test Loss: 0.938.. \n",
      "Epoch: 2111/5000..  Training Loss: 0.950..  Test Loss: 0.938.. \n",
      "Epoch: 2112/5000..  Training Loss: 0.966..  Test Loss: 0.938.. \n",
      "Epoch: 2113/5000..  Training Loss: 0.927..  Test Loss: 0.938.. \n",
      "Epoch: 2114/5000..  Training Loss: 0.959..  Test Loss: 0.938.. \n",
      "Epoch: 2115/5000..  Training Loss: 0.960..  Test Loss: 0.938.. \n",
      "Epoch: 2116/5000..  Training Loss: 0.924..  Test Loss: 0.938.. \n",
      "Epoch: 2117/5000..  Training Loss: 0.935..  Test Loss: 0.938.. \n",
      "Epoch: 2118/5000..  Training Loss: 0.962..  Test Loss: 0.938.. \n",
      "Epoch: 2119/5000..  Training Loss: 0.922..  Test Loss: 0.938.. \n",
      "Epoch: 2120/5000..  Training Loss: 0.970..  Test Loss: 0.938.. \n",
      "Epoch: 2121/5000..  Training Loss: 0.935..  Test Loss: 0.938.. \n",
      "Epoch: 2122/5000..  Training Loss: 0.954..  Test Loss: 0.938.. \n",
      "Epoch: 2123/5000..  Training Loss: 0.934..  Test Loss: 0.938.. \n",
      "Epoch: 2124/5000..  Training Loss: 0.947..  Test Loss: 0.938.. \n",
      "Epoch: 2125/5000..  Training Loss: 0.941..  Test Loss: 0.938.. \n",
      "Epoch: 2126/5000..  Training Loss: 0.929..  Test Loss: 0.938.. \n",
      "Epoch: 2127/5000..  Training Loss: 0.943..  Test Loss: 0.938.. \n",
      "Epoch: 2128/5000..  Training Loss: 0.954..  Test Loss: 0.938.. \n",
      "Epoch: 2129/5000..  Training Loss: 0.933..  Test Loss: 0.938.. \n",
      "Epoch: 2130/5000..  Training Loss: 0.953..  Test Loss: 0.938.. \n",
      "Epoch: 2131/5000..  Training Loss: 0.916..  Test Loss: 0.938.. \n",
      "Epoch: 2132/5000..  Training Loss: 0.926..  Test Loss: 0.938.. \n",
      "Epoch: 2133/5000..  Training Loss: 0.907..  Test Loss: 0.938.. \n",
      "Epoch: 2134/5000..  Training Loss: 0.961..  Test Loss: 0.938.. \n",
      "Epoch: 2135/5000..  Training Loss: 0.949..  Test Loss: 0.938.. \n",
      "Epoch: 2136/5000..  Training Loss: 0.942..  Test Loss: 0.937.. \n",
      "Epoch: 2137/5000..  Training Loss: 0.997..  Test Loss: 0.937.. \n",
      "Epoch: 2138/5000..  Training Loss: 0.943..  Test Loss: 0.937.. \n",
      "Epoch: 2139/5000..  Training Loss: 0.926..  Test Loss: 0.937.. \n",
      "Epoch: 2140/5000..  Training Loss: 0.924..  Test Loss: 0.937.. \n",
      "Epoch: 2141/5000..  Training Loss: 0.919..  Test Loss: 0.937.. \n",
      "Epoch: 2142/5000..  Training Loss: 0.932..  Test Loss: 0.937.. \n",
      "Epoch: 2143/5000..  Training Loss: 0.955..  Test Loss: 0.937.. \n",
      "Epoch: 2144/5000..  Training Loss: 0.976..  Test Loss: 0.937.. \n",
      "Epoch: 2145/5000..  Training Loss: 0.932..  Test Loss: 0.937.. \n",
      "Epoch: 2146/5000..  Training Loss: 0.939..  Test Loss: 0.937.. \n",
      "Epoch: 2147/5000..  Training Loss: 0.939..  Test Loss: 0.937.. \n",
      "Epoch: 2148/5000..  Training Loss: 0.926..  Test Loss: 0.937.. \n",
      "Epoch: 2149/5000..  Training Loss: 0.941..  Test Loss: 0.937.. \n",
      "Epoch: 2150/5000..  Training Loss: 0.955..  Test Loss: 0.937.. \n",
      "Epoch: 2151/5000..  Training Loss: 0.963..  Test Loss: 0.937.. \n",
      "Epoch: 2152/5000..  Training Loss: 0.939..  Test Loss: 0.937.. \n",
      "Epoch: 2153/5000..  Training Loss: 0.930..  Test Loss: 0.937.. \n",
      "Epoch: 2154/5000..  Training Loss: 0.923..  Test Loss: 0.937.. \n",
      "Epoch: 2155/5000..  Training Loss: 0.891..  Test Loss: 0.937.. \n",
      "Epoch: 2156/5000..  Training Loss: 0.963..  Test Loss: 0.937.. \n",
      "Epoch: 2157/5000..  Training Loss: 0.931..  Test Loss: 0.937.. \n",
      "Epoch: 2158/5000..  Training Loss: 0.986..  Test Loss: 0.937.. \n",
      "Epoch: 2159/5000..  Training Loss: 0.931..  Test Loss: 0.937.. \n",
      "Epoch: 2160/5000..  Training Loss: 0.943..  Test Loss: 0.937.. \n",
      "Epoch: 2161/5000..  Training Loss: 0.952..  Test Loss: 0.937.. \n",
      "Epoch: 2162/5000..  Training Loss: 0.912..  Test Loss: 0.937.. \n",
      "Epoch: 2163/5000..  Training Loss: 0.933..  Test Loss: 0.937.. \n",
      "Epoch: 2164/5000..  Training Loss: 0.913..  Test Loss: 0.937.. \n",
      "Epoch: 2165/5000..  Training Loss: 0.940..  Test Loss: 0.937.. \n",
      "Epoch: 2166/5000..  Training Loss: 0.973..  Test Loss: 0.937.. \n",
      "Epoch: 2167/5000..  Training Loss: 0.947..  Test Loss: 0.937.. \n",
      "Epoch: 2168/5000..  Training Loss: 0.950..  Test Loss: 0.937.. \n",
      "Epoch: 2169/5000..  Training Loss: 0.927..  Test Loss: 0.937.. \n",
      "Epoch: 2170/5000..  Training Loss: 0.945..  Test Loss: 0.937.. \n",
      "Epoch: 2171/5000..  Training Loss: 0.909..  Test Loss: 0.937.. \n",
      "Epoch: 2172/5000..  Training Loss: 0.924..  Test Loss: 0.937.. \n",
      "Epoch: 2173/5000..  Training Loss: 0.945..  Test Loss: 0.937.. \n",
      "Epoch: 2174/5000..  Training Loss: 0.911..  Test Loss: 0.937.. \n",
      "Epoch: 2175/5000..  Training Loss: 0.948..  Test Loss: 0.937.. \n",
      "Epoch: 2176/5000..  Training Loss: 0.942..  Test Loss: 0.937.. \n",
      "Epoch: 2177/5000..  Training Loss: 0.988..  Test Loss: 0.937.. \n",
      "Epoch: 2178/5000..  Training Loss: 0.939..  Test Loss: 0.937.. \n",
      "Epoch: 2179/5000..  Training Loss: 0.940..  Test Loss: 0.937.. \n",
      "Epoch: 2180/5000..  Training Loss: 0.934..  Test Loss: 0.937.. \n",
      "Epoch: 2181/5000..  Training Loss: 0.938..  Test Loss: 0.937.. \n",
      "Epoch: 2182/5000..  Training Loss: 0.916..  Test Loss: 0.937.. \n",
      "Epoch: 2183/5000..  Training Loss: 0.967..  Test Loss: 0.937.. \n",
      "Epoch: 2184/5000..  Training Loss: 0.932..  Test Loss: 0.937.. \n",
      "Epoch: 2185/5000..  Training Loss: 0.921..  Test Loss: 0.937.. \n",
      "Epoch: 2186/5000..  Training Loss: 0.946..  Test Loss: 0.937.. \n",
      "Epoch: 2187/5000..  Training Loss: 0.957..  Test Loss: 0.937.. \n",
      "Epoch: 2188/5000..  Training Loss: 0.927..  Test Loss: 0.937.. \n",
      "Epoch: 2189/5000..  Training Loss: 0.926..  Test Loss: 0.937.. \n",
      "Epoch: 2190/5000..  Training Loss: 0.947..  Test Loss: 0.937.. \n",
      "Epoch: 2191/5000..  Training Loss: 0.920..  Test Loss: 0.937.. \n",
      "Epoch: 2192/5000..  Training Loss: 0.932..  Test Loss: 0.937.. \n",
      "Epoch: 2193/5000..  Training Loss: 0.930..  Test Loss: 0.937.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2194/5000..  Training Loss: 0.936..  Test Loss: 0.937.. \n",
      "Epoch: 2195/5000..  Training Loss: 0.944..  Test Loss: 0.937.. \n",
      "Epoch: 2196/5000..  Training Loss: 0.936..  Test Loss: 0.937.. \n",
      "Epoch: 2197/5000..  Training Loss: 0.927..  Test Loss: 0.937.. \n",
      "Epoch: 2198/5000..  Training Loss: 0.923..  Test Loss: 0.937.. \n",
      "Epoch: 2199/5000..  Training Loss: 0.947..  Test Loss: 0.937.. \n",
      "Epoch: 2200/5000..  Training Loss: 0.945..  Test Loss: 0.937.. \n",
      "Epoch: 2201/5000..  Training Loss: 0.934..  Test Loss: 0.937.. \n",
      "Epoch: 2202/5000..  Training Loss: 0.929..  Test Loss: 0.937.. \n",
      "Epoch: 2203/5000..  Training Loss: 0.958..  Test Loss: 0.937.. \n",
      "Epoch: 2204/5000..  Training Loss: 0.925..  Test Loss: 0.937.. \n",
      "Epoch: 2205/5000..  Training Loss: 0.918..  Test Loss: 0.937.. \n",
      "Epoch: 2206/5000..  Training Loss: 0.928..  Test Loss: 0.937.. \n",
      "Epoch: 2207/5000..  Training Loss: 0.959..  Test Loss: 0.937.. \n",
      "Epoch: 2208/5000..  Training Loss: 0.930..  Test Loss: 0.937.. \n",
      "Epoch: 2209/5000..  Training Loss: 0.904..  Test Loss: 0.937.. \n",
      "Epoch: 2210/5000..  Training Loss: 0.902..  Test Loss: 0.937.. \n",
      "Epoch: 2211/5000..  Training Loss: 0.935..  Test Loss: 0.937.. \n",
      "Epoch: 2212/5000..  Training Loss: 0.932..  Test Loss: 0.937.. \n",
      "Epoch: 2213/5000..  Training Loss: 0.949..  Test Loss: 0.937.. \n",
      "Epoch: 2214/5000..  Training Loss: 0.954..  Test Loss: 0.937.. \n",
      "Epoch: 2215/5000..  Training Loss: 0.903..  Test Loss: 0.937.. \n",
      "Epoch: 2216/5000..  Training Loss: 0.917..  Test Loss: 0.937.. \n",
      "Epoch: 2217/5000..  Training Loss: 0.932..  Test Loss: 0.937.. \n",
      "Epoch: 2218/5000..  Training Loss: 0.897..  Test Loss: 0.937.. \n",
      "Epoch: 2219/5000..  Training Loss: 0.928..  Test Loss: 0.937.. \n",
      "Epoch: 2220/5000..  Training Loss: 0.935..  Test Loss: 0.937.. \n",
      "Epoch: 2221/5000..  Training Loss: 0.922..  Test Loss: 0.937.. \n",
      "Epoch: 2222/5000..  Training Loss: 0.915..  Test Loss: 0.937.. \n",
      "Epoch: 2223/5000..  Training Loss: 0.919..  Test Loss: 0.937.. \n",
      "Epoch: 2224/5000..  Training Loss: 0.894..  Test Loss: 0.937.. \n",
      "Epoch: 2225/5000..  Training Loss: 0.919..  Test Loss: 0.937.. \n",
      "Epoch: 2226/5000..  Training Loss: 0.932..  Test Loss: 0.937.. \n",
      "Epoch: 2227/5000..  Training Loss: 0.927..  Test Loss: 0.937.. \n",
      "Epoch: 2228/5000..  Training Loss: 0.944..  Test Loss: 0.937.. \n",
      "Epoch: 2229/5000..  Training Loss: 0.886..  Test Loss: 0.937.. \n",
      "Epoch: 2230/5000..  Training Loss: 0.918..  Test Loss: 0.937.. \n",
      "Epoch: 2231/5000..  Training Loss: 0.923..  Test Loss: 0.937.. \n",
      "Epoch: 2232/5000..  Training Loss: 0.961..  Test Loss: 0.937.. \n",
      "Epoch: 2233/5000..  Training Loss: 0.922..  Test Loss: 0.937.. \n",
      "Epoch: 2234/5000..  Training Loss: 0.937..  Test Loss: 0.937.. \n",
      "Epoch: 2235/5000..  Training Loss: 0.952..  Test Loss: 0.937.. \n",
      "Epoch: 2236/5000..  Training Loss: 0.944..  Test Loss: 0.937.. \n",
      "Epoch: 2237/5000..  Training Loss: 0.953..  Test Loss: 0.937.. \n",
      "Epoch: 2238/5000..  Training Loss: 0.950..  Test Loss: 0.937.. \n",
      "Epoch: 2239/5000..  Training Loss: 0.917..  Test Loss: 0.937.. \n",
      "Epoch: 2240/5000..  Training Loss: 0.959..  Test Loss: 0.937.. \n",
      "Epoch: 2241/5000..  Training Loss: 0.909..  Test Loss: 0.937.. \n",
      "Epoch: 2242/5000..  Training Loss: 0.936..  Test Loss: 0.937.. \n",
      "Epoch: 2243/5000..  Training Loss: 0.942..  Test Loss: 0.937.. \n",
      "Epoch: 2244/5000..  Training Loss: 0.947..  Test Loss: 0.937.. \n",
      "Epoch: 2245/5000..  Training Loss: 0.913..  Test Loss: 0.937.. \n",
      "Epoch: 2246/5000..  Training Loss: 0.933..  Test Loss: 0.937.. \n",
      "Epoch: 2247/5000..  Training Loss: 0.941..  Test Loss: 0.937.. \n",
      "Epoch: 2248/5000..  Training Loss: 0.925..  Test Loss: 0.937.. \n",
      "Epoch: 2249/5000..  Training Loss: 0.926..  Test Loss: 0.937.. \n",
      "Epoch: 2250/5000..  Training Loss: 0.960..  Test Loss: 0.937.. \n",
      "Epoch: 2251/5000..  Training Loss: 0.969..  Test Loss: 0.937.. \n",
      "Epoch: 2252/5000..  Training Loss: 0.914..  Test Loss: 0.937.. \n",
      "Epoch: 2253/5000..  Training Loss: 0.960..  Test Loss: 0.937.. \n",
      "Epoch: 2254/5000..  Training Loss: 0.947..  Test Loss: 0.937.. \n",
      "Epoch: 2255/5000..  Training Loss: 0.924..  Test Loss: 0.937.. \n",
      "Epoch: 2256/5000..  Training Loss: 0.919..  Test Loss: 0.936.. \n",
      "Epoch: 2257/5000..  Training Loss: 0.905..  Test Loss: 0.936.. \n",
      "Epoch: 2258/5000..  Training Loss: 0.908..  Test Loss: 0.936.. \n",
      "Epoch: 2259/5000..  Training Loss: 0.940..  Test Loss: 0.936.. \n",
      "Epoch: 2260/5000..  Training Loss: 0.925..  Test Loss: 0.936.. \n",
      "Epoch: 2261/5000..  Training Loss: 0.942..  Test Loss: 0.936.. \n",
      "Epoch: 2262/5000..  Training Loss: 0.934..  Test Loss: 0.936.. \n",
      "Epoch: 2263/5000..  Training Loss: 0.934..  Test Loss: 0.936.. \n",
      "Epoch: 2264/5000..  Training Loss: 0.950..  Test Loss: 0.936.. \n",
      "Epoch: 2265/5000..  Training Loss: 0.931..  Test Loss: 0.936.. \n",
      "Epoch: 2266/5000..  Training Loss: 0.959..  Test Loss: 0.936.. \n",
      "Epoch: 2267/5000..  Training Loss: 0.905..  Test Loss: 0.936.. \n",
      "Epoch: 2268/5000..  Training Loss: 0.956..  Test Loss: 0.936.. \n",
      "Epoch: 2269/5000..  Training Loss: 0.971..  Test Loss: 0.936.. \n",
      "Epoch: 2270/5000..  Training Loss: 0.919..  Test Loss: 0.936.. \n",
      "Epoch: 2271/5000..  Training Loss: 0.915..  Test Loss: 0.936.. \n",
      "Epoch: 2272/5000..  Training Loss: 0.908..  Test Loss: 0.936.. \n",
      "Epoch: 2273/5000..  Training Loss: 0.933..  Test Loss: 0.936.. \n",
      "Epoch: 2274/5000..  Training Loss: 0.927..  Test Loss: 0.936.. \n",
      "Epoch: 2275/5000..  Training Loss: 0.935..  Test Loss: 0.936.. \n",
      "Epoch: 2276/5000..  Training Loss: 0.945..  Test Loss: 0.936.. \n",
      "Epoch: 2277/5000..  Training Loss: 0.928..  Test Loss: 0.936.. \n",
      "Epoch: 2278/5000..  Training Loss: 0.967..  Test Loss: 0.936.. \n",
      "Epoch: 2279/5000..  Training Loss: 0.931..  Test Loss: 0.936.. \n",
      "Epoch: 2280/5000..  Training Loss: 0.938..  Test Loss: 0.936.. \n",
      "Epoch: 2281/5000..  Training Loss: 0.961..  Test Loss: 0.936.. \n",
      "Epoch: 2282/5000..  Training Loss: 0.965..  Test Loss: 0.936.. \n",
      "Epoch: 2283/5000..  Training Loss: 0.936..  Test Loss: 0.936.. \n",
      "Epoch: 2284/5000..  Training Loss: 0.930..  Test Loss: 0.936.. \n",
      "Epoch: 2285/5000..  Training Loss: 0.945..  Test Loss: 0.936.. \n",
      "Epoch: 2286/5000..  Training Loss: 0.949..  Test Loss: 0.936.. \n",
      "Epoch: 2287/5000..  Training Loss: 0.911..  Test Loss: 0.936.. \n",
      "Epoch: 2288/5000..  Training Loss: 0.955..  Test Loss: 0.936.. \n",
      "Epoch: 2289/5000..  Training Loss: 0.952..  Test Loss: 0.936.. \n",
      "Epoch: 2290/5000..  Training Loss: 0.916..  Test Loss: 0.936.. \n",
      "Epoch: 2291/5000..  Training Loss: 0.900..  Test Loss: 0.936.. \n",
      "Epoch: 2292/5000..  Training Loss: 0.942..  Test Loss: 0.936.. \n",
      "Epoch: 2293/5000..  Training Loss: 0.941..  Test Loss: 0.936.. \n",
      "Epoch: 2294/5000..  Training Loss: 0.904..  Test Loss: 0.936.. \n",
      "Epoch: 2295/5000..  Training Loss: 0.958..  Test Loss: 0.936.. \n",
      "Epoch: 2296/5000..  Training Loss: 0.914..  Test Loss: 0.936.. \n",
      "Epoch: 2297/5000..  Training Loss: 0.974..  Test Loss: 0.936.. \n",
      "Epoch: 2298/5000..  Training Loss: 0.938..  Test Loss: 0.936.. \n",
      "Epoch: 2299/5000..  Training Loss: 0.954..  Test Loss: 0.936.. \n",
      "Epoch: 2300/5000..  Training Loss: 0.973..  Test Loss: 0.936.. \n",
      "Epoch: 2301/5000..  Training Loss: 0.960..  Test Loss: 0.936.. \n",
      "Epoch: 2302/5000..  Training Loss: 0.930..  Test Loss: 0.936.. \n",
      "Epoch: 2303/5000..  Training Loss: 0.928..  Test Loss: 0.936.. \n",
      "Epoch: 2304/5000..  Training Loss: 0.955..  Test Loss: 0.936.. \n",
      "Epoch: 2305/5000..  Training Loss: 0.904..  Test Loss: 0.936.. \n",
      "Epoch: 2306/5000..  Training Loss: 0.941..  Test Loss: 0.936.. \n",
      "Epoch: 2307/5000..  Training Loss: 0.949..  Test Loss: 0.936.. \n",
      "Epoch: 2308/5000..  Training Loss: 0.973..  Test Loss: 0.936.. \n",
      "Epoch: 2309/5000..  Training Loss: 0.933..  Test Loss: 0.936.. \n",
      "Epoch: 2310/5000..  Training Loss: 0.964..  Test Loss: 0.936.. \n",
      "Epoch: 2311/5000..  Training Loss: 0.946..  Test Loss: 0.936.. \n",
      "Epoch: 2312/5000..  Training Loss: 0.925..  Test Loss: 0.936.. \n",
      "Epoch: 2313/5000..  Training Loss: 0.943..  Test Loss: 0.936.. \n",
      "Epoch: 2314/5000..  Training Loss: 0.953..  Test Loss: 0.936.. \n",
      "Epoch: 2315/5000..  Training Loss: 0.910..  Test Loss: 0.936.. \n",
      "Epoch: 2316/5000..  Training Loss: 0.920..  Test Loss: 0.936.. \n",
      "Epoch: 2317/5000..  Training Loss: 0.927..  Test Loss: 0.936.. \n",
      "Epoch: 2318/5000..  Training Loss: 0.935..  Test Loss: 0.936.. \n",
      "Epoch: 2319/5000..  Training Loss: 0.924..  Test Loss: 0.936.. \n",
      "Epoch: 2320/5000..  Training Loss: 0.926..  Test Loss: 0.936.. \n",
      "Epoch: 2321/5000..  Training Loss: 0.951..  Test Loss: 0.936.. \n",
      "Epoch: 2322/5000..  Training Loss: 0.937..  Test Loss: 0.936.. \n",
      "Epoch: 2323/5000..  Training Loss: 0.938..  Test Loss: 0.936.. \n",
      "Epoch: 2324/5000..  Training Loss: 0.885..  Test Loss: 0.936.. \n",
      "Epoch: 2325/5000..  Training Loss: 0.910..  Test Loss: 0.936.. \n",
      "Epoch: 2326/5000..  Training Loss: 0.931..  Test Loss: 0.936.. \n",
      "Epoch: 2327/5000..  Training Loss: 0.965..  Test Loss: 0.936.. \n",
      "Epoch: 2328/5000..  Training Loss: 0.955..  Test Loss: 0.936.. \n",
      "Epoch: 2329/5000..  Training Loss: 0.949..  Test Loss: 0.936.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2330/5000..  Training Loss: 0.918..  Test Loss: 0.936.. \n",
      "Epoch: 2331/5000..  Training Loss: 0.947..  Test Loss: 0.936.. \n",
      "Epoch: 2332/5000..  Training Loss: 0.956..  Test Loss: 0.936.. \n",
      "Epoch: 2333/5000..  Training Loss: 0.915..  Test Loss: 0.936.. \n",
      "Epoch: 2334/5000..  Training Loss: 0.939..  Test Loss: 0.936.. \n",
      "Epoch: 2335/5000..  Training Loss: 0.932..  Test Loss: 0.936.. \n",
      "Epoch: 2336/5000..  Training Loss: 0.915..  Test Loss: 0.936.. \n",
      "Epoch: 2337/5000..  Training Loss: 0.930..  Test Loss: 0.936.. \n",
      "Epoch: 2338/5000..  Training Loss: 0.917..  Test Loss: 0.936.. \n",
      "Epoch: 2339/5000..  Training Loss: 0.919..  Test Loss: 0.936.. \n",
      "Epoch: 2340/5000..  Training Loss: 0.950..  Test Loss: 0.936.. \n",
      "Epoch: 2341/5000..  Training Loss: 0.937..  Test Loss: 0.936.. \n",
      "Epoch: 2342/5000..  Training Loss: 0.928..  Test Loss: 0.936.. \n",
      "Epoch: 2343/5000..  Training Loss: 0.970..  Test Loss: 0.936.. \n",
      "Epoch: 2344/5000..  Training Loss: 0.935..  Test Loss: 0.936.. \n",
      "Epoch: 2345/5000..  Training Loss: 0.941..  Test Loss: 0.936.. \n",
      "Epoch: 2346/5000..  Training Loss: 0.924..  Test Loss: 0.936.. \n",
      "Epoch: 2347/5000..  Training Loss: 0.945..  Test Loss: 0.936.. \n",
      "Epoch: 2348/5000..  Training Loss: 0.933..  Test Loss: 0.936.. \n",
      "Epoch: 2349/5000..  Training Loss: 0.937..  Test Loss: 0.936.. \n",
      "Epoch: 2350/5000..  Training Loss: 0.904..  Test Loss: 0.936.. \n",
      "Epoch: 2351/5000..  Training Loss: 0.940..  Test Loss: 0.936.. \n",
      "Epoch: 2352/5000..  Training Loss: 0.926..  Test Loss: 0.936.. \n",
      "Epoch: 2353/5000..  Training Loss: 0.916..  Test Loss: 0.936.. \n",
      "Epoch: 2354/5000..  Training Loss: 0.909..  Test Loss: 0.936.. \n",
      "Epoch: 2355/5000..  Training Loss: 0.936..  Test Loss: 0.936.. \n",
      "Epoch: 2356/5000..  Training Loss: 0.942..  Test Loss: 0.936.. \n",
      "Epoch: 2357/5000..  Training Loss: 0.933..  Test Loss: 0.936.. \n",
      "Epoch: 2358/5000..  Training Loss: 0.970..  Test Loss: 0.936.. \n",
      "Epoch: 2359/5000..  Training Loss: 0.939..  Test Loss: 0.936.. \n",
      "Epoch: 2360/5000..  Training Loss: 0.908..  Test Loss: 0.936.. \n",
      "Epoch: 2361/5000..  Training Loss: 0.917..  Test Loss: 0.936.. \n",
      "Epoch: 2362/5000..  Training Loss: 0.940..  Test Loss: 0.936.. \n",
      "Epoch: 2363/5000..  Training Loss: 0.949..  Test Loss: 0.936.. \n",
      "Epoch: 2364/5000..  Training Loss: 0.906..  Test Loss: 0.936.. \n",
      "Epoch: 2365/5000..  Training Loss: 0.901..  Test Loss: 0.936.. \n",
      "Epoch: 2366/5000..  Training Loss: 0.957..  Test Loss: 0.936.. \n",
      "Epoch: 2367/5000..  Training Loss: 0.933..  Test Loss: 0.936.. \n",
      "Epoch: 2368/5000..  Training Loss: 0.938..  Test Loss: 0.936.. \n",
      "Epoch: 2369/5000..  Training Loss: 0.968..  Test Loss: 0.936.. \n",
      "Epoch: 2370/5000..  Training Loss: 0.903..  Test Loss: 0.936.. \n",
      "Epoch: 2371/5000..  Training Loss: 0.932..  Test Loss: 0.936.. \n",
      "Epoch: 2372/5000..  Training Loss: 0.939..  Test Loss: 0.936.. \n",
      "Epoch: 2373/5000..  Training Loss: 0.906..  Test Loss: 0.936.. \n",
      "Epoch: 2374/5000..  Training Loss: 0.962..  Test Loss: 0.936.. \n",
      "Epoch: 2375/5000..  Training Loss: 0.883..  Test Loss: 0.936.. \n",
      "Epoch: 2376/5000..  Training Loss: 0.949..  Test Loss: 0.936.. \n",
      "Epoch: 2377/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2378/5000..  Training Loss: 0.920..  Test Loss: 0.935.. \n",
      "Epoch: 2379/5000..  Training Loss: 0.931..  Test Loss: 0.935.. \n",
      "Epoch: 2380/5000..  Training Loss: 0.917..  Test Loss: 0.935.. \n",
      "Epoch: 2381/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2382/5000..  Training Loss: 0.922..  Test Loss: 0.935.. \n",
      "Epoch: 2383/5000..  Training Loss: 0.897..  Test Loss: 0.935.. \n",
      "Epoch: 2384/5000..  Training Loss: 0.924..  Test Loss: 0.935.. \n",
      "Epoch: 2385/5000..  Training Loss: 0.969..  Test Loss: 0.935.. \n",
      "Epoch: 2386/5000..  Training Loss: 0.958..  Test Loss: 0.935.. \n",
      "Epoch: 2387/5000..  Training Loss: 0.943..  Test Loss: 0.935.. \n",
      "Epoch: 2388/5000..  Training Loss: 0.949..  Test Loss: 0.935.. \n",
      "Epoch: 2389/5000..  Training Loss: 0.936..  Test Loss: 0.935.. \n",
      "Epoch: 2390/5000..  Training Loss: 0.928..  Test Loss: 0.935.. \n",
      "Epoch: 2391/5000..  Training Loss: 0.950..  Test Loss: 0.935.. \n",
      "Epoch: 2392/5000..  Training Loss: 0.887..  Test Loss: 0.935.. \n",
      "Epoch: 2393/5000..  Training Loss: 0.928..  Test Loss: 0.935.. \n",
      "Epoch: 2394/5000..  Training Loss: 0.961..  Test Loss: 0.935.. \n",
      "Epoch: 2395/5000..  Training Loss: 0.960..  Test Loss: 0.935.. \n",
      "Epoch: 2396/5000..  Training Loss: 0.911..  Test Loss: 0.935.. \n",
      "Epoch: 2397/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2398/5000..  Training Loss: 0.916..  Test Loss: 0.935.. \n",
      "Epoch: 2399/5000..  Training Loss: 0.941..  Test Loss: 0.935.. \n",
      "Epoch: 2400/5000..  Training Loss: 0.954..  Test Loss: 0.935.. \n",
      "Epoch: 2401/5000..  Training Loss: 0.943..  Test Loss: 0.935.. \n",
      "Epoch: 2402/5000..  Training Loss: 0.911..  Test Loss: 0.935.. \n",
      "Epoch: 2403/5000..  Training Loss: 0.969..  Test Loss: 0.935.. \n",
      "Epoch: 2404/5000..  Training Loss: 0.932..  Test Loss: 0.935.. \n",
      "Epoch: 2405/5000..  Training Loss: 0.939..  Test Loss: 0.935.. \n",
      "Epoch: 2406/5000..  Training Loss: 0.923..  Test Loss: 0.935.. \n",
      "Epoch: 2407/5000..  Training Loss: 0.946..  Test Loss: 0.935.. \n",
      "Epoch: 2408/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2409/5000..  Training Loss: 0.919..  Test Loss: 0.935.. \n",
      "Epoch: 2410/5000..  Training Loss: 0.946..  Test Loss: 0.935.. \n",
      "Epoch: 2411/5000..  Training Loss: 0.922..  Test Loss: 0.935.. \n",
      "Epoch: 2412/5000..  Training Loss: 0.928..  Test Loss: 0.935.. \n",
      "Epoch: 2413/5000..  Training Loss: 0.973..  Test Loss: 0.935.. \n",
      "Epoch: 2414/5000..  Training Loss: 0.920..  Test Loss: 0.935.. \n",
      "Epoch: 2415/5000..  Training Loss: 0.935..  Test Loss: 0.935.. \n",
      "Epoch: 2416/5000..  Training Loss: 0.967..  Test Loss: 0.935.. \n",
      "Epoch: 2417/5000..  Training Loss: 0.971..  Test Loss: 0.935.. \n",
      "Epoch: 2418/5000..  Training Loss: 0.961..  Test Loss: 0.935.. \n",
      "Epoch: 2419/5000..  Training Loss: 0.953..  Test Loss: 0.935.. \n",
      "Epoch: 2420/5000..  Training Loss: 0.916..  Test Loss: 0.935.. \n",
      "Epoch: 2421/5000..  Training Loss: 0.930..  Test Loss: 0.935.. \n",
      "Epoch: 2422/5000..  Training Loss: 0.909..  Test Loss: 0.935.. \n",
      "Epoch: 2423/5000..  Training Loss: 0.918..  Test Loss: 0.935.. \n",
      "Epoch: 2424/5000..  Training Loss: 0.933..  Test Loss: 0.935.. \n",
      "Epoch: 2425/5000..  Training Loss: 0.944..  Test Loss: 0.935.. \n",
      "Epoch: 2426/5000..  Training Loss: 0.945..  Test Loss: 0.935.. \n",
      "Epoch: 2427/5000..  Training Loss: 0.922..  Test Loss: 0.935.. \n",
      "Epoch: 2428/5000..  Training Loss: 0.902..  Test Loss: 0.935.. \n",
      "Epoch: 2429/5000..  Training Loss: 0.973..  Test Loss: 0.935.. \n",
      "Epoch: 2430/5000..  Training Loss: 0.939..  Test Loss: 0.935.. \n",
      "Epoch: 2431/5000..  Training Loss: 0.928..  Test Loss: 0.935.. \n",
      "Epoch: 2432/5000..  Training Loss: 0.907..  Test Loss: 0.935.. \n",
      "Epoch: 2433/5000..  Training Loss: 0.919..  Test Loss: 0.935.. \n",
      "Epoch: 2434/5000..  Training Loss: 0.915..  Test Loss: 0.935.. \n",
      "Epoch: 2435/5000..  Training Loss: 0.908..  Test Loss: 0.935.. \n",
      "Epoch: 2436/5000..  Training Loss: 0.925..  Test Loss: 0.935.. \n",
      "Epoch: 2437/5000..  Training Loss: 0.932..  Test Loss: 0.935.. \n",
      "Epoch: 2438/5000..  Training Loss: 0.938..  Test Loss: 0.935.. \n",
      "Epoch: 2439/5000..  Training Loss: 0.933..  Test Loss: 0.935.. \n",
      "Epoch: 2440/5000..  Training Loss: 0.926..  Test Loss: 0.935.. \n",
      "Epoch: 2441/5000..  Training Loss: 0.945..  Test Loss: 0.935.. \n",
      "Epoch: 2442/5000..  Training Loss: 0.946..  Test Loss: 0.935.. \n",
      "Epoch: 2443/5000..  Training Loss: 0.947..  Test Loss: 0.935.. \n",
      "Epoch: 2444/5000..  Training Loss: 0.924..  Test Loss: 0.935.. \n",
      "Epoch: 2445/5000..  Training Loss: 0.954..  Test Loss: 0.935.. \n",
      "Epoch: 2446/5000..  Training Loss: 0.948..  Test Loss: 0.935.. \n",
      "Epoch: 2447/5000..  Training Loss: 0.959..  Test Loss: 0.935.. \n",
      "Epoch: 2448/5000..  Training Loss: 0.907..  Test Loss: 0.935.. \n",
      "Epoch: 2449/5000..  Training Loss: 0.944..  Test Loss: 0.935.. \n",
      "Epoch: 2450/5000..  Training Loss: 0.955..  Test Loss: 0.935.. \n",
      "Epoch: 2451/5000..  Training Loss: 0.928..  Test Loss: 0.935.. \n",
      "Epoch: 2452/5000..  Training Loss: 0.959..  Test Loss: 0.935.. \n",
      "Epoch: 2453/5000..  Training Loss: 0.913..  Test Loss: 0.935.. \n",
      "Epoch: 2454/5000..  Training Loss: 0.930..  Test Loss: 0.935.. \n",
      "Epoch: 2455/5000..  Training Loss: 0.902..  Test Loss: 0.935.. \n",
      "Epoch: 2456/5000..  Training Loss: 0.934..  Test Loss: 0.935.. \n",
      "Epoch: 2457/5000..  Training Loss: 0.882..  Test Loss: 0.935.. \n",
      "Epoch: 2458/5000..  Training Loss: 0.925..  Test Loss: 0.935.. \n",
      "Epoch: 2459/5000..  Training Loss: 0.927..  Test Loss: 0.935.. \n",
      "Epoch: 2460/5000..  Training Loss: 0.933..  Test Loss: 0.935.. \n",
      "Epoch: 2461/5000..  Training Loss: 0.931..  Test Loss: 0.935.. \n",
      "Epoch: 2462/5000..  Training Loss: 0.952..  Test Loss: 0.935.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2463/5000..  Training Loss: 0.907..  Test Loss: 0.935.. \n",
      "Epoch: 2464/5000..  Training Loss: 0.914..  Test Loss: 0.935.. \n",
      "Epoch: 2465/5000..  Training Loss: 0.973..  Test Loss: 0.935.. \n",
      "Epoch: 2466/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2467/5000..  Training Loss: 0.963..  Test Loss: 0.935.. \n",
      "Epoch: 2468/5000..  Training Loss: 0.913..  Test Loss: 0.935.. \n",
      "Epoch: 2469/5000..  Training Loss: 0.940..  Test Loss: 0.935.. \n",
      "Epoch: 2470/5000..  Training Loss: 0.913..  Test Loss: 0.935.. \n",
      "Epoch: 2471/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2472/5000..  Training Loss: 0.942..  Test Loss: 0.935.. \n",
      "Epoch: 2473/5000..  Training Loss: 0.934..  Test Loss: 0.935.. \n",
      "Epoch: 2474/5000..  Training Loss: 0.961..  Test Loss: 0.935.. \n",
      "Epoch: 2475/5000..  Training Loss: 0.976..  Test Loss: 0.935.. \n",
      "Epoch: 2476/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2477/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2478/5000..  Training Loss: 0.941..  Test Loss: 0.935.. \n",
      "Epoch: 2479/5000..  Training Loss: 0.932..  Test Loss: 0.935.. \n",
      "Epoch: 2480/5000..  Training Loss: 0.950..  Test Loss: 0.935.. \n",
      "Epoch: 2481/5000..  Training Loss: 0.902..  Test Loss: 0.935.. \n",
      "Epoch: 2482/5000..  Training Loss: 0.944..  Test Loss: 0.935.. \n",
      "Epoch: 2483/5000..  Training Loss: 0.919..  Test Loss: 0.935.. \n",
      "Epoch: 2484/5000..  Training Loss: 0.933..  Test Loss: 0.935.. \n",
      "Epoch: 2485/5000..  Training Loss: 0.917..  Test Loss: 0.935.. \n",
      "Epoch: 2486/5000..  Training Loss: 0.935..  Test Loss: 0.935.. \n",
      "Epoch: 2487/5000..  Training Loss: 0.953..  Test Loss: 0.935.. \n",
      "Epoch: 2488/5000..  Training Loss: 0.929..  Test Loss: 0.935.. \n",
      "Epoch: 2489/5000..  Training Loss: 0.900..  Test Loss: 0.935.. \n",
      "Epoch: 2490/5000..  Training Loss: 0.940..  Test Loss: 0.935.. \n",
      "Epoch: 2491/5000..  Training Loss: 0.932..  Test Loss: 0.935.. \n",
      "Epoch: 2492/5000..  Training Loss: 0.907..  Test Loss: 0.935.. \n",
      "Epoch: 2493/5000..  Training Loss: 0.938..  Test Loss: 0.935.. \n",
      "Epoch: 2494/5000..  Training Loss: 0.915..  Test Loss: 0.935.. \n",
      "Epoch: 2495/5000..  Training Loss: 0.932..  Test Loss: 0.935.. \n",
      "Epoch: 2496/5000..  Training Loss: 0.937..  Test Loss: 0.935.. \n",
      "Epoch: 2497/5000..  Training Loss: 0.968..  Test Loss: 0.935.. \n",
      "Epoch: 2498/5000..  Training Loss: 0.931..  Test Loss: 0.935.. \n",
      "Epoch: 2499/5000..  Training Loss: 0.973..  Test Loss: 0.935.. \n",
      "Epoch: 2500/5000..  Training Loss: 0.930..  Test Loss: 0.935.. \n",
      "Epoch: 2501/5000..  Training Loss: 0.942..  Test Loss: 0.935.. \n",
      "Epoch: 2502/5000..  Training Loss: 0.959..  Test Loss: 0.935.. \n",
      "Epoch: 2503/5000..  Training Loss: 0.969..  Test Loss: 0.934.. \n",
      "Epoch: 2504/5000..  Training Loss: 0.936..  Test Loss: 0.934.. \n",
      "Epoch: 2505/5000..  Training Loss: 0.911..  Test Loss: 0.934.. \n",
      "Epoch: 2506/5000..  Training Loss: 0.934..  Test Loss: 0.934.. \n",
      "Epoch: 2507/5000..  Training Loss: 0.957..  Test Loss: 0.934.. \n",
      "Epoch: 2508/5000..  Training Loss: 0.930..  Test Loss: 0.934.. \n",
      "Epoch: 2509/5000..  Training Loss: 0.968..  Test Loss: 0.934.. \n",
      "Epoch: 2510/5000..  Training Loss: 0.908..  Test Loss: 0.934.. \n",
      "Epoch: 2511/5000..  Training Loss: 0.960..  Test Loss: 0.934.. \n",
      "Epoch: 2512/5000..  Training Loss: 0.928..  Test Loss: 0.934.. \n",
      "Epoch: 2513/5000..  Training Loss: 0.942..  Test Loss: 0.934.. \n",
      "Epoch: 2514/5000..  Training Loss: 0.936..  Test Loss: 0.934.. \n",
      "Epoch: 2515/5000..  Training Loss: 0.910..  Test Loss: 0.934.. \n",
      "Epoch: 2516/5000..  Training Loss: 0.968..  Test Loss: 0.934.. \n",
      "Epoch: 2517/5000..  Training Loss: 0.930..  Test Loss: 0.934.. \n",
      "Epoch: 2518/5000..  Training Loss: 0.926..  Test Loss: 0.934.. \n",
      "Epoch: 2519/5000..  Training Loss: 0.947..  Test Loss: 0.934.. \n",
      "Epoch: 2520/5000..  Training Loss: 0.950..  Test Loss: 0.934.. \n",
      "Epoch: 2521/5000..  Training Loss: 0.957..  Test Loss: 0.934.. \n",
      "Epoch: 2522/5000..  Training Loss: 0.956..  Test Loss: 0.934.. \n",
      "Epoch: 2523/5000..  Training Loss: 0.927..  Test Loss: 0.934.. \n",
      "Epoch: 2524/5000..  Training Loss: 0.948..  Test Loss: 0.934.. \n",
      "Epoch: 2525/5000..  Training Loss: 0.899..  Test Loss: 0.934.. \n",
      "Epoch: 2526/5000..  Training Loss: 0.953..  Test Loss: 0.934.. \n",
      "Epoch: 2527/5000..  Training Loss: 0.949..  Test Loss: 0.934.. \n",
      "Epoch: 2528/5000..  Training Loss: 0.912..  Test Loss: 0.934.. \n",
      "Epoch: 2529/5000..  Training Loss: 0.919..  Test Loss: 0.934.. \n",
      "Epoch: 2530/5000..  Training Loss: 0.937..  Test Loss: 0.934.. \n",
      "Epoch: 2531/5000..  Training Loss: 0.945..  Test Loss: 0.934.. \n",
      "Epoch: 2532/5000..  Training Loss: 0.927..  Test Loss: 0.934.. \n",
      "Epoch: 2533/5000..  Training Loss: 0.951..  Test Loss: 0.934.. \n",
      "Epoch: 2534/5000..  Training Loss: 0.902..  Test Loss: 0.934.. \n",
      "Epoch: 2535/5000..  Training Loss: 0.918..  Test Loss: 0.934.. \n",
      "Epoch: 2536/5000..  Training Loss: 0.935..  Test Loss: 0.934.. \n",
      "Epoch: 2537/5000..  Training Loss: 0.935..  Test Loss: 0.934.. \n",
      "Epoch: 2538/5000..  Training Loss: 0.935..  Test Loss: 0.934.. \n",
      "Epoch: 2539/5000..  Training Loss: 0.923..  Test Loss: 0.934.. \n",
      "Epoch: 2540/5000..  Training Loss: 0.937..  Test Loss: 0.934.. \n",
      "Epoch: 2541/5000..  Training Loss: 0.961..  Test Loss: 0.934.. \n",
      "Epoch: 2542/5000..  Training Loss: 0.944..  Test Loss: 0.934.. \n",
      "Epoch: 2543/5000..  Training Loss: 0.904..  Test Loss: 0.934.. \n",
      "Epoch: 2544/5000..  Training Loss: 0.945..  Test Loss: 0.934.. \n",
      "Epoch: 2545/5000..  Training Loss: 0.948..  Test Loss: 0.934.. \n",
      "Epoch: 2546/5000..  Training Loss: 0.917..  Test Loss: 0.934.. \n",
      "Epoch: 2547/5000..  Training Loss: 0.915..  Test Loss: 0.934.. \n",
      "Epoch: 2548/5000..  Training Loss: 0.944..  Test Loss: 0.934.. \n",
      "Epoch: 2549/5000..  Training Loss: 0.970..  Test Loss: 0.934.. \n",
      "Epoch: 2550/5000..  Training Loss: 0.917..  Test Loss: 0.934.. \n",
      "Epoch: 2551/5000..  Training Loss: 0.956..  Test Loss: 0.934.. \n",
      "Epoch: 2552/5000..  Training Loss: 0.936..  Test Loss: 0.934.. \n",
      "Epoch: 2553/5000..  Training Loss: 0.951..  Test Loss: 0.934.. \n",
      "Epoch: 2554/5000..  Training Loss: 0.957..  Test Loss: 0.934.. \n",
      "Epoch: 2555/5000..  Training Loss: 0.918..  Test Loss: 0.934.. \n",
      "Epoch: 2556/5000..  Training Loss: 0.934..  Test Loss: 0.934.. \n",
      "Epoch: 2557/5000..  Training Loss: 0.968..  Test Loss: 0.934.. \n",
      "Epoch: 2558/5000..  Training Loss: 0.930..  Test Loss: 0.934.. \n",
      "Epoch: 2559/5000..  Training Loss: 0.954..  Test Loss: 0.934.. \n",
      "Epoch: 2560/5000..  Training Loss: 0.916..  Test Loss: 0.934.. \n",
      "Epoch: 2561/5000..  Training Loss: 0.938..  Test Loss: 0.934.. \n",
      "Epoch: 2562/5000..  Training Loss: 0.926..  Test Loss: 0.934.. \n",
      "Epoch: 2563/5000..  Training Loss: 0.926..  Test Loss: 0.934.. \n",
      "Epoch: 2564/5000..  Training Loss: 0.951..  Test Loss: 0.934.. \n",
      "Epoch: 2565/5000..  Training Loss: 0.966..  Test Loss: 0.934.. \n",
      "Epoch: 2566/5000..  Training Loss: 0.954..  Test Loss: 0.934.. \n",
      "Epoch: 2567/5000..  Training Loss: 0.961..  Test Loss: 0.934.. \n",
      "Epoch: 2568/5000..  Training Loss: 0.923..  Test Loss: 0.934.. \n",
      "Epoch: 2569/5000..  Training Loss: 0.934..  Test Loss: 0.934.. \n",
      "Epoch: 2570/5000..  Training Loss: 0.930..  Test Loss: 0.934.. \n",
      "Epoch: 2571/5000..  Training Loss: 0.962..  Test Loss: 0.934.. \n",
      "Epoch: 2572/5000..  Training Loss: 0.930..  Test Loss: 0.934.. \n",
      "Epoch: 2573/5000..  Training Loss: 0.943..  Test Loss: 0.934.. \n",
      "Epoch: 2574/5000..  Training Loss: 0.931..  Test Loss: 0.934.. \n",
      "Epoch: 2575/5000..  Training Loss: 0.945..  Test Loss: 0.934.. \n",
      "Epoch: 2576/5000..  Training Loss: 0.931..  Test Loss: 0.934.. \n",
      "Epoch: 2577/5000..  Training Loss: 0.937..  Test Loss: 0.934.. \n",
      "Epoch: 2578/5000..  Training Loss: 0.944..  Test Loss: 0.934.. \n",
      "Epoch: 2579/5000..  Training Loss: 0.907..  Test Loss: 0.934.. \n",
      "Epoch: 2580/5000..  Training Loss: 0.931..  Test Loss: 0.934.. \n",
      "Epoch: 2581/5000..  Training Loss: 0.900..  Test Loss: 0.934.. \n",
      "Epoch: 2582/5000..  Training Loss: 0.927..  Test Loss: 0.934.. \n",
      "Epoch: 2583/5000..  Training Loss: 0.894..  Test Loss: 0.934.. \n",
      "Epoch: 2584/5000..  Training Loss: 0.946..  Test Loss: 0.934.. \n",
      "Epoch: 2585/5000..  Training Loss: 0.902..  Test Loss: 0.934.. \n",
      "Epoch: 2586/5000..  Training Loss: 0.958..  Test Loss: 0.934.. \n",
      "Epoch: 2587/5000..  Training Loss: 0.914..  Test Loss: 0.934.. \n",
      "Epoch: 2588/5000..  Training Loss: 0.918..  Test Loss: 0.934.. \n",
      "Epoch: 2589/5000..  Training Loss: 0.929..  Test Loss: 0.934.. \n",
      "Epoch: 2590/5000..  Training Loss: 0.972..  Test Loss: 0.934.. \n",
      "Epoch: 2591/5000..  Training Loss: 0.943..  Test Loss: 0.934.. \n",
      "Epoch: 2592/5000..  Training Loss: 0.939..  Test Loss: 0.934.. \n",
      "Epoch: 2593/5000..  Training Loss: 0.915..  Test Loss: 0.934.. \n",
      "Epoch: 2594/5000..  Training Loss: 0.916..  Test Loss: 0.934.. \n",
      "Epoch: 2595/5000..  Training Loss: 0.920..  Test Loss: 0.934.. \n",
      "Epoch: 2596/5000..  Training Loss: 0.950..  Test Loss: 0.934.. \n",
      "Epoch: 2597/5000..  Training Loss: 0.927..  Test Loss: 0.934.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2598/5000..  Training Loss: 0.933..  Test Loss: 0.934.. \n",
      "Epoch: 2599/5000..  Training Loss: 0.911..  Test Loss: 0.934.. \n",
      "Epoch: 2600/5000..  Training Loss: 0.933..  Test Loss: 0.934.. \n",
      "Epoch: 2601/5000..  Training Loss: 0.928..  Test Loss: 0.934.. \n",
      "Epoch: 2602/5000..  Training Loss: 0.926..  Test Loss: 0.934.. \n",
      "Epoch: 2603/5000..  Training Loss: 0.922..  Test Loss: 0.934.. \n",
      "Epoch: 2604/5000..  Training Loss: 0.939..  Test Loss: 0.934.. \n",
      "Epoch: 2605/5000..  Training Loss: 0.940..  Test Loss: 0.934.. \n",
      "Epoch: 2606/5000..  Training Loss: 0.931..  Test Loss: 0.934.. \n",
      "Epoch: 2607/5000..  Training Loss: 0.918..  Test Loss: 0.934.. \n",
      "Epoch: 2608/5000..  Training Loss: 0.960..  Test Loss: 0.934.. \n",
      "Epoch: 2609/5000..  Training Loss: 0.931..  Test Loss: 0.934.. \n",
      "Epoch: 2610/5000..  Training Loss: 0.921..  Test Loss: 0.934.. \n",
      "Epoch: 2611/5000..  Training Loss: 0.957..  Test Loss: 0.934.. \n",
      "Epoch: 2612/5000..  Training Loss: 0.913..  Test Loss: 0.934.. \n",
      "Epoch: 2613/5000..  Training Loss: 0.921..  Test Loss: 0.934.. \n",
      "Epoch: 2614/5000..  Training Loss: 0.958..  Test Loss: 0.934.. \n",
      "Epoch: 2615/5000..  Training Loss: 0.950..  Test Loss: 0.934.. \n",
      "Epoch: 2616/5000..  Training Loss: 0.931..  Test Loss: 0.934.. \n",
      "Epoch: 2617/5000..  Training Loss: 0.927..  Test Loss: 0.934.. \n",
      "Epoch: 2618/5000..  Training Loss: 0.926..  Test Loss: 0.934.. \n",
      "Epoch: 2619/5000..  Training Loss: 0.912..  Test Loss: 0.934.. \n",
      "Epoch: 2620/5000..  Training Loss: 0.961..  Test Loss: 0.934.. \n",
      "Epoch: 2621/5000..  Training Loss: 0.940..  Test Loss: 0.934.. \n",
      "Epoch: 2622/5000..  Training Loss: 0.923..  Test Loss: 0.934.. \n",
      "Epoch: 2623/5000..  Training Loss: 0.952..  Test Loss: 0.934.. \n",
      "Epoch: 2624/5000..  Training Loss: 0.959..  Test Loss: 0.934.. \n",
      "Epoch: 2625/5000..  Training Loss: 0.976..  Test Loss: 0.934.. \n",
      "Epoch: 2626/5000..  Training Loss: 0.954..  Test Loss: 0.934.. \n",
      "Epoch: 2627/5000..  Training Loss: 0.920..  Test Loss: 0.934.. \n",
      "Epoch: 2628/5000..  Training Loss: 0.946..  Test Loss: 0.934.. \n",
      "Epoch: 2629/5000..  Training Loss: 0.954..  Test Loss: 0.934.. \n",
      "Epoch: 2630/5000..  Training Loss: 0.925..  Test Loss: 0.934.. \n",
      "Epoch: 2631/5000..  Training Loss: 0.931..  Test Loss: 0.934.. \n",
      "Epoch: 2632/5000..  Training Loss: 0.914..  Test Loss: 0.934.. \n",
      "Epoch: 2633/5000..  Training Loss: 0.907..  Test Loss: 0.934.. \n",
      "Epoch: 2634/5000..  Training Loss: 0.915..  Test Loss: 0.934.. \n",
      "Epoch: 2635/5000..  Training Loss: 0.943..  Test Loss: 0.934.. \n",
      "Epoch: 2636/5000..  Training Loss: 0.939..  Test Loss: 0.934.. \n",
      "Epoch: 2637/5000..  Training Loss: 0.922..  Test Loss: 0.934.. \n",
      "Epoch: 2638/5000..  Training Loss: 0.922..  Test Loss: 0.934.. \n",
      "Epoch: 2639/5000..  Training Loss: 0.921..  Test Loss: 0.934.. \n",
      "Epoch: 2640/5000..  Training Loss: 0.951..  Test Loss: 0.933.. \n",
      "Epoch: 2641/5000..  Training Loss: 0.909..  Test Loss: 0.933.. \n",
      "Epoch: 2642/5000..  Training Loss: 0.932..  Test Loss: 0.933.. \n",
      "Epoch: 2643/5000..  Training Loss: 0.954..  Test Loss: 0.933.. \n",
      "Epoch: 2644/5000..  Training Loss: 0.926..  Test Loss: 0.933.. \n",
      "Epoch: 2645/5000..  Training Loss: 0.981..  Test Loss: 0.933.. \n",
      "Epoch: 2646/5000..  Training Loss: 0.938..  Test Loss: 0.933.. \n",
      "Epoch: 2647/5000..  Training Loss: 0.961..  Test Loss: 0.933.. \n",
      "Epoch: 2648/5000..  Training Loss: 0.917..  Test Loss: 0.933.. \n",
      "Epoch: 2649/5000..  Training Loss: 0.918..  Test Loss: 0.933.. \n",
      "Epoch: 2650/5000..  Training Loss: 0.932..  Test Loss: 0.933.. \n",
      "Epoch: 2651/5000..  Training Loss: 0.928..  Test Loss: 0.933.. \n",
      "Epoch: 2652/5000..  Training Loss: 0.955..  Test Loss: 0.933.. \n",
      "Epoch: 2653/5000..  Training Loss: 0.926..  Test Loss: 0.933.. \n",
      "Epoch: 2654/5000..  Training Loss: 0.941..  Test Loss: 0.933.. \n",
      "Epoch: 2655/5000..  Training Loss: 0.908..  Test Loss: 0.933.. \n",
      "Epoch: 2656/5000..  Training Loss: 0.960..  Test Loss: 0.933.. \n",
      "Epoch: 2657/5000..  Training Loss: 0.932..  Test Loss: 0.933.. \n",
      "Epoch: 2658/5000..  Training Loss: 0.947..  Test Loss: 0.933.. \n",
      "Epoch: 2659/5000..  Training Loss: 0.906..  Test Loss: 0.933.. \n",
      "Epoch: 2660/5000..  Training Loss: 0.918..  Test Loss: 0.933.. \n",
      "Epoch: 2661/5000..  Training Loss: 0.963..  Test Loss: 0.933.. \n",
      "Epoch: 2662/5000..  Training Loss: 0.948..  Test Loss: 0.933.. \n",
      "Epoch: 2663/5000..  Training Loss: 0.961..  Test Loss: 0.933.. \n",
      "Epoch: 2664/5000..  Training Loss: 0.954..  Test Loss: 0.933.. \n",
      "Epoch: 2665/5000..  Training Loss: 0.896..  Test Loss: 0.933.. \n",
      "Epoch: 2666/5000..  Training Loss: 0.908..  Test Loss: 0.933.. \n",
      "Epoch: 2667/5000..  Training Loss: 0.928..  Test Loss: 0.933.. \n",
      "Epoch: 2668/5000..  Training Loss: 0.941..  Test Loss: 0.933.. \n",
      "Epoch: 2669/5000..  Training Loss: 0.940..  Test Loss: 0.933.. \n",
      "Epoch: 2670/5000..  Training Loss: 0.901..  Test Loss: 0.933.. \n",
      "Epoch: 2671/5000..  Training Loss: 0.952..  Test Loss: 0.933.. \n",
      "Epoch: 2672/5000..  Training Loss: 0.906..  Test Loss: 0.933.. \n",
      "Epoch: 2673/5000..  Training Loss: 0.915..  Test Loss: 0.933.. \n",
      "Epoch: 2674/5000..  Training Loss: 0.920..  Test Loss: 0.933.. \n",
      "Epoch: 2675/5000..  Training Loss: 0.930..  Test Loss: 0.933.. \n",
      "Epoch: 2676/5000..  Training Loss: 0.960..  Test Loss: 0.933.. \n",
      "Epoch: 2677/5000..  Training Loss: 0.929..  Test Loss: 0.933.. \n",
      "Epoch: 2678/5000..  Training Loss: 0.947..  Test Loss: 0.933.. \n",
      "Epoch: 2679/5000..  Training Loss: 0.910..  Test Loss: 0.933.. \n",
      "Epoch: 2680/5000..  Training Loss: 0.923..  Test Loss: 0.933.. \n",
      "Epoch: 2681/5000..  Training Loss: 0.884..  Test Loss: 0.933.. \n",
      "Epoch: 2682/5000..  Training Loss: 0.946..  Test Loss: 0.933.. \n",
      "Epoch: 2683/5000..  Training Loss: 0.913..  Test Loss: 0.933.. \n",
      "Epoch: 2684/5000..  Training Loss: 0.934..  Test Loss: 0.933.. \n",
      "Epoch: 2685/5000..  Training Loss: 0.947..  Test Loss: 0.933.. \n",
      "Epoch: 2686/5000..  Training Loss: 0.932..  Test Loss: 0.933.. \n",
      "Epoch: 2687/5000..  Training Loss: 0.975..  Test Loss: 0.933.. \n",
      "Epoch: 2688/5000..  Training Loss: 0.948..  Test Loss: 0.933.. \n",
      "Epoch: 2689/5000..  Training Loss: 0.940..  Test Loss: 0.933.. \n",
      "Epoch: 2690/5000..  Training Loss: 0.931..  Test Loss: 0.933.. \n",
      "Epoch: 2691/5000..  Training Loss: 0.931..  Test Loss: 0.933.. \n",
      "Epoch: 2692/5000..  Training Loss: 0.949..  Test Loss: 0.933.. \n",
      "Epoch: 2693/5000..  Training Loss: 0.936..  Test Loss: 0.933.. \n",
      "Epoch: 2694/5000..  Training Loss: 0.922..  Test Loss: 0.933.. \n",
      "Epoch: 2695/5000..  Training Loss: 0.954..  Test Loss: 0.933.. \n",
      "Epoch: 2696/5000..  Training Loss: 0.936..  Test Loss: 0.933.. \n",
      "Epoch: 2697/5000..  Training Loss: 0.926..  Test Loss: 0.933.. \n",
      "Epoch: 2698/5000..  Training Loss: 0.922..  Test Loss: 0.933.. \n",
      "Epoch: 2699/5000..  Training Loss: 0.927..  Test Loss: 0.933.. \n",
      "Epoch: 2700/5000..  Training Loss: 0.941..  Test Loss: 0.933.. \n",
      "Epoch: 2701/5000..  Training Loss: 0.955..  Test Loss: 0.933.. \n",
      "Epoch: 2702/5000..  Training Loss: 0.947..  Test Loss: 0.933.. \n",
      "Epoch: 2703/5000..  Training Loss: 0.902..  Test Loss: 0.933.. \n",
      "Epoch: 2704/5000..  Training Loss: 0.937..  Test Loss: 0.933.. \n",
      "Epoch: 2705/5000..  Training Loss: 0.912..  Test Loss: 0.933.. \n",
      "Epoch: 2706/5000..  Training Loss: 0.925..  Test Loss: 0.933.. \n",
      "Epoch: 2707/5000..  Training Loss: 0.962..  Test Loss: 0.933.. \n",
      "Epoch: 2708/5000..  Training Loss: 0.934..  Test Loss: 0.933.. \n",
      "Epoch: 2709/5000..  Training Loss: 0.920..  Test Loss: 0.933.. \n",
      "Epoch: 2710/5000..  Training Loss: 0.902..  Test Loss: 0.933.. \n",
      "Epoch: 2711/5000..  Training Loss: 0.958..  Test Loss: 0.933.. \n",
      "Epoch: 2712/5000..  Training Loss: 0.924..  Test Loss: 0.933.. \n",
      "Epoch: 2713/5000..  Training Loss: 0.877..  Test Loss: 0.933.. \n",
      "Epoch: 2714/5000..  Training Loss: 0.911..  Test Loss: 0.933.. \n",
      "Epoch: 2715/5000..  Training Loss: 0.923..  Test Loss: 0.933.. \n",
      "Epoch: 2716/5000..  Training Loss: 0.950..  Test Loss: 0.933.. \n",
      "Epoch: 2717/5000..  Training Loss: 0.923..  Test Loss: 0.933.. \n",
      "Epoch: 2718/5000..  Training Loss: 0.913..  Test Loss: 0.933.. \n",
      "Epoch: 2719/5000..  Training Loss: 0.934..  Test Loss: 0.933.. \n",
      "Epoch: 2720/5000..  Training Loss: 0.934..  Test Loss: 0.933.. \n",
      "Epoch: 2721/5000..  Training Loss: 0.938..  Test Loss: 0.933.. \n",
      "Epoch: 2722/5000..  Training Loss: 0.917..  Test Loss: 0.933.. \n",
      "Epoch: 2723/5000..  Training Loss: 0.905..  Test Loss: 0.933.. \n",
      "Epoch: 2724/5000..  Training Loss: 0.927..  Test Loss: 0.933.. \n",
      "Epoch: 2725/5000..  Training Loss: 0.933..  Test Loss: 0.933.. \n",
      "Epoch: 2726/5000..  Training Loss: 0.912..  Test Loss: 0.933.. \n",
      "Epoch: 2727/5000..  Training Loss: 0.915..  Test Loss: 0.933.. \n",
      "Epoch: 2728/5000..  Training Loss: 0.899..  Test Loss: 0.933.. \n",
      "Epoch: 2729/5000..  Training Loss: 0.944..  Test Loss: 0.933.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2730/5000..  Training Loss: 0.929..  Test Loss: 0.933.. \n",
      "Epoch: 2731/5000..  Training Loss: 0.932..  Test Loss: 0.933.. \n",
      "Epoch: 2732/5000..  Training Loss: 0.916..  Test Loss: 0.933.. \n",
      "Epoch: 2733/5000..  Training Loss: 0.928..  Test Loss: 0.933.. \n",
      "Epoch: 2734/5000..  Training Loss: 0.942..  Test Loss: 0.933.. \n",
      "Epoch: 2735/5000..  Training Loss: 0.949..  Test Loss: 0.933.. \n",
      "Epoch: 2736/5000..  Training Loss: 0.925..  Test Loss: 0.933.. \n",
      "Epoch: 2737/5000..  Training Loss: 0.939..  Test Loss: 0.933.. \n",
      "Epoch: 2738/5000..  Training Loss: 0.940..  Test Loss: 0.933.. \n",
      "Epoch: 2739/5000..  Training Loss: 0.927..  Test Loss: 0.933.. \n",
      "Epoch: 2740/5000..  Training Loss: 0.951..  Test Loss: 0.933.. \n",
      "Epoch: 2741/5000..  Training Loss: 0.932..  Test Loss: 0.933.. \n",
      "Epoch: 2742/5000..  Training Loss: 0.898..  Test Loss: 0.933.. \n",
      "Epoch: 2743/5000..  Training Loss: 0.939..  Test Loss: 0.933.. \n",
      "Epoch: 2744/5000..  Training Loss: 0.942..  Test Loss: 0.933.. \n",
      "Epoch: 2745/5000..  Training Loss: 0.944..  Test Loss: 0.933.. \n",
      "Epoch: 2746/5000..  Training Loss: 0.943..  Test Loss: 0.933.. \n",
      "Epoch: 2747/5000..  Training Loss: 0.896..  Test Loss: 0.933.. \n",
      "Epoch: 2748/5000..  Training Loss: 0.990..  Test Loss: 0.933.. \n",
      "Epoch: 2749/5000..  Training Loss: 0.937..  Test Loss: 0.933.. \n",
      "Epoch: 2750/5000..  Training Loss: 0.933..  Test Loss: 0.933.. \n",
      "Epoch: 2751/5000..  Training Loss: 0.952..  Test Loss: 0.933.. \n",
      "Epoch: 2752/5000..  Training Loss: 0.891..  Test Loss: 0.933.. \n",
      "Epoch: 2753/5000..  Training Loss: 0.923..  Test Loss: 0.933.. \n",
      "Epoch: 2754/5000..  Training Loss: 0.941..  Test Loss: 0.933.. \n",
      "Epoch: 2755/5000..  Training Loss: 0.911..  Test Loss: 0.933.. \n",
      "Epoch: 2756/5000..  Training Loss: 0.952..  Test Loss: 0.933.. \n",
      "Epoch: 2757/5000..  Training Loss: 0.923..  Test Loss: 0.933.. \n",
      "Epoch: 2758/5000..  Training Loss: 0.957..  Test Loss: 0.933.. \n",
      "Epoch: 2759/5000..  Training Loss: 0.904..  Test Loss: 0.933.. \n",
      "Epoch: 2760/5000..  Training Loss: 0.963..  Test Loss: 0.933.. \n",
      "Epoch: 2761/5000..  Training Loss: 0.937..  Test Loss: 0.933.. \n",
      "Epoch: 2762/5000..  Training Loss: 0.905..  Test Loss: 0.933.. \n",
      "Epoch: 2763/5000..  Training Loss: 0.918..  Test Loss: 0.933.. \n",
      "Epoch: 2764/5000..  Training Loss: 0.925..  Test Loss: 0.933.. \n",
      "Epoch: 2765/5000..  Training Loss: 0.938..  Test Loss: 0.933.. \n",
      "Epoch: 2766/5000..  Training Loss: 0.907..  Test Loss: 0.933.. \n",
      "Epoch: 2767/5000..  Training Loss: 0.931..  Test Loss: 0.933.. \n",
      "Epoch: 2768/5000..  Training Loss: 0.961..  Test Loss: 0.933.. \n",
      "Epoch: 2769/5000..  Training Loss: 0.928..  Test Loss: 0.933.. \n",
      "Epoch: 2770/5000..  Training Loss: 0.948..  Test Loss: 0.933.. \n",
      "Epoch: 2771/5000..  Training Loss: 0.944..  Test Loss: 0.933.. \n",
      "Epoch: 2772/5000..  Training Loss: 0.918..  Test Loss: 0.933.. \n",
      "Epoch: 2773/5000..  Training Loss: 0.939..  Test Loss: 0.932.. \n",
      "Epoch: 2774/5000..  Training Loss: 0.994..  Test Loss: 0.932.. \n",
      "Epoch: 2775/5000..  Training Loss: 0.935..  Test Loss: 0.932.. \n",
      "Epoch: 2776/5000..  Training Loss: 0.960..  Test Loss: 0.932.. \n",
      "Epoch: 2777/5000..  Training Loss: 0.910..  Test Loss: 0.932.. \n",
      "Epoch: 2778/5000..  Training Loss: 0.948..  Test Loss: 0.932.. \n",
      "Epoch: 2779/5000..  Training Loss: 0.941..  Test Loss: 0.932.. \n",
      "Epoch: 2780/5000..  Training Loss: 0.939..  Test Loss: 0.932.. \n",
      "Epoch: 2781/5000..  Training Loss: 0.941..  Test Loss: 0.932.. \n",
      "Epoch: 2782/5000..  Training Loss: 0.954..  Test Loss: 0.932.. \n",
      "Epoch: 2783/5000..  Training Loss: 0.940..  Test Loss: 0.932.. \n",
      "Epoch: 2784/5000..  Training Loss: 0.953..  Test Loss: 0.932.. \n",
      "Epoch: 2785/5000..  Training Loss: 0.926..  Test Loss: 0.932.. \n",
      "Epoch: 2786/5000..  Training Loss: 0.939..  Test Loss: 0.932.. \n",
      "Epoch: 2787/5000..  Training Loss: 0.923..  Test Loss: 0.932.. \n",
      "Epoch: 2788/5000..  Training Loss: 0.932..  Test Loss: 0.932.. \n",
      "Epoch: 2789/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2790/5000..  Training Loss: 0.913..  Test Loss: 0.932.. \n",
      "Epoch: 2791/5000..  Training Loss: 0.921..  Test Loss: 0.932.. \n",
      "Epoch: 2792/5000..  Training Loss: 0.944..  Test Loss: 0.932.. \n",
      "Epoch: 2793/5000..  Training Loss: 0.930..  Test Loss: 0.932.. \n",
      "Epoch: 2794/5000..  Training Loss: 0.918..  Test Loss: 0.932.. \n",
      "Epoch: 2795/5000..  Training Loss: 0.913..  Test Loss: 0.932.. \n",
      "Epoch: 2796/5000..  Training Loss: 0.926..  Test Loss: 0.932.. \n",
      "Epoch: 2797/5000..  Training Loss: 0.899..  Test Loss: 0.932.. \n",
      "Epoch: 2798/5000..  Training Loss: 0.910..  Test Loss: 0.932.. \n",
      "Epoch: 2799/5000..  Training Loss: 0.945..  Test Loss: 0.932.. \n",
      "Epoch: 2800/5000..  Training Loss: 0.926..  Test Loss: 0.932.. \n",
      "Epoch: 2801/5000..  Training Loss: 0.933..  Test Loss: 0.932.. \n",
      "Epoch: 2802/5000..  Training Loss: 0.925..  Test Loss: 0.932.. \n",
      "Epoch: 2803/5000..  Training Loss: 0.929..  Test Loss: 0.932.. \n",
      "Epoch: 2804/5000..  Training Loss: 0.910..  Test Loss: 0.932.. \n",
      "Epoch: 2805/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2806/5000..  Training Loss: 0.949..  Test Loss: 0.932.. \n",
      "Epoch: 2807/5000..  Training Loss: 0.950..  Test Loss: 0.932.. \n",
      "Epoch: 2808/5000..  Training Loss: 0.923..  Test Loss: 0.932.. \n",
      "Epoch: 2809/5000..  Training Loss: 0.942..  Test Loss: 0.932.. \n",
      "Epoch: 2810/5000..  Training Loss: 0.930..  Test Loss: 0.932.. \n",
      "Epoch: 2811/5000..  Training Loss: 0.925..  Test Loss: 0.932.. \n",
      "Epoch: 2812/5000..  Training Loss: 0.922..  Test Loss: 0.932.. \n",
      "Epoch: 2813/5000..  Training Loss: 0.936..  Test Loss: 0.932.. \n",
      "Epoch: 2814/5000..  Training Loss: 0.940..  Test Loss: 0.932.. \n",
      "Epoch: 2815/5000..  Training Loss: 0.935..  Test Loss: 0.932.. \n",
      "Epoch: 2816/5000..  Training Loss: 0.938..  Test Loss: 0.932.. \n",
      "Epoch: 2817/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2818/5000..  Training Loss: 0.918..  Test Loss: 0.932.. \n",
      "Epoch: 2819/5000..  Training Loss: 0.962..  Test Loss: 0.932.. \n",
      "Epoch: 2820/5000..  Training Loss: 0.905..  Test Loss: 0.932.. \n",
      "Epoch: 2821/5000..  Training Loss: 0.911..  Test Loss: 0.932.. \n",
      "Epoch: 2822/5000..  Training Loss: 0.912..  Test Loss: 0.932.. \n",
      "Epoch: 2823/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2824/5000..  Training Loss: 0.977..  Test Loss: 0.932.. \n",
      "Epoch: 2825/5000..  Training Loss: 0.926..  Test Loss: 0.932.. \n",
      "Epoch: 2826/5000..  Training Loss: 0.899..  Test Loss: 0.932.. \n",
      "Epoch: 2827/5000..  Training Loss: 0.942..  Test Loss: 0.932.. \n",
      "Epoch: 2828/5000..  Training Loss: 0.959..  Test Loss: 0.932.. \n",
      "Epoch: 2829/5000..  Training Loss: 0.927..  Test Loss: 0.932.. \n",
      "Epoch: 2830/5000..  Training Loss: 0.925..  Test Loss: 0.932.. \n",
      "Epoch: 2831/5000..  Training Loss: 0.915..  Test Loss: 0.932.. \n",
      "Epoch: 2832/5000..  Training Loss: 0.924..  Test Loss: 0.932.. \n",
      "Epoch: 2833/5000..  Training Loss: 0.949..  Test Loss: 0.932.. \n",
      "Epoch: 2834/5000..  Training Loss: 0.935..  Test Loss: 0.932.. \n",
      "Epoch: 2835/5000..  Training Loss: 0.925..  Test Loss: 0.932.. \n",
      "Epoch: 2836/5000..  Training Loss: 0.910..  Test Loss: 0.932.. \n",
      "Epoch: 2837/5000..  Training Loss: 0.911..  Test Loss: 0.932.. \n",
      "Epoch: 2838/5000..  Training Loss: 0.962..  Test Loss: 0.932.. \n",
      "Epoch: 2839/5000..  Training Loss: 0.915..  Test Loss: 0.932.. \n",
      "Epoch: 2840/5000..  Training Loss: 0.927..  Test Loss: 0.932.. \n",
      "Epoch: 2841/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2842/5000..  Training Loss: 0.915..  Test Loss: 0.932.. \n",
      "Epoch: 2843/5000..  Training Loss: 0.932..  Test Loss: 0.932.. \n",
      "Epoch: 2844/5000..  Training Loss: 0.910..  Test Loss: 0.932.. \n",
      "Epoch: 2845/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2846/5000..  Training Loss: 0.934..  Test Loss: 0.932.. \n",
      "Epoch: 2847/5000..  Training Loss: 0.911..  Test Loss: 0.932.. \n",
      "Epoch: 2848/5000..  Training Loss: 0.968..  Test Loss: 0.932.. \n",
      "Epoch: 2849/5000..  Training Loss: 0.896..  Test Loss: 0.932.. \n",
      "Epoch: 2850/5000..  Training Loss: 0.920..  Test Loss: 0.932.. \n",
      "Epoch: 2851/5000..  Training Loss: 0.910..  Test Loss: 0.932.. \n",
      "Epoch: 2852/5000..  Training Loss: 0.918..  Test Loss: 0.932.. \n",
      "Epoch: 2853/5000..  Training Loss: 0.937..  Test Loss: 0.932.. \n",
      "Epoch: 2854/5000..  Training Loss: 0.937..  Test Loss: 0.932.. \n",
      "Epoch: 2855/5000..  Training Loss: 0.932..  Test Loss: 0.932.. \n",
      "Epoch: 2856/5000..  Training Loss: 0.938..  Test Loss: 0.932.. \n",
      "Epoch: 2857/5000..  Training Loss: 0.942..  Test Loss: 0.932.. \n",
      "Epoch: 2858/5000..  Training Loss: 0.953..  Test Loss: 0.932.. \n",
      "Epoch: 2859/5000..  Training Loss: 0.940..  Test Loss: 0.932.. \n",
      "Epoch: 2860/5000..  Training Loss: 0.935..  Test Loss: 0.932.. \n",
      "Epoch: 2861/5000..  Training Loss: 0.877..  Test Loss: 0.932.. \n",
      "Epoch: 2862/5000..  Training Loss: 0.933..  Test Loss: 0.932.. \n",
      "Epoch: 2863/5000..  Training Loss: 0.930..  Test Loss: 0.932.. \n",
      "Epoch: 2864/5000..  Training Loss: 0.921..  Test Loss: 0.932.. \n",
      "Epoch: 2865/5000..  Training Loss: 0.921..  Test Loss: 0.932.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2866/5000..  Training Loss: 0.973..  Test Loss: 0.932.. \n",
      "Epoch: 2867/5000..  Training Loss: 0.919..  Test Loss: 0.932.. \n",
      "Epoch: 2868/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2869/5000..  Training Loss: 0.922..  Test Loss: 0.932.. \n",
      "Epoch: 2870/5000..  Training Loss: 0.911..  Test Loss: 0.932.. \n",
      "Epoch: 2871/5000..  Training Loss: 0.971..  Test Loss: 0.932.. \n",
      "Epoch: 2872/5000..  Training Loss: 0.930..  Test Loss: 0.932.. \n",
      "Epoch: 2873/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2874/5000..  Training Loss: 0.879..  Test Loss: 0.932.. \n",
      "Epoch: 2875/5000..  Training Loss: 0.943..  Test Loss: 0.932.. \n",
      "Epoch: 2876/5000..  Training Loss: 0.925..  Test Loss: 0.932.. \n",
      "Epoch: 2877/5000..  Training Loss: 0.947..  Test Loss: 0.932.. \n",
      "Epoch: 2878/5000..  Training Loss: 0.914..  Test Loss: 0.932.. \n",
      "Epoch: 2879/5000..  Training Loss: 0.957..  Test Loss: 0.932.. \n",
      "Epoch: 2880/5000..  Training Loss: 0.919..  Test Loss: 0.932.. \n",
      "Epoch: 2881/5000..  Training Loss: 0.893..  Test Loss: 0.932.. \n",
      "Epoch: 2882/5000..  Training Loss: 0.885..  Test Loss: 0.932.. \n",
      "Epoch: 2883/5000..  Training Loss: 0.934..  Test Loss: 0.932.. \n",
      "Epoch: 2884/5000..  Training Loss: 0.953..  Test Loss: 0.932.. \n",
      "Epoch: 2885/5000..  Training Loss: 0.924..  Test Loss: 0.932.. \n",
      "Epoch: 2886/5000..  Training Loss: 0.896..  Test Loss: 0.932.. \n",
      "Epoch: 2887/5000..  Training Loss: 0.951..  Test Loss: 0.932.. \n",
      "Epoch: 2888/5000..  Training Loss: 0.905..  Test Loss: 0.932.. \n",
      "Epoch: 2889/5000..  Training Loss: 0.949..  Test Loss: 0.932.. \n",
      "Epoch: 2890/5000..  Training Loss: 0.927..  Test Loss: 0.932.. \n",
      "Epoch: 2891/5000..  Training Loss: 0.897..  Test Loss: 0.932.. \n",
      "Epoch: 2892/5000..  Training Loss: 0.912..  Test Loss: 0.932.. \n",
      "Epoch: 2893/5000..  Training Loss: 0.957..  Test Loss: 0.932.. \n",
      "Epoch: 2894/5000..  Training Loss: 0.928..  Test Loss: 0.932.. \n",
      "Epoch: 2895/5000..  Training Loss: 0.932..  Test Loss: 0.932.. \n",
      "Epoch: 2896/5000..  Training Loss: 0.943..  Test Loss: 0.932.. \n",
      "Epoch: 2897/5000..  Training Loss: 0.913..  Test Loss: 0.932.. \n",
      "Epoch: 2898/5000..  Training Loss: 0.891..  Test Loss: 0.932.. \n",
      "Epoch: 2899/5000..  Training Loss: 0.953..  Test Loss: 0.932.. \n",
      "Epoch: 2900/5000..  Training Loss: 0.925..  Test Loss: 0.932.. \n",
      "Epoch: 2901/5000..  Training Loss: 0.949..  Test Loss: 0.932.. \n",
      "Epoch: 2902/5000..  Training Loss: 0.918..  Test Loss: 0.932.. \n",
      "Epoch: 2903/5000..  Training Loss: 0.941..  Test Loss: 0.932.. \n",
      "Epoch: 2904/5000..  Training Loss: 0.923..  Test Loss: 0.932.. \n",
      "Epoch: 2905/5000..  Training Loss: 0.919..  Test Loss: 0.932.. \n",
      "Epoch: 2906/5000..  Training Loss: 0.959..  Test Loss: 0.932.. \n",
      "Epoch: 2907/5000..  Training Loss: 0.906..  Test Loss: 0.932.. \n",
      "Epoch: 2908/5000..  Training Loss: 0.931..  Test Loss: 0.932.. \n",
      "Epoch: 2909/5000..  Training Loss: 0.951..  Test Loss: 0.932.. \n",
      "Epoch: 2910/5000..  Training Loss: 0.939..  Test Loss: 0.932.. \n",
      "Epoch: 2911/5000..  Training Loss: 0.929..  Test Loss: 0.932.. \n",
      "Epoch: 2912/5000..  Training Loss: 0.941..  Test Loss: 0.932.. \n",
      "Epoch: 2913/5000..  Training Loss: 0.954..  Test Loss: 0.932.. \n",
      "Epoch: 2914/5000..  Training Loss: 0.935..  Test Loss: 0.931.. \n",
      "Epoch: 2915/5000..  Training Loss: 0.947..  Test Loss: 0.931.. \n",
      "Epoch: 2916/5000..  Training Loss: 0.913..  Test Loss: 0.931.. \n",
      "Epoch: 2917/5000..  Training Loss: 0.967..  Test Loss: 0.931.. \n",
      "Epoch: 2918/5000..  Training Loss: 0.936..  Test Loss: 0.931.. \n",
      "Epoch: 2919/5000..  Training Loss: 0.927..  Test Loss: 0.931.. \n",
      "Epoch: 2920/5000..  Training Loss: 0.933..  Test Loss: 0.931.. \n",
      "Epoch: 2921/5000..  Training Loss: 0.948..  Test Loss: 0.931.. \n",
      "Epoch: 2922/5000..  Training Loss: 0.913..  Test Loss: 0.931.. \n",
      "Epoch: 2923/5000..  Training Loss: 0.909..  Test Loss: 0.931.. \n",
      "Epoch: 2924/5000..  Training Loss: 0.948..  Test Loss: 0.931.. \n",
      "Epoch: 2925/5000..  Training Loss: 0.941..  Test Loss: 0.931.. \n",
      "Epoch: 2926/5000..  Training Loss: 0.916..  Test Loss: 0.931.. \n",
      "Epoch: 2927/5000..  Training Loss: 0.911..  Test Loss: 0.931.. \n",
      "Epoch: 2928/5000..  Training Loss: 0.947..  Test Loss: 0.931.. \n",
      "Epoch: 2929/5000..  Training Loss: 0.902..  Test Loss: 0.931.. \n",
      "Epoch: 2930/5000..  Training Loss: 0.911..  Test Loss: 0.931.. \n",
      "Epoch: 2931/5000..  Training Loss: 0.929..  Test Loss: 0.931.. \n",
      "Epoch: 2932/5000..  Training Loss: 0.952..  Test Loss: 0.931.. \n",
      "Epoch: 2933/5000..  Training Loss: 0.927..  Test Loss: 0.931.. \n",
      "Epoch: 2934/5000..  Training Loss: 0.949..  Test Loss: 0.931.. \n",
      "Epoch: 2935/5000..  Training Loss: 0.948..  Test Loss: 0.931.. \n",
      "Epoch: 2936/5000..  Training Loss: 0.959..  Test Loss: 0.931.. \n",
      "Epoch: 2937/5000..  Training Loss: 0.915..  Test Loss: 0.931.. \n",
      "Epoch: 2938/5000..  Training Loss: 0.925..  Test Loss: 0.931.. \n",
      "Epoch: 2939/5000..  Training Loss: 0.920..  Test Loss: 0.931.. \n",
      "Epoch: 2940/5000..  Training Loss: 0.931..  Test Loss: 0.931.. \n",
      "Epoch: 2941/5000..  Training Loss: 0.929..  Test Loss: 0.931.. \n",
      "Epoch: 2942/5000..  Training Loss: 0.915..  Test Loss: 0.931.. \n",
      "Epoch: 2943/5000..  Training Loss: 0.951..  Test Loss: 0.931.. \n",
      "Epoch: 2944/5000..  Training Loss: 0.927..  Test Loss: 0.931.. \n",
      "Epoch: 2945/5000..  Training Loss: 0.924..  Test Loss: 0.931.. \n",
      "Epoch: 2946/5000..  Training Loss: 0.934..  Test Loss: 0.931.. \n",
      "Epoch: 2947/5000..  Training Loss: 0.920..  Test Loss: 0.931.. \n",
      "Epoch: 2948/5000..  Training Loss: 0.943..  Test Loss: 0.931.. \n",
      "Epoch: 2949/5000..  Training Loss: 0.951..  Test Loss: 0.931.. \n",
      "Epoch: 2950/5000..  Training Loss: 0.959..  Test Loss: 0.931.. \n",
      "Epoch: 2951/5000..  Training Loss: 0.924..  Test Loss: 0.931.. \n",
      "Epoch: 2952/5000..  Training Loss: 0.943..  Test Loss: 0.931.. \n",
      "Epoch: 2953/5000..  Training Loss: 0.926..  Test Loss: 0.931.. \n",
      "Epoch: 2954/5000..  Training Loss: 0.932..  Test Loss: 0.931.. \n",
      "Epoch: 2955/5000..  Training Loss: 0.953..  Test Loss: 0.931.. \n",
      "Epoch: 2956/5000..  Training Loss: 0.940..  Test Loss: 0.931.. \n",
      "Epoch: 2957/5000..  Training Loss: 0.933..  Test Loss: 0.931.. \n",
      "Epoch: 2958/5000..  Training Loss: 0.944..  Test Loss: 0.931.. \n",
      "Epoch: 2959/5000..  Training Loss: 0.931..  Test Loss: 0.931.. \n",
      "Epoch: 2960/5000..  Training Loss: 0.882..  Test Loss: 0.931.. \n",
      "Epoch: 2961/5000..  Training Loss: 0.917..  Test Loss: 0.931.. \n",
      "Epoch: 2962/5000..  Training Loss: 0.949..  Test Loss: 0.931.. \n",
      "Epoch: 2963/5000..  Training Loss: 0.936..  Test Loss: 0.931.. \n",
      "Epoch: 2964/5000..  Training Loss: 0.941..  Test Loss: 0.931.. \n",
      "Epoch: 2965/5000..  Training Loss: 0.917..  Test Loss: 0.931.. \n",
      "Epoch: 2966/5000..  Training Loss: 0.928..  Test Loss: 0.931.. \n",
      "Epoch: 2967/5000..  Training Loss: 0.901..  Test Loss: 0.931.. \n",
      "Epoch: 2968/5000..  Training Loss: 0.927..  Test Loss: 0.931.. \n",
      "Epoch: 2969/5000..  Training Loss: 0.926..  Test Loss: 0.931.. \n",
      "Epoch: 2970/5000..  Training Loss: 0.981..  Test Loss: 0.931.. \n",
      "Epoch: 2971/5000..  Training Loss: 0.915..  Test Loss: 0.931.. \n",
      "Epoch: 2972/5000..  Training Loss: 0.977..  Test Loss: 0.931.. \n",
      "Epoch: 2973/5000..  Training Loss: 0.928..  Test Loss: 0.931.. \n",
      "Epoch: 2974/5000..  Training Loss: 0.925..  Test Loss: 0.931.. \n",
      "Epoch: 2975/5000..  Training Loss: 0.933..  Test Loss: 0.931.. \n",
      "Epoch: 2976/5000..  Training Loss: 0.919..  Test Loss: 0.931.. \n",
      "Epoch: 2977/5000..  Training Loss: 0.935..  Test Loss: 0.931.. \n",
      "Epoch: 2978/5000..  Training Loss: 0.895..  Test Loss: 0.931.. \n",
      "Epoch: 2979/5000..  Training Loss: 0.952..  Test Loss: 0.931.. \n",
      "Epoch: 2980/5000..  Training Loss: 0.947..  Test Loss: 0.931.. \n",
      "Epoch: 2981/5000..  Training Loss: 0.926..  Test Loss: 0.931.. \n",
      "Epoch: 2982/5000..  Training Loss: 0.973..  Test Loss: 0.931.. \n",
      "Epoch: 2983/5000..  Training Loss: 0.938..  Test Loss: 0.931.. \n",
      "Epoch: 2984/5000..  Training Loss: 0.936..  Test Loss: 0.931.. \n",
      "Epoch: 2985/5000..  Training Loss: 0.920..  Test Loss: 0.931.. \n",
      "Epoch: 2986/5000..  Training Loss: 0.948..  Test Loss: 0.931.. \n",
      "Epoch: 2987/5000..  Training Loss: 0.918..  Test Loss: 0.931.. \n",
      "Epoch: 2988/5000..  Training Loss: 0.959..  Test Loss: 0.931.. \n",
      "Epoch: 2989/5000..  Training Loss: 0.956..  Test Loss: 0.931.. \n",
      "Epoch: 2990/5000..  Training Loss: 0.932..  Test Loss: 0.931.. \n",
      "Epoch: 2991/5000..  Training Loss: 0.901..  Test Loss: 0.931.. \n",
      "Epoch: 2992/5000..  Training Loss: 0.924..  Test Loss: 0.931.. \n",
      "Epoch: 2993/5000..  Training Loss: 0.946..  Test Loss: 0.931.. \n",
      "Epoch: 2994/5000..  Training Loss: 0.952..  Test Loss: 0.931.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2995/5000..  Training Loss: 0.946..  Test Loss: 0.931.. \n",
      "Epoch: 2996/5000..  Training Loss: 0.928..  Test Loss: 0.931.. \n",
      "Epoch: 2997/5000..  Training Loss: 0.948..  Test Loss: 0.931.. \n",
      "Epoch: 2998/5000..  Training Loss: 0.912..  Test Loss: 0.931.. \n",
      "Epoch: 2999/5000..  Training Loss: 0.905..  Test Loss: 0.931.. \n",
      "Epoch: 3000/5000..  Training Loss: 0.925..  Test Loss: 0.931.. \n",
      "Epoch: 3001/5000..  Training Loss: 0.955..  Test Loss: 0.931.. \n",
      "Epoch: 3002/5000..  Training Loss: 0.918..  Test Loss: 0.931.. \n",
      "Epoch: 3003/5000..  Training Loss: 0.905..  Test Loss: 0.931.. \n",
      "Epoch: 3004/5000..  Training Loss: 0.922..  Test Loss: 0.931.. \n",
      "Epoch: 3005/5000..  Training Loss: 0.874..  Test Loss: 0.931.. \n",
      "Epoch: 3006/5000..  Training Loss: 0.889..  Test Loss: 0.931.. \n",
      "Epoch: 3007/5000..  Training Loss: 0.941..  Test Loss: 0.931.. \n",
      "Epoch: 3008/5000..  Training Loss: 0.909..  Test Loss: 0.931.. \n",
      "Epoch: 3009/5000..  Training Loss: 0.912..  Test Loss: 0.931.. \n",
      "Epoch: 3010/5000..  Training Loss: 0.926..  Test Loss: 0.931.. \n",
      "Epoch: 3011/5000..  Training Loss: 0.923..  Test Loss: 0.931.. \n",
      "Epoch: 3012/5000..  Training Loss: 0.947..  Test Loss: 0.931.. \n",
      "Epoch: 3013/5000..  Training Loss: 0.902..  Test Loss: 0.931.. \n",
      "Epoch: 3014/5000..  Training Loss: 0.919..  Test Loss: 0.931.. \n",
      "Epoch: 3015/5000..  Training Loss: 0.925..  Test Loss: 0.931.. \n",
      "Epoch: 3016/5000..  Training Loss: 0.939..  Test Loss: 0.931.. \n",
      "Epoch: 3017/5000..  Training Loss: 0.936..  Test Loss: 0.931.. \n",
      "Epoch: 3018/5000..  Training Loss: 0.923..  Test Loss: 0.931.. \n",
      "Epoch: 3019/5000..  Training Loss: 0.941..  Test Loss: 0.931.. \n",
      "Epoch: 3020/5000..  Training Loss: 0.902..  Test Loss: 0.931.. \n",
      "Epoch: 3021/5000..  Training Loss: 0.915..  Test Loss: 0.931.. \n",
      "Epoch: 3022/5000..  Training Loss: 0.915..  Test Loss: 0.931.. \n",
      "Epoch: 3023/5000..  Training Loss: 0.934..  Test Loss: 0.931.. \n",
      "Epoch: 3024/5000..  Training Loss: 0.938..  Test Loss: 0.931.. \n",
      "Epoch: 3025/5000..  Training Loss: 0.919..  Test Loss: 0.931.. \n",
      "Epoch: 3026/5000..  Training Loss: 0.929..  Test Loss: 0.931.. \n",
      "Epoch: 3027/5000..  Training Loss: 0.916..  Test Loss: 0.931.. \n",
      "Epoch: 3028/5000..  Training Loss: 0.943..  Test Loss: 0.931.. \n",
      "Epoch: 3029/5000..  Training Loss: 0.949..  Test Loss: 0.931.. \n",
      "Epoch: 3030/5000..  Training Loss: 0.944..  Test Loss: 0.931.. \n",
      "Epoch: 3031/5000..  Training Loss: 0.961..  Test Loss: 0.931.. \n",
      "Epoch: 3032/5000..  Training Loss: 0.915..  Test Loss: 0.931.. \n",
      "Epoch: 3033/5000..  Training Loss: 0.918..  Test Loss: 0.931.. \n",
      "Epoch: 3034/5000..  Training Loss: 0.924..  Test Loss: 0.931.. \n",
      "Epoch: 3035/5000..  Training Loss: 0.922..  Test Loss: 0.931.. \n",
      "Epoch: 3036/5000..  Training Loss: 0.966..  Test Loss: 0.931.. \n",
      "Epoch: 3037/5000..  Training Loss: 0.960..  Test Loss: 0.931.. \n",
      "Epoch: 3038/5000..  Training Loss: 0.947..  Test Loss: 0.931.. \n",
      "Epoch: 3039/5000..  Training Loss: 0.935..  Test Loss: 0.931.. \n",
      "Epoch: 3040/5000..  Training Loss: 0.934..  Test Loss: 0.931.. \n",
      "Epoch: 3041/5000..  Training Loss: 0.924..  Test Loss: 0.931.. \n",
      "Epoch: 3042/5000..  Training Loss: 0.919..  Test Loss: 0.931.. \n",
      "Epoch: 3043/5000..  Training Loss: 0.941..  Test Loss: 0.931.. \n",
      "Epoch: 3044/5000..  Training Loss: 0.930..  Test Loss: 0.931.. \n",
      "Epoch: 3045/5000..  Training Loss: 0.960..  Test Loss: 0.931.. \n",
      "Epoch: 3046/5000..  Training Loss: 0.923..  Test Loss: 0.931.. \n",
      "Epoch: 3047/5000..  Training Loss: 0.953..  Test Loss: 0.931.. \n",
      "Epoch: 3048/5000..  Training Loss: 0.955..  Test Loss: 0.931.. \n",
      "Epoch: 3049/5000..  Training Loss: 0.922..  Test Loss: 0.931.. \n",
      "Epoch: 3050/5000..  Training Loss: 0.943..  Test Loss: 0.931.. \n",
      "Epoch: 3051/5000..  Training Loss: 0.935..  Test Loss: 0.931.. \n",
      "Epoch: 3052/5000..  Training Loss: 0.943..  Test Loss: 0.930.. \n",
      "Epoch: 3053/5000..  Training Loss: 0.906..  Test Loss: 0.930.. \n",
      "Epoch: 3054/5000..  Training Loss: 0.945..  Test Loss: 0.930.. \n",
      "Epoch: 3055/5000..  Training Loss: 0.926..  Test Loss: 0.930.. \n",
      "Epoch: 3056/5000..  Training Loss: 0.913..  Test Loss: 0.930.. \n",
      "Epoch: 3057/5000..  Training Loss: 0.919..  Test Loss: 0.930.. \n",
      "Epoch: 3058/5000..  Training Loss: 0.938..  Test Loss: 0.930.. \n",
      "Epoch: 3059/5000..  Training Loss: 0.916..  Test Loss: 0.930.. \n",
      "Epoch: 3060/5000..  Training Loss: 0.929..  Test Loss: 0.930.. \n",
      "Epoch: 3061/5000..  Training Loss: 0.900..  Test Loss: 0.930.. \n",
      "Epoch: 3062/5000..  Training Loss: 0.968..  Test Loss: 0.930.. \n",
      "Epoch: 3063/5000..  Training Loss: 0.900..  Test Loss: 0.930.. \n",
      "Epoch: 3064/5000..  Training Loss: 0.925..  Test Loss: 0.930.. \n",
      "Epoch: 3065/5000..  Training Loss: 0.934..  Test Loss: 0.930.. \n",
      "Epoch: 3066/5000..  Training Loss: 0.918..  Test Loss: 0.930.. \n",
      "Epoch: 3067/5000..  Training Loss: 0.926..  Test Loss: 0.930.. \n",
      "Epoch: 3068/5000..  Training Loss: 0.939..  Test Loss: 0.930.. \n",
      "Epoch: 3069/5000..  Training Loss: 0.906..  Test Loss: 0.930.. \n",
      "Epoch: 3070/5000..  Training Loss: 0.922..  Test Loss: 0.930.. \n",
      "Epoch: 3071/5000..  Training Loss: 0.920..  Test Loss: 0.930.. \n",
      "Epoch: 3072/5000..  Training Loss: 0.943..  Test Loss: 0.930.. \n",
      "Epoch: 3073/5000..  Training Loss: 0.903..  Test Loss: 0.930.. \n",
      "Epoch: 3074/5000..  Training Loss: 0.953..  Test Loss: 0.930.. \n",
      "Epoch: 3075/5000..  Training Loss: 0.907..  Test Loss: 0.930.. \n",
      "Epoch: 3076/5000..  Training Loss: 0.933..  Test Loss: 0.930.. \n",
      "Epoch: 3077/5000..  Training Loss: 0.975..  Test Loss: 0.930.. \n",
      "Epoch: 3078/5000..  Training Loss: 0.937..  Test Loss: 0.930.. \n",
      "Epoch: 3079/5000..  Training Loss: 0.937..  Test Loss: 0.930.. \n",
      "Epoch: 3080/5000..  Training Loss: 0.904..  Test Loss: 0.930.. \n",
      "Epoch: 3081/5000..  Training Loss: 0.919..  Test Loss: 0.930.. \n",
      "Epoch: 3082/5000..  Training Loss: 0.940..  Test Loss: 0.930.. \n",
      "Epoch: 3083/5000..  Training Loss: 0.951..  Test Loss: 0.930.. \n",
      "Epoch: 3084/5000..  Training Loss: 0.878..  Test Loss: 0.930.. \n",
      "Epoch: 3085/5000..  Training Loss: 0.899..  Test Loss: 0.930.. \n",
      "Epoch: 3086/5000..  Training Loss: 0.959..  Test Loss: 0.930.. \n",
      "Epoch: 3087/5000..  Training Loss: 0.972..  Test Loss: 0.930.. \n",
      "Epoch: 3088/5000..  Training Loss: 0.945..  Test Loss: 0.930.. \n",
      "Epoch: 3089/5000..  Training Loss: 0.948..  Test Loss: 0.930.. \n",
      "Epoch: 3090/5000..  Training Loss: 0.941..  Test Loss: 0.930.. \n",
      "Epoch: 3091/5000..  Training Loss: 0.930..  Test Loss: 0.930.. \n",
      "Epoch: 3092/5000..  Training Loss: 0.950..  Test Loss: 0.930.. \n",
      "Epoch: 3093/5000..  Training Loss: 0.946..  Test Loss: 0.930.. \n",
      "Epoch: 3094/5000..  Training Loss: 0.972..  Test Loss: 0.930.. \n",
      "Epoch: 3095/5000..  Training Loss: 0.915..  Test Loss: 0.930.. \n",
      "Epoch: 3096/5000..  Training Loss: 0.927..  Test Loss: 0.930.. \n",
      "Epoch: 3097/5000..  Training Loss: 0.967..  Test Loss: 0.930.. \n",
      "Epoch: 3098/5000..  Training Loss: 0.933..  Test Loss: 0.930.. \n",
      "Epoch: 3099/5000..  Training Loss: 0.949..  Test Loss: 0.930.. \n",
      "Epoch: 3100/5000..  Training Loss: 0.894..  Test Loss: 0.930.. \n",
      "Epoch: 3101/5000..  Training Loss: 0.936..  Test Loss: 0.930.. \n",
      "Epoch: 3102/5000..  Training Loss: 0.933..  Test Loss: 0.930.. \n",
      "Epoch: 3103/5000..  Training Loss: 0.920..  Test Loss: 0.930.. \n",
      "Epoch: 3104/5000..  Training Loss: 0.954..  Test Loss: 0.930.. \n",
      "Epoch: 3105/5000..  Training Loss: 0.929..  Test Loss: 0.930.. \n",
      "Epoch: 3106/5000..  Training Loss: 0.939..  Test Loss: 0.930.. \n",
      "Epoch: 3107/5000..  Training Loss: 0.936..  Test Loss: 0.930.. \n",
      "Epoch: 3108/5000..  Training Loss: 0.899..  Test Loss: 0.930.. \n",
      "Epoch: 3109/5000..  Training Loss: 0.902..  Test Loss: 0.930.. \n",
      "Epoch: 3110/5000..  Training Loss: 0.939..  Test Loss: 0.930.. \n",
      "Epoch: 3111/5000..  Training Loss: 0.924..  Test Loss: 0.930.. \n",
      "Epoch: 3112/5000..  Training Loss: 0.897..  Test Loss: 0.930.. \n",
      "Epoch: 3113/5000..  Training Loss: 0.899..  Test Loss: 0.930.. \n",
      "Epoch: 3114/5000..  Training Loss: 0.920..  Test Loss: 0.930.. \n",
      "Epoch: 3115/5000..  Training Loss: 0.937..  Test Loss: 0.930.. \n",
      "Epoch: 3116/5000..  Training Loss: 0.909..  Test Loss: 0.930.. \n",
      "Epoch: 3117/5000..  Training Loss: 0.907..  Test Loss: 0.930.. \n",
      "Epoch: 3118/5000..  Training Loss: 0.936..  Test Loss: 0.930.. \n",
      "Epoch: 3119/5000..  Training Loss: 0.921..  Test Loss: 0.930.. \n",
      "Epoch: 3120/5000..  Training Loss: 0.963..  Test Loss: 0.930.. \n",
      "Epoch: 3121/5000..  Training Loss: 0.929..  Test Loss: 0.930.. \n",
      "Epoch: 3122/5000..  Training Loss: 0.933..  Test Loss: 0.930.. \n",
      "Epoch: 3123/5000..  Training Loss: 0.938..  Test Loss: 0.930.. \n",
      "Epoch: 3124/5000..  Training Loss: 0.960..  Test Loss: 0.930.. \n",
      "Epoch: 3125/5000..  Training Loss: 0.909..  Test Loss: 0.930.. \n",
      "Epoch: 3126/5000..  Training Loss: 0.945..  Test Loss: 0.930.. \n",
      "Epoch: 3127/5000..  Training Loss: 0.956..  Test Loss: 0.930.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3128/5000..  Training Loss: 0.927..  Test Loss: 0.930.. \n",
      "Epoch: 3129/5000..  Training Loss: 0.908..  Test Loss: 0.930.. \n",
      "Epoch: 3130/5000..  Training Loss: 0.893..  Test Loss: 0.930.. \n",
      "Epoch: 3131/5000..  Training Loss: 0.950..  Test Loss: 0.930.. \n",
      "Epoch: 3132/5000..  Training Loss: 0.924..  Test Loss: 0.930.. \n",
      "Epoch: 3133/5000..  Training Loss: 0.910..  Test Loss: 0.930.. \n",
      "Epoch: 3134/5000..  Training Loss: 0.919..  Test Loss: 0.930.. \n",
      "Epoch: 3135/5000..  Training Loss: 0.949..  Test Loss: 0.930.. \n",
      "Epoch: 3136/5000..  Training Loss: 0.921..  Test Loss: 0.930.. \n",
      "Epoch: 3137/5000..  Training Loss: 0.897..  Test Loss: 0.930.. \n",
      "Epoch: 3138/5000..  Training Loss: 0.924..  Test Loss: 0.930.. \n",
      "Epoch: 3139/5000..  Training Loss: 0.953..  Test Loss: 0.930.. \n",
      "Epoch: 3140/5000..  Training Loss: 0.895..  Test Loss: 0.930.. \n",
      "Epoch: 3141/5000..  Training Loss: 0.923..  Test Loss: 0.930.. \n",
      "Epoch: 3142/5000..  Training Loss: 0.913..  Test Loss: 0.930.. \n",
      "Epoch: 3143/5000..  Training Loss: 0.949..  Test Loss: 0.930.. \n",
      "Epoch: 3144/5000..  Training Loss: 0.957..  Test Loss: 0.930.. \n",
      "Epoch: 3145/5000..  Training Loss: 0.956..  Test Loss: 0.930.. \n",
      "Epoch: 3146/5000..  Training Loss: 0.912..  Test Loss: 0.930.. \n",
      "Epoch: 3147/5000..  Training Loss: 0.921..  Test Loss: 0.930.. \n",
      "Epoch: 3148/5000..  Training Loss: 0.936..  Test Loss: 0.930.. \n",
      "Epoch: 3149/5000..  Training Loss: 0.948..  Test Loss: 0.930.. \n",
      "Epoch: 3150/5000..  Training Loss: 0.923..  Test Loss: 0.930.. \n",
      "Epoch: 3151/5000..  Training Loss: 0.937..  Test Loss: 0.930.. \n",
      "Epoch: 3152/5000..  Training Loss: 0.941..  Test Loss: 0.930.. \n",
      "Epoch: 3153/5000..  Training Loss: 0.928..  Test Loss: 0.930.. \n",
      "Epoch: 3154/5000..  Training Loss: 0.915..  Test Loss: 0.930.. \n",
      "Epoch: 3155/5000..  Training Loss: 0.930..  Test Loss: 0.930.. \n",
      "Epoch: 3156/5000..  Training Loss: 0.950..  Test Loss: 0.930.. \n",
      "Epoch: 3157/5000..  Training Loss: 0.932..  Test Loss: 0.930.. \n",
      "Epoch: 3158/5000..  Training Loss: 0.944..  Test Loss: 0.930.. \n",
      "Epoch: 3159/5000..  Training Loss: 0.957..  Test Loss: 0.930.. \n",
      "Epoch: 3160/5000..  Training Loss: 0.947..  Test Loss: 0.930.. \n",
      "Epoch: 3161/5000..  Training Loss: 0.901..  Test Loss: 0.930.. \n",
      "Epoch: 3162/5000..  Training Loss: 0.933..  Test Loss: 0.930.. \n",
      "Epoch: 3163/5000..  Training Loss: 0.921..  Test Loss: 0.930.. \n",
      "Epoch: 3164/5000..  Training Loss: 0.932..  Test Loss: 0.930.. \n",
      "Epoch: 3165/5000..  Training Loss: 0.921..  Test Loss: 0.930.. \n",
      "Epoch: 3166/5000..  Training Loss: 0.966..  Test Loss: 0.930.. \n",
      "Epoch: 3167/5000..  Training Loss: 0.967..  Test Loss: 0.930.. \n",
      "Epoch: 3168/5000..  Training Loss: 0.923..  Test Loss: 0.930.. \n",
      "Epoch: 3169/5000..  Training Loss: 0.913..  Test Loss: 0.930.. \n",
      "Epoch: 3170/5000..  Training Loss: 0.892..  Test Loss: 0.930.. \n",
      "Epoch: 3171/5000..  Training Loss: 0.909..  Test Loss: 0.930.. \n",
      "Epoch: 3172/5000..  Training Loss: 0.927..  Test Loss: 0.930.. \n",
      "Epoch: 3173/5000..  Training Loss: 0.949..  Test Loss: 0.930.. \n",
      "Epoch: 3174/5000..  Training Loss: 0.916..  Test Loss: 0.930.. \n",
      "Epoch: 3175/5000..  Training Loss: 0.935..  Test Loss: 0.930.. \n",
      "Epoch: 3176/5000..  Training Loss: 0.955..  Test Loss: 0.930.. \n",
      "Epoch: 3177/5000..  Training Loss: 0.921..  Test Loss: 0.930.. \n",
      "Epoch: 3178/5000..  Training Loss: 0.904..  Test Loss: 0.930.. \n",
      "Epoch: 3179/5000..  Training Loss: 0.893..  Test Loss: 0.930.. \n",
      "Epoch: 3180/5000..  Training Loss: 0.939..  Test Loss: 0.930.. \n",
      "Epoch: 3181/5000..  Training Loss: 0.966..  Test Loss: 0.930.. \n",
      "Epoch: 3182/5000..  Training Loss: 0.943..  Test Loss: 0.930.. \n",
      "Epoch: 3183/5000..  Training Loss: 0.926..  Test Loss: 0.930.. \n",
      "Epoch: 3184/5000..  Training Loss: 0.936..  Test Loss: 0.930.. \n",
      "Epoch: 3185/5000..  Training Loss: 0.960..  Test Loss: 0.930.. \n",
      "Epoch: 3186/5000..  Training Loss: 0.898..  Test Loss: 0.930.. \n",
      "Epoch: 3187/5000..  Training Loss: 0.886..  Test Loss: 0.930.. \n",
      "Epoch: 3188/5000..  Training Loss: 0.962..  Test Loss: 0.930.. \n",
      "Epoch: 3189/5000..  Training Loss: 0.928..  Test Loss: 0.930.. \n",
      "Epoch: 3190/5000..  Training Loss: 0.931..  Test Loss: 0.930.. \n",
      "Epoch: 3191/5000..  Training Loss: 0.933..  Test Loss: 0.930.. \n",
      "Epoch: 3192/5000..  Training Loss: 0.958..  Test Loss: 0.930.. \n",
      "Epoch: 3193/5000..  Training Loss: 0.930..  Test Loss: 0.929.. \n",
      "Epoch: 3194/5000..  Training Loss: 0.922..  Test Loss: 0.929.. \n",
      "Epoch: 3195/5000..  Training Loss: 0.892..  Test Loss: 0.929.. \n",
      "Epoch: 3196/5000..  Training Loss: 0.938..  Test Loss: 0.929.. \n",
      "Epoch: 3197/5000..  Training Loss: 0.944..  Test Loss: 0.929.. \n",
      "Epoch: 3198/5000..  Training Loss: 0.949..  Test Loss: 0.929.. \n",
      "Epoch: 3199/5000..  Training Loss: 0.941..  Test Loss: 0.929.. \n",
      "Epoch: 3200/5000..  Training Loss: 0.947..  Test Loss: 0.929.. \n",
      "Epoch: 3201/5000..  Training Loss: 0.924..  Test Loss: 0.929.. \n",
      "Epoch: 3202/5000..  Training Loss: 0.948..  Test Loss: 0.929.. \n",
      "Epoch: 3203/5000..  Training Loss: 0.947..  Test Loss: 0.929.. \n",
      "Epoch: 3204/5000..  Training Loss: 0.955..  Test Loss: 0.929.. \n",
      "Epoch: 3205/5000..  Training Loss: 0.905..  Test Loss: 0.929.. \n",
      "Epoch: 3206/5000..  Training Loss: 0.916..  Test Loss: 0.929.. \n",
      "Epoch: 3207/5000..  Training Loss: 0.917..  Test Loss: 0.929.. \n",
      "Epoch: 3208/5000..  Training Loss: 0.965..  Test Loss: 0.929.. \n",
      "Epoch: 3209/5000..  Training Loss: 0.928..  Test Loss: 0.929.. \n",
      "Epoch: 3210/5000..  Training Loss: 0.881..  Test Loss: 0.929.. \n",
      "Epoch: 3211/5000..  Training Loss: 0.939..  Test Loss: 0.929.. \n",
      "Epoch: 3212/5000..  Training Loss: 0.953..  Test Loss: 0.929.. \n",
      "Epoch: 3213/5000..  Training Loss: 0.948..  Test Loss: 0.929.. \n",
      "Epoch: 3214/5000..  Training Loss: 0.932..  Test Loss: 0.929.. \n",
      "Epoch: 3215/5000..  Training Loss: 0.907..  Test Loss: 0.929.. \n",
      "Epoch: 3216/5000..  Training Loss: 0.901..  Test Loss: 0.929.. \n",
      "Epoch: 3217/5000..  Training Loss: 0.944..  Test Loss: 0.929.. \n",
      "Epoch: 3218/5000..  Training Loss: 0.928..  Test Loss: 0.929.. \n",
      "Epoch: 3219/5000..  Training Loss: 0.903..  Test Loss: 0.929.. \n",
      "Epoch: 3220/5000..  Training Loss: 0.946..  Test Loss: 0.929.. \n",
      "Epoch: 3221/5000..  Training Loss: 0.945..  Test Loss: 0.929.. \n",
      "Epoch: 3222/5000..  Training Loss: 0.925..  Test Loss: 0.929.. \n",
      "Epoch: 3223/5000..  Training Loss: 0.918..  Test Loss: 0.929.. \n",
      "Epoch: 3224/5000..  Training Loss: 0.923..  Test Loss: 0.929.. \n",
      "Epoch: 3225/5000..  Training Loss: 0.939..  Test Loss: 0.929.. \n",
      "Epoch: 3226/5000..  Training Loss: 0.875..  Test Loss: 0.929.. \n",
      "Epoch: 3227/5000..  Training Loss: 0.904..  Test Loss: 0.929.. \n",
      "Epoch: 3228/5000..  Training Loss: 0.923..  Test Loss: 0.929.. \n",
      "Epoch: 3229/5000..  Training Loss: 0.908..  Test Loss: 0.929.. \n",
      "Epoch: 3230/5000..  Training Loss: 0.935..  Test Loss: 0.929.. \n",
      "Epoch: 3231/5000..  Training Loss: 0.934..  Test Loss: 0.929.. \n",
      "Epoch: 3232/5000..  Training Loss: 0.928..  Test Loss: 0.929.. \n",
      "Epoch: 3233/5000..  Training Loss: 0.922..  Test Loss: 0.929.. \n",
      "Epoch: 3234/5000..  Training Loss: 0.916..  Test Loss: 0.929.. \n",
      "Epoch: 3235/5000..  Training Loss: 0.922..  Test Loss: 0.929.. \n",
      "Epoch: 3236/5000..  Training Loss: 0.927..  Test Loss: 0.929.. \n",
      "Epoch: 3237/5000..  Training Loss: 0.940..  Test Loss: 0.929.. \n",
      "Epoch: 3238/5000..  Training Loss: 0.903..  Test Loss: 0.929.. \n",
      "Epoch: 3239/5000..  Training Loss: 0.909..  Test Loss: 0.929.. \n",
      "Epoch: 3240/5000..  Training Loss: 0.900..  Test Loss: 0.929.. \n",
      "Epoch: 3241/5000..  Training Loss: 0.916..  Test Loss: 0.929.. \n",
      "Epoch: 3242/5000..  Training Loss: 0.907..  Test Loss: 0.929.. \n",
      "Epoch: 3243/5000..  Training Loss: 0.926..  Test Loss: 0.929.. \n",
      "Epoch: 3244/5000..  Training Loss: 0.942..  Test Loss: 0.929.. \n",
      "Epoch: 3245/5000..  Training Loss: 0.898..  Test Loss: 0.929.. \n",
      "Epoch: 3246/5000..  Training Loss: 0.905..  Test Loss: 0.929.. \n",
      "Epoch: 3247/5000..  Training Loss: 0.929..  Test Loss: 0.929.. \n",
      "Epoch: 3248/5000..  Training Loss: 0.898..  Test Loss: 0.929.. \n",
      "Epoch: 3249/5000..  Training Loss: 0.954..  Test Loss: 0.929.. \n",
      "Epoch: 3250/5000..  Training Loss: 0.964..  Test Loss: 0.929.. \n",
      "Epoch: 3251/5000..  Training Loss: 0.937..  Test Loss: 0.929.. \n",
      "Epoch: 3252/5000..  Training Loss: 0.956..  Test Loss: 0.929.. \n",
      "Epoch: 3253/5000..  Training Loss: 0.944..  Test Loss: 0.929.. \n",
      "Epoch: 3254/5000..  Training Loss: 0.903..  Test Loss: 0.929.. \n",
      "Epoch: 3255/5000..  Training Loss: 0.944..  Test Loss: 0.929.. \n",
      "Epoch: 3256/5000..  Training Loss: 0.946..  Test Loss: 0.929.. \n",
      "Epoch: 3257/5000..  Training Loss: 0.944..  Test Loss: 0.929.. \n",
      "Epoch: 3258/5000..  Training Loss: 0.935..  Test Loss: 0.929.. \n",
      "Epoch: 3259/5000..  Training Loss: 0.915..  Test Loss: 0.929.. \n",
      "Epoch: 3260/5000..  Training Loss: 0.893..  Test Loss: 0.929.. \n",
      "Epoch: 3261/5000..  Training Loss: 0.954..  Test Loss: 0.929.. \n",
      "Epoch: 3262/5000..  Training Loss: 0.942..  Test Loss: 0.929.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3263/5000..  Training Loss: 0.910..  Test Loss: 0.929.. \n",
      "Epoch: 3264/5000..  Training Loss: 0.946..  Test Loss: 0.929.. \n",
      "Epoch: 3265/5000..  Training Loss: 0.942..  Test Loss: 0.929.. \n",
      "Epoch: 3266/5000..  Training Loss: 0.953..  Test Loss: 0.929.. \n",
      "Epoch: 3267/5000..  Training Loss: 0.909..  Test Loss: 0.929.. \n",
      "Epoch: 3268/5000..  Training Loss: 0.918..  Test Loss: 0.929.. \n",
      "Epoch: 3269/5000..  Training Loss: 0.939..  Test Loss: 0.929.. \n",
      "Epoch: 3270/5000..  Training Loss: 0.946..  Test Loss: 0.929.. \n",
      "Epoch: 3271/5000..  Training Loss: 0.900..  Test Loss: 0.929.. \n",
      "Epoch: 3272/5000..  Training Loss: 0.941..  Test Loss: 0.929.. \n",
      "Epoch: 3273/5000..  Training Loss: 0.951..  Test Loss: 0.929.. \n",
      "Epoch: 3274/5000..  Training Loss: 0.922..  Test Loss: 0.929.. \n",
      "Epoch: 3275/5000..  Training Loss: 0.924..  Test Loss: 0.929.. \n",
      "Epoch: 3276/5000..  Training Loss: 0.944..  Test Loss: 0.929.. \n",
      "Epoch: 3277/5000..  Training Loss: 0.937..  Test Loss: 0.929.. \n",
      "Epoch: 3278/5000..  Training Loss: 0.947..  Test Loss: 0.929.. \n",
      "Epoch: 3279/5000..  Training Loss: 0.971..  Test Loss: 0.929.. \n",
      "Epoch: 3280/5000..  Training Loss: 0.935..  Test Loss: 0.929.. \n",
      "Epoch: 3281/5000..  Training Loss: 0.920..  Test Loss: 0.929.. \n",
      "Epoch: 3282/5000..  Training Loss: 0.913..  Test Loss: 0.929.. \n",
      "Epoch: 3283/5000..  Training Loss: 0.934..  Test Loss: 0.929.. \n",
      "Epoch: 3284/5000..  Training Loss: 0.926..  Test Loss: 0.929.. \n",
      "Epoch: 3285/5000..  Training Loss: 0.933..  Test Loss: 0.929.. \n",
      "Epoch: 3286/5000..  Training Loss: 0.931..  Test Loss: 0.929.. \n",
      "Epoch: 3287/5000..  Training Loss: 0.901..  Test Loss: 0.929.. \n",
      "Epoch: 3288/5000..  Training Loss: 0.956..  Test Loss: 0.929.. \n",
      "Epoch: 3289/5000..  Training Loss: 0.982..  Test Loss: 0.929.. \n",
      "Epoch: 3290/5000..  Training Loss: 0.916..  Test Loss: 0.929.. \n",
      "Epoch: 3291/5000..  Training Loss: 0.941..  Test Loss: 0.929.. \n",
      "Epoch: 3292/5000..  Training Loss: 0.908..  Test Loss: 0.929.. \n",
      "Epoch: 3293/5000..  Training Loss: 0.925..  Test Loss: 0.929.. \n",
      "Epoch: 3294/5000..  Training Loss: 0.922..  Test Loss: 0.929.. \n",
      "Epoch: 3295/5000..  Training Loss: 0.911..  Test Loss: 0.929.. \n",
      "Epoch: 3296/5000..  Training Loss: 0.931..  Test Loss: 0.929.. \n",
      "Epoch: 3297/5000..  Training Loss: 0.919..  Test Loss: 0.929.. \n",
      "Epoch: 3298/5000..  Training Loss: 0.938..  Test Loss: 0.929.. \n",
      "Epoch: 3299/5000..  Training Loss: 0.892..  Test Loss: 0.929.. \n",
      "Epoch: 3300/5000..  Training Loss: 0.922..  Test Loss: 0.929.. \n",
      "Epoch: 3301/5000..  Training Loss: 0.939..  Test Loss: 0.929.. \n",
      "Epoch: 3302/5000..  Training Loss: 0.938..  Test Loss: 0.929.. \n",
      "Epoch: 3303/5000..  Training Loss: 0.955..  Test Loss: 0.929.. \n",
      "Epoch: 3304/5000..  Training Loss: 0.954..  Test Loss: 0.929.. \n",
      "Epoch: 3305/5000..  Training Loss: 0.941..  Test Loss: 0.929.. \n",
      "Epoch: 3306/5000..  Training Loss: 0.939..  Test Loss: 0.929.. \n",
      "Epoch: 3307/5000..  Training Loss: 0.946..  Test Loss: 0.929.. \n",
      "Epoch: 3308/5000..  Training Loss: 0.872..  Test Loss: 0.929.. \n",
      "Epoch: 3309/5000..  Training Loss: 0.909..  Test Loss: 0.929.. \n",
      "Epoch: 3310/5000..  Training Loss: 0.970..  Test Loss: 0.929.. \n",
      "Epoch: 3311/5000..  Training Loss: 0.888..  Test Loss: 0.929.. \n",
      "Epoch: 3312/5000..  Training Loss: 0.925..  Test Loss: 0.929.. \n",
      "Epoch: 3313/5000..  Training Loss: 0.944..  Test Loss: 0.929.. \n",
      "Epoch: 3314/5000..  Training Loss: 0.941..  Test Loss: 0.929.. \n",
      "Epoch: 3315/5000..  Training Loss: 0.932..  Test Loss: 0.929.. \n",
      "Epoch: 3316/5000..  Training Loss: 0.922..  Test Loss: 0.929.. \n",
      "Epoch: 3317/5000..  Training Loss: 0.914..  Test Loss: 0.929.. \n",
      "Epoch: 3318/5000..  Training Loss: 0.930..  Test Loss: 0.929.. \n",
      "Epoch: 3319/5000..  Training Loss: 0.900..  Test Loss: 0.929.. \n",
      "Epoch: 3320/5000..  Training Loss: 0.917..  Test Loss: 0.929.. \n",
      "Epoch: 3321/5000..  Training Loss: 0.945..  Test Loss: 0.929.. \n",
      "Epoch: 3322/5000..  Training Loss: 0.956..  Test Loss: 0.929.. \n",
      "Epoch: 3323/5000..  Training Loss: 0.959..  Test Loss: 0.929.. \n",
      "Epoch: 3324/5000..  Training Loss: 0.925..  Test Loss: 0.929.. \n",
      "Epoch: 3325/5000..  Training Loss: 0.952..  Test Loss: 0.929.. \n",
      "Epoch: 3326/5000..  Training Loss: 0.917..  Test Loss: 0.929.. \n",
      "Epoch: 3327/5000..  Training Loss: 0.941..  Test Loss: 0.929.. \n",
      "Epoch: 3328/5000..  Training Loss: 0.876..  Test Loss: 0.929.. \n",
      "Epoch: 3329/5000..  Training Loss: 0.906..  Test Loss: 0.929.. \n",
      "Epoch: 3330/5000..  Training Loss: 0.872..  Test Loss: 0.929.. \n",
      "Epoch: 3331/5000..  Training Loss: 0.945..  Test Loss: 0.929.. \n",
      "Epoch: 3332/5000..  Training Loss: 0.925..  Test Loss: 0.928.. \n",
      "Epoch: 3333/5000..  Training Loss: 0.956..  Test Loss: 0.928.. \n",
      "Epoch: 3334/5000..  Training Loss: 0.965..  Test Loss: 0.928.. \n",
      "Epoch: 3335/5000..  Training Loss: 0.949..  Test Loss: 0.928.. \n",
      "Epoch: 3336/5000..  Training Loss: 0.937..  Test Loss: 0.928.. \n",
      "Epoch: 3337/5000..  Training Loss: 0.936..  Test Loss: 0.928.. \n",
      "Epoch: 3338/5000..  Training Loss: 0.969..  Test Loss: 0.928.. \n",
      "Epoch: 3339/5000..  Training Loss: 0.919..  Test Loss: 0.928.. \n",
      "Epoch: 3340/5000..  Training Loss: 0.961..  Test Loss: 0.928.. \n",
      "Epoch: 3341/5000..  Training Loss: 0.915..  Test Loss: 0.928.. \n",
      "Epoch: 3342/5000..  Training Loss: 0.937..  Test Loss: 0.928.. \n",
      "Epoch: 3343/5000..  Training Loss: 0.940..  Test Loss: 0.928.. \n",
      "Epoch: 3344/5000..  Training Loss: 0.956..  Test Loss: 0.928.. \n",
      "Epoch: 3345/5000..  Training Loss: 0.906..  Test Loss: 0.928.. \n",
      "Epoch: 3346/5000..  Training Loss: 0.905..  Test Loss: 0.928.. \n",
      "Epoch: 3347/5000..  Training Loss: 0.953..  Test Loss: 0.928.. \n",
      "Epoch: 3348/5000..  Training Loss: 0.887..  Test Loss: 0.928.. \n",
      "Epoch: 3349/5000..  Training Loss: 0.943..  Test Loss: 0.928.. \n",
      "Epoch: 3350/5000..  Training Loss: 0.880..  Test Loss: 0.928.. \n",
      "Epoch: 3351/5000..  Training Loss: 0.930..  Test Loss: 0.928.. \n",
      "Epoch: 3352/5000..  Training Loss: 0.920..  Test Loss: 0.928.. \n",
      "Epoch: 3353/5000..  Training Loss: 0.939..  Test Loss: 0.928.. \n",
      "Epoch: 3354/5000..  Training Loss: 0.931..  Test Loss: 0.928.. \n",
      "Epoch: 3355/5000..  Training Loss: 0.912..  Test Loss: 0.928.. \n",
      "Epoch: 3356/5000..  Training Loss: 0.936..  Test Loss: 0.928.. \n",
      "Epoch: 3357/5000..  Training Loss: 0.915..  Test Loss: 0.928.. \n",
      "Epoch: 3358/5000..  Training Loss: 0.918..  Test Loss: 0.928.. \n",
      "Epoch: 3359/5000..  Training Loss: 0.909..  Test Loss: 0.928.. \n",
      "Epoch: 3360/5000..  Training Loss: 0.950..  Test Loss: 0.928.. \n",
      "Epoch: 3361/5000..  Training Loss: 0.904..  Test Loss: 0.928.. \n",
      "Epoch: 3362/5000..  Training Loss: 0.912..  Test Loss: 0.928.. \n",
      "Epoch: 3363/5000..  Training Loss: 0.909..  Test Loss: 0.928.. \n",
      "Epoch: 3364/5000..  Training Loss: 0.916..  Test Loss: 0.928.. \n",
      "Epoch: 3365/5000..  Training Loss: 0.948..  Test Loss: 0.928.. \n",
      "Epoch: 3366/5000..  Training Loss: 0.928..  Test Loss: 0.928.. \n",
      "Epoch: 3367/5000..  Training Loss: 0.935..  Test Loss: 0.928.. \n",
      "Epoch: 3368/5000..  Training Loss: 0.935..  Test Loss: 0.928.. \n",
      "Epoch: 3369/5000..  Training Loss: 0.913..  Test Loss: 0.928.. \n",
      "Epoch: 3370/5000..  Training Loss: 0.934..  Test Loss: 0.928.. \n",
      "Epoch: 3371/5000..  Training Loss: 0.914..  Test Loss: 0.928.. \n",
      "Epoch: 3372/5000..  Training Loss: 0.932..  Test Loss: 0.928.. \n",
      "Epoch: 3373/5000..  Training Loss: 0.942..  Test Loss: 0.928.. \n",
      "Epoch: 3374/5000..  Training Loss: 0.925..  Test Loss: 0.928.. \n",
      "Epoch: 3375/5000..  Training Loss: 0.906..  Test Loss: 0.928.. \n",
      "Epoch: 3376/5000..  Training Loss: 0.937..  Test Loss: 0.928.. \n",
      "Epoch: 3377/5000..  Training Loss: 0.938..  Test Loss: 0.928.. \n",
      "Epoch: 3378/5000..  Training Loss: 0.917..  Test Loss: 0.928.. \n",
      "Epoch: 3379/5000..  Training Loss: 0.907..  Test Loss: 0.928.. \n",
      "Epoch: 3380/5000..  Training Loss: 0.923..  Test Loss: 0.928.. \n",
      "Epoch: 3381/5000..  Training Loss: 0.921..  Test Loss: 0.928.. \n",
      "Epoch: 3382/5000..  Training Loss: 0.936..  Test Loss: 0.928.. \n",
      "Epoch: 3383/5000..  Training Loss: 0.938..  Test Loss: 0.928.. \n",
      "Epoch: 3384/5000..  Training Loss: 0.908..  Test Loss: 0.928.. \n",
      "Epoch: 3385/5000..  Training Loss: 0.926..  Test Loss: 0.928.. \n",
      "Epoch: 3386/5000..  Training Loss: 0.965..  Test Loss: 0.928.. \n",
      "Epoch: 3387/5000..  Training Loss: 0.936..  Test Loss: 0.928.. \n",
      "Epoch: 3388/5000..  Training Loss: 0.904..  Test Loss: 0.928.. \n",
      "Epoch: 3389/5000..  Training Loss: 0.924..  Test Loss: 0.928.. \n",
      "Epoch: 3390/5000..  Training Loss: 0.914..  Test Loss: 0.928.. \n",
      "Epoch: 3391/5000..  Training Loss: 0.970..  Test Loss: 0.928.. \n",
      "Epoch: 3392/5000..  Training Loss: 0.935..  Test Loss: 0.928.. \n",
      "Epoch: 3393/5000..  Training Loss: 0.923..  Test Loss: 0.928.. \n",
      "Epoch: 3394/5000..  Training Loss: 0.895..  Test Loss: 0.928.. \n",
      "Epoch: 3395/5000..  Training Loss: 0.957..  Test Loss: 0.928.. \n",
      "Epoch: 3396/5000..  Training Loss: 0.926..  Test Loss: 0.928.. \n",
      "Epoch: 3397/5000..  Training Loss: 0.899..  Test Loss: 0.928.. \n",
      "Epoch: 3398/5000..  Training Loss: 0.929..  Test Loss: 0.928.. \n",
      "Epoch: 3399/5000..  Training Loss: 0.952..  Test Loss: 0.928.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3400/5000..  Training Loss: 0.968..  Test Loss: 0.928.. \n",
      "Epoch: 3401/5000..  Training Loss: 0.915..  Test Loss: 0.928.. \n",
      "Epoch: 3402/5000..  Training Loss: 0.945..  Test Loss: 0.928.. \n",
      "Epoch: 3403/5000..  Training Loss: 0.909..  Test Loss: 0.928.. \n",
      "Epoch: 3404/5000..  Training Loss: 0.928..  Test Loss: 0.928.. \n",
      "Epoch: 3405/5000..  Training Loss: 0.956..  Test Loss: 0.928.. \n",
      "Epoch: 3406/5000..  Training Loss: 0.943..  Test Loss: 0.928.. \n",
      "Epoch: 3407/5000..  Training Loss: 0.919..  Test Loss: 0.928.. \n",
      "Epoch: 3408/5000..  Training Loss: 0.918..  Test Loss: 0.928.. \n",
      "Epoch: 3409/5000..  Training Loss: 0.896..  Test Loss: 0.928.. \n",
      "Epoch: 3410/5000..  Training Loss: 0.932..  Test Loss: 0.928.. \n",
      "Epoch: 3411/5000..  Training Loss: 0.946..  Test Loss: 0.928.. \n",
      "Epoch: 3412/5000..  Training Loss: 0.916..  Test Loss: 0.928.. \n",
      "Epoch: 3413/5000..  Training Loss: 0.928..  Test Loss: 0.928.. \n",
      "Epoch: 3414/5000..  Training Loss: 0.925..  Test Loss: 0.928.. \n",
      "Epoch: 3415/5000..  Training Loss: 0.903..  Test Loss: 0.928.. \n",
      "Epoch: 3416/5000..  Training Loss: 0.894..  Test Loss: 0.928.. \n",
      "Epoch: 3417/5000..  Training Loss: 0.925..  Test Loss: 0.928.. \n",
      "Epoch: 3418/5000..  Training Loss: 0.920..  Test Loss: 0.928.. \n",
      "Epoch: 3419/5000..  Training Loss: 0.940..  Test Loss: 0.928.. \n",
      "Epoch: 3420/5000..  Training Loss: 0.932..  Test Loss: 0.928.. \n",
      "Epoch: 3421/5000..  Training Loss: 0.943..  Test Loss: 0.928.. \n",
      "Epoch: 3422/5000..  Training Loss: 0.919..  Test Loss: 0.928.. \n",
      "Epoch: 3423/5000..  Training Loss: 0.956..  Test Loss: 0.928.. \n",
      "Epoch: 3424/5000..  Training Loss: 0.935..  Test Loss: 0.928.. \n",
      "Epoch: 3425/5000..  Training Loss: 0.925..  Test Loss: 0.928.. \n",
      "Epoch: 3426/5000..  Training Loss: 0.944..  Test Loss: 0.928.. \n",
      "Epoch: 3427/5000..  Training Loss: 0.927..  Test Loss: 0.928.. \n",
      "Epoch: 3428/5000..  Training Loss: 0.910..  Test Loss: 0.928.. \n",
      "Epoch: 3429/5000..  Training Loss: 0.925..  Test Loss: 0.928.. \n",
      "Epoch: 3430/5000..  Training Loss: 0.954..  Test Loss: 0.928.. \n",
      "Epoch: 3431/5000..  Training Loss: 0.937..  Test Loss: 0.928.. \n",
      "Epoch: 3432/5000..  Training Loss: 0.949..  Test Loss: 0.928.. \n",
      "Epoch: 3433/5000..  Training Loss: 0.936..  Test Loss: 0.928.. \n",
      "Epoch: 3434/5000..  Training Loss: 0.908..  Test Loss: 0.928.. \n",
      "Epoch: 3435/5000..  Training Loss: 0.936..  Test Loss: 0.928.. \n",
      "Epoch: 3436/5000..  Training Loss: 0.931..  Test Loss: 0.928.. \n",
      "Epoch: 3437/5000..  Training Loss: 0.923..  Test Loss: 0.928.. \n",
      "Epoch: 3438/5000..  Training Loss: 0.959..  Test Loss: 0.928.. \n",
      "Epoch: 3439/5000..  Training Loss: 0.924..  Test Loss: 0.928.. \n",
      "Epoch: 3440/5000..  Training Loss: 0.942..  Test Loss: 0.928.. \n",
      "Epoch: 3441/5000..  Training Loss: 0.921..  Test Loss: 0.928.. \n",
      "Epoch: 3442/5000..  Training Loss: 0.919..  Test Loss: 0.928.. \n",
      "Epoch: 3443/5000..  Training Loss: 0.934..  Test Loss: 0.928.. \n",
      "Epoch: 3444/5000..  Training Loss: 0.892..  Test Loss: 0.928.. \n",
      "Epoch: 3445/5000..  Training Loss: 0.926..  Test Loss: 0.928.. \n",
      "Epoch: 3446/5000..  Training Loss: 0.959..  Test Loss: 0.928.. \n",
      "Epoch: 3447/5000..  Training Loss: 0.931..  Test Loss: 0.928.. \n",
      "Epoch: 3448/5000..  Training Loss: 0.906..  Test Loss: 0.928.. \n",
      "Epoch: 3449/5000..  Training Loss: 0.911..  Test Loss: 0.928.. \n",
      "Epoch: 3450/5000..  Training Loss: 0.919..  Test Loss: 0.928.. \n",
      "Epoch: 3451/5000..  Training Loss: 0.946..  Test Loss: 0.928.. \n",
      "Epoch: 3452/5000..  Training Loss: 0.909..  Test Loss: 0.928.. \n",
      "Epoch: 3453/5000..  Training Loss: 0.944..  Test Loss: 0.928.. \n",
      "Epoch: 3454/5000..  Training Loss: 0.950..  Test Loss: 0.928.. \n",
      "Epoch: 3455/5000..  Training Loss: 0.936..  Test Loss: 0.928.. \n",
      "Epoch: 3456/5000..  Training Loss: 0.898..  Test Loss: 0.928.. \n",
      "Epoch: 3457/5000..  Training Loss: 0.939..  Test Loss: 0.928.. \n",
      "Epoch: 3458/5000..  Training Loss: 0.954..  Test Loss: 0.928.. \n",
      "Epoch: 3459/5000..  Training Loss: 0.955..  Test Loss: 0.928.. \n",
      "Epoch: 3460/5000..  Training Loss: 0.935..  Test Loss: 0.928.. \n",
      "Epoch: 3461/5000..  Training Loss: 0.904..  Test Loss: 0.928.. \n",
      "Epoch: 3462/5000..  Training Loss: 0.962..  Test Loss: 0.928.. \n",
      "Epoch: 3463/5000..  Training Loss: 0.939..  Test Loss: 0.928.. \n",
      "Epoch: 3464/5000..  Training Loss: 0.938..  Test Loss: 0.928.. \n",
      "Epoch: 3465/5000..  Training Loss: 0.922..  Test Loss: 0.928.. \n",
      "Epoch: 3466/5000..  Training Loss: 0.956..  Test Loss: 0.928.. \n",
      "Epoch: 3467/5000..  Training Loss: 0.943..  Test Loss: 0.928.. \n",
      "Epoch: 3468/5000..  Training Loss: 0.907..  Test Loss: 0.928.. \n",
      "Epoch: 3469/5000..  Training Loss: 0.963..  Test Loss: 0.928.. \n",
      "Epoch: 3470/5000..  Training Loss: 0.901..  Test Loss: 0.928.. \n",
      "Epoch: 3471/5000..  Training Loss: 0.959..  Test Loss: 0.928.. \n",
      "Epoch: 3472/5000..  Training Loss: 0.933..  Test Loss: 0.928.. \n",
      "Epoch: 3473/5000..  Training Loss: 0.926..  Test Loss: 0.928.. \n",
      "Epoch: 3474/5000..  Training Loss: 0.913..  Test Loss: 0.928.. \n",
      "Epoch: 3475/5000..  Training Loss: 0.927..  Test Loss: 0.928.. \n",
      "Epoch: 3476/5000..  Training Loss: 0.920..  Test Loss: 0.928.. \n",
      "Epoch: 3477/5000..  Training Loss: 0.949..  Test Loss: 0.928.. \n",
      "Epoch: 3478/5000..  Training Loss: 0.937..  Test Loss: 0.928.. \n",
      "Epoch: 3479/5000..  Training Loss: 0.942..  Test Loss: 0.928.. \n",
      "Epoch: 3480/5000..  Training Loss: 0.926..  Test Loss: 0.927.. \n",
      "Epoch: 3481/5000..  Training Loss: 0.914..  Test Loss: 0.927.. \n",
      "Epoch: 3482/5000..  Training Loss: 0.947..  Test Loss: 0.927.. \n",
      "Epoch: 3483/5000..  Training Loss: 0.940..  Test Loss: 0.927.. \n",
      "Epoch: 3484/5000..  Training Loss: 0.881..  Test Loss: 0.927.. \n",
      "Epoch: 3485/5000..  Training Loss: 0.906..  Test Loss: 0.927.. \n",
      "Epoch: 3486/5000..  Training Loss: 0.919..  Test Loss: 0.927.. \n",
      "Epoch: 3487/5000..  Training Loss: 0.924..  Test Loss: 0.927.. \n",
      "Epoch: 3488/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3489/5000..  Training Loss: 0.958..  Test Loss: 0.927.. \n",
      "Epoch: 3490/5000..  Training Loss: 0.897..  Test Loss: 0.927.. \n",
      "Epoch: 3491/5000..  Training Loss: 0.932..  Test Loss: 0.927.. \n",
      "Epoch: 3492/5000..  Training Loss: 0.939..  Test Loss: 0.927.. \n",
      "Epoch: 3493/5000..  Training Loss: 0.935..  Test Loss: 0.927.. \n",
      "Epoch: 3494/5000..  Training Loss: 0.920..  Test Loss: 0.927.. \n",
      "Epoch: 3495/5000..  Training Loss: 0.939..  Test Loss: 0.927.. \n",
      "Epoch: 3496/5000..  Training Loss: 0.925..  Test Loss: 0.927.. \n",
      "Epoch: 3497/5000..  Training Loss: 0.932..  Test Loss: 0.927.. \n",
      "Epoch: 3498/5000..  Training Loss: 0.923..  Test Loss: 0.927.. \n",
      "Epoch: 3499/5000..  Training Loss: 0.908..  Test Loss: 0.927.. \n",
      "Epoch: 3500/5000..  Training Loss: 0.893..  Test Loss: 0.927.. \n",
      "Epoch: 3501/5000..  Training Loss: 0.906..  Test Loss: 0.927.. \n",
      "Epoch: 3502/5000..  Training Loss: 0.915..  Test Loss: 0.927.. \n",
      "Epoch: 3503/5000..  Training Loss: 0.912..  Test Loss: 0.927.. \n",
      "Epoch: 3504/5000..  Training Loss: 0.933..  Test Loss: 0.927.. \n",
      "Epoch: 3505/5000..  Training Loss: 0.947..  Test Loss: 0.927.. \n",
      "Epoch: 3506/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3507/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3508/5000..  Training Loss: 0.946..  Test Loss: 0.927.. \n",
      "Epoch: 3509/5000..  Training Loss: 0.978..  Test Loss: 0.927.. \n",
      "Epoch: 3510/5000..  Training Loss: 0.918..  Test Loss: 0.927.. \n",
      "Epoch: 3511/5000..  Training Loss: 0.930..  Test Loss: 0.927.. \n",
      "Epoch: 3512/5000..  Training Loss: 0.925..  Test Loss: 0.927.. \n",
      "Epoch: 3513/5000..  Training Loss: 0.921..  Test Loss: 0.927.. \n",
      "Epoch: 3514/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3515/5000..  Training Loss: 0.929..  Test Loss: 0.927.. \n",
      "Epoch: 3516/5000..  Training Loss: 0.934..  Test Loss: 0.927.. \n",
      "Epoch: 3517/5000..  Training Loss: 0.957..  Test Loss: 0.927.. \n",
      "Epoch: 3518/5000..  Training Loss: 0.947..  Test Loss: 0.927.. \n",
      "Epoch: 3519/5000..  Training Loss: 0.955..  Test Loss: 0.927.. \n",
      "Epoch: 3520/5000..  Training Loss: 0.923..  Test Loss: 0.927.. \n",
      "Epoch: 3521/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3522/5000..  Training Loss: 0.932..  Test Loss: 0.927.. \n",
      "Epoch: 3523/5000..  Training Loss: 0.915..  Test Loss: 0.927.. \n",
      "Epoch: 3524/5000..  Training Loss: 0.956..  Test Loss: 0.927.. \n",
      "Epoch: 3525/5000..  Training Loss: 0.926..  Test Loss: 0.927.. \n",
      "Epoch: 3526/5000..  Training Loss: 0.924..  Test Loss: 0.927.. \n",
      "Epoch: 3527/5000..  Training Loss: 0.931..  Test Loss: 0.927.. \n",
      "Epoch: 3528/5000..  Training Loss: 0.930..  Test Loss: 0.927.. \n",
      "Epoch: 3529/5000..  Training Loss: 0.930..  Test Loss: 0.927.. \n",
      "Epoch: 3530/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3531/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3532/5000..  Training Loss: 0.915..  Test Loss: 0.927.. \n",
      "Epoch: 3533/5000..  Training Loss: 0.930..  Test Loss: 0.927.. \n",
      "Epoch: 3534/5000..  Training Loss: 0.928..  Test Loss: 0.927.. \n",
      "Epoch: 3535/5000..  Training Loss: 0.922..  Test Loss: 0.927.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3536/5000..  Training Loss: 0.907..  Test Loss: 0.927.. \n",
      "Epoch: 3537/5000..  Training Loss: 0.923..  Test Loss: 0.927.. \n",
      "Epoch: 3538/5000..  Training Loss: 0.907..  Test Loss: 0.927.. \n",
      "Epoch: 3539/5000..  Training Loss: 0.914..  Test Loss: 0.927.. \n",
      "Epoch: 3540/5000..  Training Loss: 0.939..  Test Loss: 0.927.. \n",
      "Epoch: 3541/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3542/5000..  Training Loss: 0.933..  Test Loss: 0.927.. \n",
      "Epoch: 3543/5000..  Training Loss: 0.916..  Test Loss: 0.927.. \n",
      "Epoch: 3544/5000..  Training Loss: 0.943..  Test Loss: 0.927.. \n",
      "Epoch: 3545/5000..  Training Loss: 0.943..  Test Loss: 0.927.. \n",
      "Epoch: 3546/5000..  Training Loss: 0.922..  Test Loss: 0.927.. \n",
      "Epoch: 3547/5000..  Training Loss: 0.939..  Test Loss: 0.927.. \n",
      "Epoch: 3548/5000..  Training Loss: 0.933..  Test Loss: 0.927.. \n",
      "Epoch: 3549/5000..  Training Loss: 0.916..  Test Loss: 0.927.. \n",
      "Epoch: 3550/5000..  Training Loss: 0.930..  Test Loss: 0.927.. \n",
      "Epoch: 3551/5000..  Training Loss: 0.955..  Test Loss: 0.927.. \n",
      "Epoch: 3552/5000..  Training Loss: 0.907..  Test Loss: 0.927.. \n",
      "Epoch: 3553/5000..  Training Loss: 0.944..  Test Loss: 0.927.. \n",
      "Epoch: 3554/5000..  Training Loss: 0.889..  Test Loss: 0.927.. \n",
      "Epoch: 3555/5000..  Training Loss: 0.901..  Test Loss: 0.927.. \n",
      "Epoch: 3556/5000..  Training Loss: 0.931..  Test Loss: 0.927.. \n",
      "Epoch: 3557/5000..  Training Loss: 0.937..  Test Loss: 0.927.. \n",
      "Epoch: 3558/5000..  Training Loss: 0.937..  Test Loss: 0.927.. \n",
      "Epoch: 3559/5000..  Training Loss: 0.935..  Test Loss: 0.927.. \n",
      "Epoch: 3560/5000..  Training Loss: 0.897..  Test Loss: 0.927.. \n",
      "Epoch: 3561/5000..  Training Loss: 0.900..  Test Loss: 0.927.. \n",
      "Epoch: 3562/5000..  Training Loss: 0.934..  Test Loss: 0.927.. \n",
      "Epoch: 3563/5000..  Training Loss: 0.914..  Test Loss: 0.927.. \n",
      "Epoch: 3564/5000..  Training Loss: 0.957..  Test Loss: 0.927.. \n",
      "Epoch: 3565/5000..  Training Loss: 0.922..  Test Loss: 0.927.. \n",
      "Epoch: 3566/5000..  Training Loss: 0.942..  Test Loss: 0.927.. \n",
      "Epoch: 3567/5000..  Training Loss: 0.941..  Test Loss: 0.927.. \n",
      "Epoch: 3568/5000..  Training Loss: 0.923..  Test Loss: 0.927.. \n",
      "Epoch: 3569/5000..  Training Loss: 0.912..  Test Loss: 0.927.. \n",
      "Epoch: 3570/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3571/5000..  Training Loss: 0.932..  Test Loss: 0.927.. \n",
      "Epoch: 3572/5000..  Training Loss: 0.901..  Test Loss: 0.927.. \n",
      "Epoch: 3573/5000..  Training Loss: 0.929..  Test Loss: 0.927.. \n",
      "Epoch: 3574/5000..  Training Loss: 0.926..  Test Loss: 0.927.. \n",
      "Epoch: 3575/5000..  Training Loss: 0.902..  Test Loss: 0.927.. \n",
      "Epoch: 3576/5000..  Training Loss: 0.917..  Test Loss: 0.927.. \n",
      "Epoch: 3577/5000..  Training Loss: 0.901..  Test Loss: 0.927.. \n",
      "Epoch: 3578/5000..  Training Loss: 0.931..  Test Loss: 0.927.. \n",
      "Epoch: 3579/5000..  Training Loss: 0.937..  Test Loss: 0.927.. \n",
      "Epoch: 3580/5000..  Training Loss: 0.938..  Test Loss: 0.927.. \n",
      "Epoch: 3581/5000..  Training Loss: 0.927..  Test Loss: 0.927.. \n",
      "Epoch: 3582/5000..  Training Loss: 0.921..  Test Loss: 0.927.. \n",
      "Epoch: 3583/5000..  Training Loss: 0.921..  Test Loss: 0.927.. \n",
      "Epoch: 3584/5000..  Training Loss: 0.917..  Test Loss: 0.927.. \n",
      "Epoch: 3585/5000..  Training Loss: 0.922..  Test Loss: 0.927.. \n",
      "Epoch: 3586/5000..  Training Loss: 0.952..  Test Loss: 0.927.. \n",
      "Epoch: 3587/5000..  Training Loss: 0.932..  Test Loss: 0.927.. \n",
      "Epoch: 3588/5000..  Training Loss: 0.922..  Test Loss: 0.927.. \n",
      "Epoch: 3589/5000..  Training Loss: 0.922..  Test Loss: 0.927.. \n",
      "Epoch: 3590/5000..  Training Loss: 0.936..  Test Loss: 0.927.. \n",
      "Epoch: 3591/5000..  Training Loss: 0.938..  Test Loss: 0.927.. \n",
      "Epoch: 3592/5000..  Training Loss: 0.914..  Test Loss: 0.927.. \n",
      "Epoch: 3593/5000..  Training Loss: 0.934..  Test Loss: 0.927.. \n",
      "Epoch: 3594/5000..  Training Loss: 0.949..  Test Loss: 0.927.. \n",
      "Epoch: 3595/5000..  Training Loss: 0.924..  Test Loss: 0.927.. \n",
      "Epoch: 3596/5000..  Training Loss: 0.915..  Test Loss: 0.927.. \n",
      "Epoch: 3597/5000..  Training Loss: 0.942..  Test Loss: 0.927.. \n",
      "Epoch: 3598/5000..  Training Loss: 0.903..  Test Loss: 0.927.. \n",
      "Epoch: 3599/5000..  Training Loss: 0.939..  Test Loss: 0.927.. \n",
      "Epoch: 3600/5000..  Training Loss: 0.947..  Test Loss: 0.927.. \n",
      "Epoch: 3601/5000..  Training Loss: 0.956..  Test Loss: 0.927.. \n",
      "Epoch: 3602/5000..  Training Loss: 0.907..  Test Loss: 0.927.. \n",
      "Epoch: 3603/5000..  Training Loss: 0.929..  Test Loss: 0.927.. \n",
      "Epoch: 3604/5000..  Training Loss: 0.911..  Test Loss: 0.927.. \n",
      "Epoch: 3605/5000..  Training Loss: 0.923..  Test Loss: 0.927.. \n",
      "Epoch: 3606/5000..  Training Loss: 0.953..  Test Loss: 0.927.. \n",
      "Epoch: 3607/5000..  Training Loss: 0.903..  Test Loss: 0.927.. \n",
      "Epoch: 3608/5000..  Training Loss: 0.928..  Test Loss: 0.927.. \n",
      "Epoch: 3609/5000..  Training Loss: 0.920..  Test Loss: 0.927.. \n",
      "Epoch: 3610/5000..  Training Loss: 0.926..  Test Loss: 0.927.. \n",
      "Epoch: 3611/5000..  Training Loss: 0.942..  Test Loss: 0.927.. \n",
      "Epoch: 3612/5000..  Training Loss: 0.920..  Test Loss: 0.927.. \n",
      "Epoch: 3613/5000..  Training Loss: 0.901..  Test Loss: 0.927.. \n",
      "Epoch: 3614/5000..  Training Loss: 0.939..  Test Loss: 0.927.. \n",
      "Epoch: 3615/5000..  Training Loss: 0.909..  Test Loss: 0.927.. \n",
      "Epoch: 3616/5000..  Training Loss: 0.941..  Test Loss: 0.927.. \n",
      "Epoch: 3617/5000..  Training Loss: 0.946..  Test Loss: 0.927.. \n",
      "Epoch: 3618/5000..  Training Loss: 0.936..  Test Loss: 0.927.. \n",
      "Epoch: 3619/5000..  Training Loss: 0.917..  Test Loss: 0.927.. \n",
      "Epoch: 3620/5000..  Training Loss: 0.926..  Test Loss: 0.927.. \n",
      "Epoch: 3621/5000..  Training Loss: 0.939..  Test Loss: 0.927.. \n",
      "Epoch: 3622/5000..  Training Loss: 0.961..  Test Loss: 0.927.. \n",
      "Epoch: 3623/5000..  Training Loss: 0.938..  Test Loss: 0.927.. \n",
      "Epoch: 3624/5000..  Training Loss: 0.962..  Test Loss: 0.927.. \n",
      "Epoch: 3625/5000..  Training Loss: 0.936..  Test Loss: 0.927.. \n",
      "Epoch: 3626/5000..  Training Loss: 0.946..  Test Loss: 0.927.. \n",
      "Epoch: 3627/5000..  Training Loss: 0.917..  Test Loss: 0.927.. \n",
      "Epoch: 3628/5000..  Training Loss: 0.930..  Test Loss: 0.927.. \n",
      "Epoch: 3629/5000..  Training Loss: 0.943..  Test Loss: 0.927.. \n",
      "Epoch: 3630/5000..  Training Loss: 0.937..  Test Loss: 0.927.. \n",
      "Epoch: 3631/5000..  Training Loss: 0.924..  Test Loss: 0.927.. \n",
      "Epoch: 3632/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3633/5000..  Training Loss: 0.883..  Test Loss: 0.926.. \n",
      "Epoch: 3634/5000..  Training Loss: 0.928..  Test Loss: 0.926.. \n",
      "Epoch: 3635/5000..  Training Loss: 0.920..  Test Loss: 0.926.. \n",
      "Epoch: 3636/5000..  Training Loss: 0.914..  Test Loss: 0.926.. \n",
      "Epoch: 3637/5000..  Training Loss: 0.945..  Test Loss: 0.926.. \n",
      "Epoch: 3638/5000..  Training Loss: 0.897..  Test Loss: 0.926.. \n",
      "Epoch: 3639/5000..  Training Loss: 0.947..  Test Loss: 0.926.. \n",
      "Epoch: 3640/5000..  Training Loss: 0.918..  Test Loss: 0.926.. \n",
      "Epoch: 3641/5000..  Training Loss: 0.913..  Test Loss: 0.926.. \n",
      "Epoch: 3642/5000..  Training Loss: 0.893..  Test Loss: 0.926.. \n",
      "Epoch: 3643/5000..  Training Loss: 0.930..  Test Loss: 0.926.. \n",
      "Epoch: 3644/5000..  Training Loss: 0.929..  Test Loss: 0.926.. \n",
      "Epoch: 3645/5000..  Training Loss: 0.917..  Test Loss: 0.926.. \n",
      "Epoch: 3646/5000..  Training Loss: 0.913..  Test Loss: 0.926.. \n",
      "Epoch: 3647/5000..  Training Loss: 0.937..  Test Loss: 0.926.. \n",
      "Epoch: 3648/5000..  Training Loss: 0.888..  Test Loss: 0.926.. \n",
      "Epoch: 3649/5000..  Training Loss: 0.947..  Test Loss: 0.926.. \n",
      "Epoch: 3650/5000..  Training Loss: 0.937..  Test Loss: 0.926.. \n",
      "Epoch: 3651/5000..  Training Loss: 0.971..  Test Loss: 0.926.. \n",
      "Epoch: 3652/5000..  Training Loss: 0.963..  Test Loss: 0.926.. \n",
      "Epoch: 3653/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n",
      "Epoch: 3654/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3655/5000..  Training Loss: 0.925..  Test Loss: 0.926.. \n",
      "Epoch: 3656/5000..  Training Loss: 0.953..  Test Loss: 0.926.. \n",
      "Epoch: 3657/5000..  Training Loss: 0.954..  Test Loss: 0.926.. \n",
      "Epoch: 3658/5000..  Training Loss: 0.914..  Test Loss: 0.926.. \n",
      "Epoch: 3659/5000..  Training Loss: 0.968..  Test Loss: 0.926.. \n",
      "Epoch: 3660/5000..  Training Loss: 0.934..  Test Loss: 0.926.. \n",
      "Epoch: 3661/5000..  Training Loss: 0.960..  Test Loss: 0.926.. \n",
      "Epoch: 3662/5000..  Training Loss: 0.942..  Test Loss: 0.926.. \n",
      "Epoch: 3663/5000..  Training Loss: 0.898..  Test Loss: 0.926.. \n",
      "Epoch: 3664/5000..  Training Loss: 0.901..  Test Loss: 0.926.. \n",
      "Epoch: 3665/5000..  Training Loss: 0.940..  Test Loss: 0.926.. \n",
      "Epoch: 3666/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3667/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3668/5000..  Training Loss: 0.907..  Test Loss: 0.926.. \n",
      "Epoch: 3669/5000..  Training Loss: 0.924..  Test Loss: 0.926.. \n",
      "Epoch: 3670/5000..  Training Loss: 0.937..  Test Loss: 0.926.. \n",
      "Epoch: 3671/5000..  Training Loss: 0.927..  Test Loss: 0.926.. \n",
      "Epoch: 3672/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n",
      "Epoch: 3673/5000..  Training Loss: 0.944..  Test Loss: 0.926.. \n",
      "Epoch: 3674/5000..  Training Loss: 0.909..  Test Loss: 0.926.. \n",
      "Epoch: 3675/5000..  Training Loss: 0.903..  Test Loss: 0.926.. \n",
      "Epoch: 3676/5000..  Training Loss: 0.933..  Test Loss: 0.926.. \n",
      "Epoch: 3677/5000..  Training Loss: 0.968..  Test Loss: 0.926.. \n",
      "Epoch: 3678/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n",
      "Epoch: 3679/5000..  Training Loss: 0.936..  Test Loss: 0.926.. \n",
      "Epoch: 3680/5000..  Training Loss: 0.934..  Test Loss: 0.926.. \n",
      "Epoch: 3681/5000..  Training Loss: 0.941..  Test Loss: 0.926.. \n",
      "Epoch: 3682/5000..  Training Loss: 0.921..  Test Loss: 0.926.. \n",
      "Epoch: 3683/5000..  Training Loss: 0.911..  Test Loss: 0.926.. \n",
      "Epoch: 3684/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n",
      "Epoch: 3685/5000..  Training Loss: 0.922..  Test Loss: 0.926.. \n",
      "Epoch: 3686/5000..  Training Loss: 0.921..  Test Loss: 0.926.. \n",
      "Epoch: 3687/5000..  Training Loss: 0.915..  Test Loss: 0.926.. \n",
      "Epoch: 3688/5000..  Training Loss: 0.891..  Test Loss: 0.926.. \n",
      "Epoch: 3689/5000..  Training Loss: 0.898..  Test Loss: 0.926.. \n",
      "Epoch: 3690/5000..  Training Loss: 0.952..  Test Loss: 0.926.. \n",
      "Epoch: 3691/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n",
      "Epoch: 3692/5000..  Training Loss: 0.891..  Test Loss: 0.926.. \n",
      "Epoch: 3693/5000..  Training Loss: 0.920..  Test Loss: 0.926.. \n",
      "Epoch: 3694/5000..  Training Loss: 0.946..  Test Loss: 0.926.. \n",
      "Epoch: 3695/5000..  Training Loss: 0.972..  Test Loss: 0.926.. \n",
      "Epoch: 3696/5000..  Training Loss: 0.909..  Test Loss: 0.926.. \n",
      "Epoch: 3697/5000..  Training Loss: 0.905..  Test Loss: 0.926.. \n",
      "Epoch: 3698/5000..  Training Loss: 0.933..  Test Loss: 0.926.. \n",
      "Epoch: 3699/5000..  Training Loss: 0.911..  Test Loss: 0.926.. \n",
      "Epoch: 3700/5000..  Training Loss: 0.915..  Test Loss: 0.926.. \n",
      "Epoch: 3701/5000..  Training Loss: 0.936..  Test Loss: 0.926.. \n",
      "Epoch: 3702/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3703/5000..  Training Loss: 0.945..  Test Loss: 0.926.. \n",
      "Epoch: 3704/5000..  Training Loss: 0.954..  Test Loss: 0.926.. \n",
      "Epoch: 3705/5000..  Training Loss: 0.929..  Test Loss: 0.926.. \n",
      "Epoch: 3706/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3707/5000..  Training Loss: 0.934..  Test Loss: 0.926.. \n",
      "Epoch: 3708/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3709/5000..  Training Loss: 0.919..  Test Loss: 0.926.. \n",
      "Epoch: 3710/5000..  Training Loss: 0.942..  Test Loss: 0.926.. \n",
      "Epoch: 3711/5000..  Training Loss: 0.916..  Test Loss: 0.926.. \n",
      "Epoch: 3712/5000..  Training Loss: 0.939..  Test Loss: 0.926.. \n",
      "Epoch: 3713/5000..  Training Loss: 0.903..  Test Loss: 0.926.. \n",
      "Epoch: 3714/5000..  Training Loss: 0.938..  Test Loss: 0.926.. \n",
      "Epoch: 3715/5000..  Training Loss: 0.927..  Test Loss: 0.926.. \n",
      "Epoch: 3716/5000..  Training Loss: 0.952..  Test Loss: 0.926.. \n",
      "Epoch: 3717/5000..  Training Loss: 0.908..  Test Loss: 0.926.. \n",
      "Epoch: 3718/5000..  Training Loss: 0.975..  Test Loss: 0.926.. \n",
      "Epoch: 3719/5000..  Training Loss: 0.946..  Test Loss: 0.926.. \n",
      "Epoch: 3720/5000..  Training Loss: 0.938..  Test Loss: 0.926.. \n",
      "Epoch: 3721/5000..  Training Loss: 0.895..  Test Loss: 0.926.. \n",
      "Epoch: 3722/5000..  Training Loss: 0.918..  Test Loss: 0.926.. \n",
      "Epoch: 3723/5000..  Training Loss: 0.919..  Test Loss: 0.926.. \n",
      "Epoch: 3724/5000..  Training Loss: 0.942..  Test Loss: 0.926.. \n",
      "Epoch: 3725/5000..  Training Loss: 0.908..  Test Loss: 0.926.. \n",
      "Epoch: 3726/5000..  Training Loss: 0.923..  Test Loss: 0.926.. \n",
      "Epoch: 3727/5000..  Training Loss: 0.913..  Test Loss: 0.926.. \n",
      "Epoch: 3728/5000..  Training Loss: 0.912..  Test Loss: 0.926.. \n",
      "Epoch: 3729/5000..  Training Loss: 0.897..  Test Loss: 0.926.. \n",
      "Epoch: 3730/5000..  Training Loss: 0.933..  Test Loss: 0.926.. \n",
      "Epoch: 3731/5000..  Training Loss: 0.919..  Test Loss: 0.926.. \n",
      "Epoch: 3732/5000..  Training Loss: 0.930..  Test Loss: 0.926.. \n",
      "Epoch: 3733/5000..  Training Loss: 0.923..  Test Loss: 0.926.. \n",
      "Epoch: 3734/5000..  Training Loss: 0.938..  Test Loss: 0.926.. \n",
      "Epoch: 3735/5000..  Training Loss: 0.921..  Test Loss: 0.926.. \n",
      "Epoch: 3736/5000..  Training Loss: 0.913..  Test Loss: 0.926.. \n",
      "Epoch: 3737/5000..  Training Loss: 0.920..  Test Loss: 0.926.. \n",
      "Epoch: 3738/5000..  Training Loss: 0.917..  Test Loss: 0.926.. \n",
      "Epoch: 3739/5000..  Training Loss: 0.907..  Test Loss: 0.926.. \n",
      "Epoch: 3740/5000..  Training Loss: 0.925..  Test Loss: 0.926.. \n",
      "Epoch: 3741/5000..  Training Loss: 0.927..  Test Loss: 0.926.. \n",
      "Epoch: 3742/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n",
      "Epoch: 3743/5000..  Training Loss: 0.936..  Test Loss: 0.926.. \n",
      "Epoch: 3744/5000..  Training Loss: 0.906..  Test Loss: 0.926.. \n",
      "Epoch: 3745/5000..  Training Loss: 0.878..  Test Loss: 0.926.. \n",
      "Epoch: 3746/5000..  Training Loss: 0.914..  Test Loss: 0.926.. \n",
      "Epoch: 3747/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3748/5000..  Training Loss: 0.926..  Test Loss: 0.926.. \n",
      "Epoch: 3749/5000..  Training Loss: 0.947..  Test Loss: 0.926.. \n",
      "Epoch: 3750/5000..  Training Loss: 0.950..  Test Loss: 0.926.. \n",
      "Epoch: 3751/5000..  Training Loss: 0.934..  Test Loss: 0.926.. \n",
      "Epoch: 3752/5000..  Training Loss: 0.889..  Test Loss: 0.926.. \n",
      "Epoch: 3753/5000..  Training Loss: 0.952..  Test Loss: 0.926.. \n",
      "Epoch: 3754/5000..  Training Loss: 0.914..  Test Loss: 0.926.. \n",
      "Epoch: 3755/5000..  Training Loss: 0.934..  Test Loss: 0.926.. \n",
      "Epoch: 3756/5000..  Training Loss: 0.917..  Test Loss: 0.926.. \n",
      "Epoch: 3757/5000..  Training Loss: 0.931..  Test Loss: 0.926.. \n",
      "Epoch: 3758/5000..  Training Loss: 0.945..  Test Loss: 0.926.. \n",
      "Epoch: 3759/5000..  Training Loss: 0.936..  Test Loss: 0.926.. \n",
      "Epoch: 3760/5000..  Training Loss: 0.944..  Test Loss: 0.926.. \n",
      "Epoch: 3761/5000..  Training Loss: 0.930..  Test Loss: 0.926.. \n",
      "Epoch: 3762/5000..  Training Loss: 0.913..  Test Loss: 0.926.. \n",
      "Epoch: 3763/5000..  Training Loss: 0.938..  Test Loss: 0.926.. \n",
      "Epoch: 3764/5000..  Training Loss: 0.911..  Test Loss: 0.926.. \n",
      "Epoch: 3765/5000..  Training Loss: 0.904..  Test Loss: 0.926.. \n",
      "Epoch: 3766/5000..  Training Loss: 0.925..  Test Loss: 0.926.. \n",
      "Epoch: 3767/5000..  Training Loss: 0.912..  Test Loss: 0.926.. \n",
      "Epoch: 3768/5000..  Training Loss: 0.929..  Test Loss: 0.926.. \n",
      "Epoch: 3769/5000..  Training Loss: 0.914..  Test Loss: 0.926.. \n",
      "Epoch: 3770/5000..  Training Loss: 0.897..  Test Loss: 0.926.. \n",
      "Epoch: 3771/5000..  Training Loss: 0.920..  Test Loss: 0.926.. \n",
      "Epoch: 3772/5000..  Training Loss: 0.921..  Test Loss: 0.926.. \n",
      "Epoch: 3773/5000..  Training Loss: 0.953..  Test Loss: 0.926.. \n",
      "Epoch: 3774/5000..  Training Loss: 0.923..  Test Loss: 0.926.. \n",
      "Epoch: 3775/5000..  Training Loss: 0.929..  Test Loss: 0.926.. \n",
      "Epoch: 3776/5000..  Training Loss: 0.949..  Test Loss: 0.926.. \n",
      "Epoch: 3777/5000..  Training Loss: 0.953..  Test Loss: 0.926.. \n",
      "Epoch: 3778/5000..  Training Loss: 0.912..  Test Loss: 0.926.. \n",
      "Epoch: 3779/5000..  Training Loss: 0.937..  Test Loss: 0.925.. \n",
      "Epoch: 3780/5000..  Training Loss: 0.919..  Test Loss: 0.925.. \n",
      "Epoch: 3781/5000..  Training Loss: 0.908..  Test Loss: 0.925.. \n",
      "Epoch: 3782/5000..  Training Loss: 0.908..  Test Loss: 0.925.. \n",
      "Epoch: 3783/5000..  Training Loss: 0.901..  Test Loss: 0.925.. \n",
      "Epoch: 3784/5000..  Training Loss: 0.915..  Test Loss: 0.925.. \n",
      "Epoch: 3785/5000..  Training Loss: 0.939..  Test Loss: 0.925.. \n",
      "Epoch: 3786/5000..  Training Loss: 0.905..  Test Loss: 0.925.. \n",
      "Epoch: 3787/5000..  Training Loss: 0.901..  Test Loss: 0.925.. \n",
      "Epoch: 3788/5000..  Training Loss: 0.966..  Test Loss: 0.925.. \n",
      "Epoch: 3789/5000..  Training Loss: 0.918..  Test Loss: 0.925.. \n",
      "Epoch: 3790/5000..  Training Loss: 0.946..  Test Loss: 0.925.. \n",
      "Epoch: 3791/5000..  Training Loss: 0.935..  Test Loss: 0.925.. \n",
      "Epoch: 3792/5000..  Training Loss: 0.935..  Test Loss: 0.925.. \n",
      "Epoch: 3793/5000..  Training Loss: 0.923..  Test Loss: 0.925.. \n",
      "Epoch: 3794/5000..  Training Loss: 0.941..  Test Loss: 0.925.. \n",
      "Epoch: 3795/5000..  Training Loss: 0.927..  Test Loss: 0.925.. \n",
      "Epoch: 3796/5000..  Training Loss: 0.941..  Test Loss: 0.925.. \n",
      "Epoch: 3797/5000..  Training Loss: 0.911..  Test Loss: 0.925.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3798/5000..  Training Loss: 0.884..  Test Loss: 0.925.. \n",
      "Epoch: 3799/5000..  Training Loss: 0.894..  Test Loss: 0.925.. \n",
      "Epoch: 3800/5000..  Training Loss: 0.918..  Test Loss: 0.925.. \n",
      "Epoch: 3801/5000..  Training Loss: 0.895..  Test Loss: 0.925.. \n",
      "Epoch: 3802/5000..  Training Loss: 0.950..  Test Loss: 0.925.. \n",
      "Epoch: 3803/5000..  Training Loss: 0.908..  Test Loss: 0.925.. \n",
      "Epoch: 3804/5000..  Training Loss: 0.940..  Test Loss: 0.925.. \n",
      "Epoch: 3805/5000..  Training Loss: 0.912..  Test Loss: 0.925.. \n",
      "Epoch: 3806/5000..  Training Loss: 0.934..  Test Loss: 0.925.. \n",
      "Epoch: 3807/5000..  Training Loss: 0.927..  Test Loss: 0.925.. \n",
      "Epoch: 3808/5000..  Training Loss: 0.910..  Test Loss: 0.925.. \n",
      "Epoch: 3809/5000..  Training Loss: 0.937..  Test Loss: 0.925.. \n",
      "Epoch: 3810/5000..  Training Loss: 0.955..  Test Loss: 0.925.. \n",
      "Epoch: 3811/5000..  Training Loss: 0.951..  Test Loss: 0.925.. \n",
      "Epoch: 3812/5000..  Training Loss: 0.917..  Test Loss: 0.925.. \n",
      "Epoch: 3813/5000..  Training Loss: 0.933..  Test Loss: 0.925.. \n",
      "Epoch: 3814/5000..  Training Loss: 0.901..  Test Loss: 0.925.. \n",
      "Epoch: 3815/5000..  Training Loss: 0.936..  Test Loss: 0.925.. \n",
      "Epoch: 3816/5000..  Training Loss: 0.912..  Test Loss: 0.925.. \n",
      "Epoch: 3817/5000..  Training Loss: 0.933..  Test Loss: 0.925.. \n",
      "Epoch: 3818/5000..  Training Loss: 0.921..  Test Loss: 0.925.. \n",
      "Epoch: 3819/5000..  Training Loss: 0.891..  Test Loss: 0.925.. \n",
      "Epoch: 3820/5000..  Training Loss: 0.931..  Test Loss: 0.925.. \n",
      "Epoch: 3821/5000..  Training Loss: 0.922..  Test Loss: 0.925.. \n",
      "Epoch: 3822/5000..  Training Loss: 0.910..  Test Loss: 0.925.. \n",
      "Epoch: 3823/5000..  Training Loss: 0.917..  Test Loss: 0.925.. \n",
      "Epoch: 3824/5000..  Training Loss: 0.921..  Test Loss: 0.925.. \n",
      "Epoch: 3825/5000..  Training Loss: 0.932..  Test Loss: 0.925.. \n",
      "Epoch: 3826/5000..  Training Loss: 0.937..  Test Loss: 0.925.. \n",
      "Epoch: 3827/5000..  Training Loss: 0.927..  Test Loss: 0.925.. \n",
      "Epoch: 3828/5000..  Training Loss: 0.878..  Test Loss: 0.925.. \n",
      "Epoch: 3829/5000..  Training Loss: 0.940..  Test Loss: 0.925.. \n",
      "Epoch: 3830/5000..  Training Loss: 0.917..  Test Loss: 0.925.. \n",
      "Epoch: 3831/5000..  Training Loss: 0.940..  Test Loss: 0.925.. \n",
      "Epoch: 3832/5000..  Training Loss: 0.927..  Test Loss: 0.925.. \n",
      "Epoch: 3833/5000..  Training Loss: 0.919..  Test Loss: 0.925.. \n",
      "Epoch: 3834/5000..  Training Loss: 0.957..  Test Loss: 0.925.. \n",
      "Epoch: 3835/5000..  Training Loss: 0.942..  Test Loss: 0.925.. \n",
      "Epoch: 3836/5000..  Training Loss: 0.952..  Test Loss: 0.925.. \n",
      "Epoch: 3837/5000..  Training Loss: 0.964..  Test Loss: 0.925.. \n",
      "Epoch: 3838/5000..  Training Loss: 0.908..  Test Loss: 0.925.. \n",
      "Epoch: 3839/5000..  Training Loss: 0.918..  Test Loss: 0.925.. \n",
      "Epoch: 3840/5000..  Training Loss: 0.941..  Test Loss: 0.925.. \n",
      "Epoch: 3841/5000..  Training Loss: 0.916..  Test Loss: 0.925.. \n",
      "Epoch: 3842/5000..  Training Loss: 0.933..  Test Loss: 0.925.. \n",
      "Epoch: 3843/5000..  Training Loss: 0.953..  Test Loss: 0.925.. \n",
      "Epoch: 3844/5000..  Training Loss: 0.916..  Test Loss: 0.925.. \n",
      "Epoch: 3845/5000..  Training Loss: 0.917..  Test Loss: 0.925.. \n",
      "Epoch: 3846/5000..  Training Loss: 0.937..  Test Loss: 0.925.. \n",
      "Epoch: 3847/5000..  Training Loss: 0.879..  Test Loss: 0.925.. \n",
      "Epoch: 3848/5000..  Training Loss: 0.901..  Test Loss: 0.925.. \n",
      "Epoch: 3849/5000..  Training Loss: 0.950..  Test Loss: 0.925.. \n",
      "Epoch: 3850/5000..  Training Loss: 0.941..  Test Loss: 0.925.. \n",
      "Epoch: 3851/5000..  Training Loss: 0.910..  Test Loss: 0.925.. \n",
      "Epoch: 3852/5000..  Training Loss: 0.957..  Test Loss: 0.925.. \n",
      "Epoch: 3853/5000..  Training Loss: 0.934..  Test Loss: 0.925.. \n",
      "Epoch: 3854/5000..  Training Loss: 0.918..  Test Loss: 0.925.. \n",
      "Epoch: 3855/5000..  Training Loss: 0.905..  Test Loss: 0.925.. \n",
      "Epoch: 3856/5000..  Training Loss: 0.891..  Test Loss: 0.925.. \n",
      "Epoch: 3857/5000..  Training Loss: 0.946..  Test Loss: 0.925.. \n",
      "Epoch: 3858/5000..  Training Loss: 0.904..  Test Loss: 0.925.. \n",
      "Epoch: 3859/5000..  Training Loss: 0.962..  Test Loss: 0.925.. \n",
      "Epoch: 3860/5000..  Training Loss: 0.959..  Test Loss: 0.925.. \n",
      "Epoch: 3861/5000..  Training Loss: 0.924..  Test Loss: 0.925.. \n",
      "Epoch: 3862/5000..  Training Loss: 0.956..  Test Loss: 0.925.. \n",
      "Epoch: 3863/5000..  Training Loss: 0.948..  Test Loss: 0.925.. \n",
      "Epoch: 3864/5000..  Training Loss: 0.914..  Test Loss: 0.925.. \n",
      "Epoch: 3865/5000..  Training Loss: 0.918..  Test Loss: 0.925.. \n",
      "Epoch: 3866/5000..  Training Loss: 0.934..  Test Loss: 0.925.. \n",
      "Epoch: 3867/5000..  Training Loss: 0.924..  Test Loss: 0.925.. \n",
      "Epoch: 3868/5000..  Training Loss: 0.906..  Test Loss: 0.925.. \n",
      "Epoch: 3869/5000..  Training Loss: 0.926..  Test Loss: 0.925.. \n",
      "Epoch: 3870/5000..  Training Loss: 0.949..  Test Loss: 0.925.. \n",
      "Epoch: 3871/5000..  Training Loss: 0.945..  Test Loss: 0.925.. \n",
      "Epoch: 3872/5000..  Training Loss: 0.925..  Test Loss: 0.925.. \n",
      "Epoch: 3873/5000..  Training Loss: 0.925..  Test Loss: 0.925.. \n",
      "Epoch: 3874/5000..  Training Loss: 0.908..  Test Loss: 0.925.. \n",
      "Epoch: 3875/5000..  Training Loss: 0.952..  Test Loss: 0.925.. \n",
      "Epoch: 3876/5000..  Training Loss: 0.905..  Test Loss: 0.925.. \n",
      "Epoch: 3877/5000..  Training Loss: 0.910..  Test Loss: 0.925.. \n",
      "Epoch: 3878/5000..  Training Loss: 0.949..  Test Loss: 0.925.. \n",
      "Epoch: 3879/5000..  Training Loss: 0.941..  Test Loss: 0.925.. \n",
      "Epoch: 3880/5000..  Training Loss: 0.907..  Test Loss: 0.925.. \n",
      "Epoch: 3881/5000..  Training Loss: 0.923..  Test Loss: 0.925.. \n",
      "Epoch: 3882/5000..  Training Loss: 0.924..  Test Loss: 0.925.. \n",
      "Epoch: 3883/5000..  Training Loss: 0.939..  Test Loss: 0.925.. \n",
      "Epoch: 3884/5000..  Training Loss: 0.932..  Test Loss: 0.925.. \n",
      "Epoch: 3885/5000..  Training Loss: 0.907..  Test Loss: 0.925.. \n",
      "Epoch: 3886/5000..  Training Loss: 0.912..  Test Loss: 0.925.. \n",
      "Epoch: 3887/5000..  Training Loss: 0.935..  Test Loss: 0.925.. \n",
      "Epoch: 3888/5000..  Training Loss: 0.924..  Test Loss: 0.925.. \n",
      "Epoch: 3889/5000..  Training Loss: 0.933..  Test Loss: 0.925.. \n",
      "Epoch: 3890/5000..  Training Loss: 0.942..  Test Loss: 0.925.. \n",
      "Epoch: 3891/5000..  Training Loss: 0.926..  Test Loss: 0.925.. \n",
      "Epoch: 3892/5000..  Training Loss: 0.943..  Test Loss: 0.925.. \n",
      "Epoch: 3893/5000..  Training Loss: 0.945..  Test Loss: 0.925.. \n",
      "Epoch: 3894/5000..  Training Loss: 0.939..  Test Loss: 0.925.. \n",
      "Epoch: 3895/5000..  Training Loss: 0.933..  Test Loss: 0.925.. \n",
      "Epoch: 3896/5000..  Training Loss: 0.915..  Test Loss: 0.925.. \n",
      "Epoch: 3897/5000..  Training Loss: 0.910..  Test Loss: 0.925.. \n",
      "Epoch: 3898/5000..  Training Loss: 0.930..  Test Loss: 0.925.. \n",
      "Epoch: 3899/5000..  Training Loss: 0.905..  Test Loss: 0.925.. \n",
      "Epoch: 3900/5000..  Training Loss: 0.920..  Test Loss: 0.925.. \n",
      "Epoch: 3901/5000..  Training Loss: 0.926..  Test Loss: 0.925.. \n",
      "Epoch: 3902/5000..  Training Loss: 0.941..  Test Loss: 0.925.. \n",
      "Epoch: 3903/5000..  Training Loss: 0.897..  Test Loss: 0.925.. \n",
      "Epoch: 3904/5000..  Training Loss: 0.937..  Test Loss: 0.925.. \n",
      "Epoch: 3905/5000..  Training Loss: 0.924..  Test Loss: 0.925.. \n",
      "Epoch: 3906/5000..  Training Loss: 0.946..  Test Loss: 0.925.. \n",
      "Epoch: 3907/5000..  Training Loss: 0.903..  Test Loss: 0.925.. \n",
      "Epoch: 3908/5000..  Training Loss: 0.954..  Test Loss: 0.925.. \n",
      "Epoch: 3909/5000..  Training Loss: 0.924..  Test Loss: 0.925.. \n",
      "Epoch: 3910/5000..  Training Loss: 0.897..  Test Loss: 0.925.. \n",
      "Epoch: 3911/5000..  Training Loss: 0.918..  Test Loss: 0.925.. \n",
      "Epoch: 3912/5000..  Training Loss: 0.928..  Test Loss: 0.925.. \n",
      "Epoch: 3913/5000..  Training Loss: 0.901..  Test Loss: 0.925.. \n",
      "Epoch: 3914/5000..  Training Loss: 0.907..  Test Loss: 0.925.. \n",
      "Epoch: 3915/5000..  Training Loss: 0.912..  Test Loss: 0.925.. \n",
      "Epoch: 3916/5000..  Training Loss: 0.962..  Test Loss: 0.925.. \n",
      "Epoch: 3917/5000..  Training Loss: 0.920..  Test Loss: 0.925.. \n",
      "Epoch: 3918/5000..  Training Loss: 0.942..  Test Loss: 0.925.. \n",
      "Epoch: 3919/5000..  Training Loss: 0.972..  Test Loss: 0.925.. \n",
      "Epoch: 3920/5000..  Training Loss: 0.910..  Test Loss: 0.925.. \n",
      "Epoch: 3921/5000..  Training Loss: 0.926..  Test Loss: 0.925.. \n",
      "Epoch: 3922/5000..  Training Loss: 0.948..  Test Loss: 0.925.. \n",
      "Epoch: 3923/5000..  Training Loss: 0.897..  Test Loss: 0.925.. \n",
      "Epoch: 3924/5000..  Training Loss: 0.930..  Test Loss: 0.925.. \n",
      "Epoch: 3925/5000..  Training Loss: 0.912..  Test Loss: 0.925.. \n",
      "Epoch: 3926/5000..  Training Loss: 0.916..  Test Loss: 0.925.. \n",
      "Epoch: 3927/5000..  Training Loss: 0.942..  Test Loss: 0.925.. \n",
      "Epoch: 3928/5000..  Training Loss: 0.923..  Test Loss: 0.925.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3929/5000..  Training Loss: 0.897..  Test Loss: 0.925.. \n",
      "Epoch: 3930/5000..  Training Loss: 0.924..  Test Loss: 0.925.. \n",
      "Epoch: 3931/5000..  Training Loss: 0.930..  Test Loss: 0.925.. \n",
      "Epoch: 3932/5000..  Training Loss: 0.914..  Test Loss: 0.925.. \n",
      "Epoch: 3933/5000..  Training Loss: 0.898..  Test Loss: 0.925.. \n",
      "Epoch: 3934/5000..  Training Loss: 0.925..  Test Loss: 0.925.. \n",
      "Epoch: 3935/5000..  Training Loss: 0.921..  Test Loss: 0.925.. \n",
      "Epoch: 3936/5000..  Training Loss: 0.921..  Test Loss: 0.925.. \n",
      "Epoch: 3937/5000..  Training Loss: 0.893..  Test Loss: 0.925.. \n",
      "Epoch: 3938/5000..  Training Loss: 0.912..  Test Loss: 0.925.. \n",
      "Epoch: 3939/5000..  Training Loss: 0.941..  Test Loss: 0.924.. \n",
      "Epoch: 3940/5000..  Training Loss: 0.933..  Test Loss: 0.924.. \n",
      "Epoch: 3941/5000..  Training Loss: 0.912..  Test Loss: 0.924.. \n",
      "Epoch: 3942/5000..  Training Loss: 0.915..  Test Loss: 0.924.. \n",
      "Epoch: 3943/5000..  Training Loss: 0.894..  Test Loss: 0.924.. \n",
      "Epoch: 3944/5000..  Training Loss: 0.926..  Test Loss: 0.924.. \n",
      "Epoch: 3945/5000..  Training Loss: 0.895..  Test Loss: 0.924.. \n",
      "Epoch: 3946/5000..  Training Loss: 0.897..  Test Loss: 0.924.. \n",
      "Epoch: 3947/5000..  Training Loss: 0.927..  Test Loss: 0.924.. \n",
      "Epoch: 3948/5000..  Training Loss: 0.912..  Test Loss: 0.924.. \n",
      "Epoch: 3949/5000..  Training Loss: 0.907..  Test Loss: 0.924.. \n",
      "Epoch: 3950/5000..  Training Loss: 0.943..  Test Loss: 0.924.. \n",
      "Epoch: 3951/5000..  Training Loss: 0.914..  Test Loss: 0.924.. \n",
      "Epoch: 3952/5000..  Training Loss: 0.891..  Test Loss: 0.924.. \n",
      "Epoch: 3953/5000..  Training Loss: 0.955..  Test Loss: 0.924.. \n",
      "Epoch: 3954/5000..  Training Loss: 0.921..  Test Loss: 0.924.. \n",
      "Epoch: 3955/5000..  Training Loss: 0.907..  Test Loss: 0.924.. \n",
      "Epoch: 3956/5000..  Training Loss: 0.919..  Test Loss: 0.924.. \n",
      "Epoch: 3957/5000..  Training Loss: 0.955..  Test Loss: 0.924.. \n",
      "Epoch: 3958/5000..  Training Loss: 0.926..  Test Loss: 0.924.. \n",
      "Epoch: 3959/5000..  Training Loss: 0.920..  Test Loss: 0.924.. \n",
      "Epoch: 3960/5000..  Training Loss: 0.916..  Test Loss: 0.924.. \n",
      "Epoch: 3961/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 3962/5000..  Training Loss: 0.911..  Test Loss: 0.924.. \n",
      "Epoch: 3963/5000..  Training Loss: 0.950..  Test Loss: 0.924.. \n",
      "Epoch: 3964/5000..  Training Loss: 0.925..  Test Loss: 0.924.. \n",
      "Epoch: 3965/5000..  Training Loss: 0.915..  Test Loss: 0.924.. \n",
      "Epoch: 3966/5000..  Training Loss: 0.931..  Test Loss: 0.924.. \n",
      "Epoch: 3967/5000..  Training Loss: 0.944..  Test Loss: 0.924.. \n",
      "Epoch: 3968/5000..  Training Loss: 0.899..  Test Loss: 0.924.. \n",
      "Epoch: 3969/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 3970/5000..  Training Loss: 0.887..  Test Loss: 0.924.. \n",
      "Epoch: 3971/5000..  Training Loss: 0.915..  Test Loss: 0.924.. \n",
      "Epoch: 3972/5000..  Training Loss: 0.953..  Test Loss: 0.924.. \n",
      "Epoch: 3973/5000..  Training Loss: 0.917..  Test Loss: 0.924.. \n",
      "Epoch: 3974/5000..  Training Loss: 0.907..  Test Loss: 0.924.. \n",
      "Epoch: 3975/5000..  Training Loss: 0.890..  Test Loss: 0.924.. \n",
      "Epoch: 3976/5000..  Training Loss: 0.948..  Test Loss: 0.924.. \n",
      "Epoch: 3977/5000..  Training Loss: 0.902..  Test Loss: 0.924.. \n",
      "Epoch: 3978/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 3979/5000..  Training Loss: 0.907..  Test Loss: 0.924.. \n",
      "Epoch: 3980/5000..  Training Loss: 0.911..  Test Loss: 0.924.. \n",
      "Epoch: 3981/5000..  Training Loss: 0.902..  Test Loss: 0.924.. \n",
      "Epoch: 3982/5000..  Training Loss: 0.949..  Test Loss: 0.924.. \n",
      "Epoch: 3983/5000..  Training Loss: 0.944..  Test Loss: 0.924.. \n",
      "Epoch: 3984/5000..  Training Loss: 0.904..  Test Loss: 0.924.. \n",
      "Epoch: 3985/5000..  Training Loss: 0.934..  Test Loss: 0.924.. \n",
      "Epoch: 3986/5000..  Training Loss: 0.920..  Test Loss: 0.924.. \n",
      "Epoch: 3987/5000..  Training Loss: 0.952..  Test Loss: 0.924.. \n",
      "Epoch: 3988/5000..  Training Loss: 0.895..  Test Loss: 0.924.. \n",
      "Epoch: 3989/5000..  Training Loss: 0.923..  Test Loss: 0.924.. \n",
      "Epoch: 3990/5000..  Training Loss: 0.914..  Test Loss: 0.924.. \n",
      "Epoch: 3991/5000..  Training Loss: 0.931..  Test Loss: 0.924.. \n",
      "Epoch: 3992/5000..  Training Loss: 0.917..  Test Loss: 0.924.. \n",
      "Epoch: 3993/5000..  Training Loss: 0.914..  Test Loss: 0.924.. \n",
      "Epoch: 3994/5000..  Training Loss: 0.908..  Test Loss: 0.924.. \n",
      "Epoch: 3995/5000..  Training Loss: 0.928..  Test Loss: 0.924.. \n",
      "Epoch: 3996/5000..  Training Loss: 0.938..  Test Loss: 0.924.. \n",
      "Epoch: 3997/5000..  Training Loss: 0.939..  Test Loss: 0.924.. \n",
      "Epoch: 3998/5000..  Training Loss: 0.919..  Test Loss: 0.924.. \n",
      "Epoch: 3999/5000..  Training Loss: 0.909..  Test Loss: 0.924.. \n",
      "Epoch: 4000/5000..  Training Loss: 0.949..  Test Loss: 0.924.. \n",
      "Epoch: 4001/5000..  Training Loss: 0.911..  Test Loss: 0.924.. \n",
      "Epoch: 4002/5000..  Training Loss: 0.903..  Test Loss: 0.924.. \n",
      "Epoch: 4003/5000..  Training Loss: 0.948..  Test Loss: 0.924.. \n",
      "Epoch: 4004/5000..  Training Loss: 0.924..  Test Loss: 0.924.. \n",
      "Epoch: 4005/5000..  Training Loss: 0.951..  Test Loss: 0.924.. \n",
      "Epoch: 4006/5000..  Training Loss: 0.923..  Test Loss: 0.924.. \n",
      "Epoch: 4007/5000..  Training Loss: 0.916..  Test Loss: 0.924.. \n",
      "Epoch: 4008/5000..  Training Loss: 0.919..  Test Loss: 0.924.. \n",
      "Epoch: 4009/5000..  Training Loss: 0.915..  Test Loss: 0.924.. \n",
      "Epoch: 4010/5000..  Training Loss: 0.896..  Test Loss: 0.924.. \n",
      "Epoch: 4011/5000..  Training Loss: 0.908..  Test Loss: 0.924.. \n",
      "Epoch: 4012/5000..  Training Loss: 0.910..  Test Loss: 0.924.. \n",
      "Epoch: 4013/5000..  Training Loss: 0.947..  Test Loss: 0.924.. \n",
      "Epoch: 4014/5000..  Training Loss: 0.924..  Test Loss: 0.924.. \n",
      "Epoch: 4015/5000..  Training Loss: 0.946..  Test Loss: 0.924.. \n",
      "Epoch: 4016/5000..  Training Loss: 0.928..  Test Loss: 0.924.. \n",
      "Epoch: 4017/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 4018/5000..  Training Loss: 0.927..  Test Loss: 0.924.. \n",
      "Epoch: 4019/5000..  Training Loss: 0.920..  Test Loss: 0.924.. \n",
      "Epoch: 4020/5000..  Training Loss: 0.925..  Test Loss: 0.924.. \n",
      "Epoch: 4021/5000..  Training Loss: 0.920..  Test Loss: 0.924.. \n",
      "Epoch: 4022/5000..  Training Loss: 0.931..  Test Loss: 0.924.. \n",
      "Epoch: 4023/5000..  Training Loss: 0.934..  Test Loss: 0.924.. \n",
      "Epoch: 4024/5000..  Training Loss: 0.907..  Test Loss: 0.924.. \n",
      "Epoch: 4025/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 4026/5000..  Training Loss: 0.914..  Test Loss: 0.924.. \n",
      "Epoch: 4027/5000..  Training Loss: 0.925..  Test Loss: 0.924.. \n",
      "Epoch: 4028/5000..  Training Loss: 0.928..  Test Loss: 0.924.. \n",
      "Epoch: 4029/5000..  Training Loss: 0.899..  Test Loss: 0.924.. \n",
      "Epoch: 4030/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 4031/5000..  Training Loss: 0.896..  Test Loss: 0.924.. \n",
      "Epoch: 4032/5000..  Training Loss: 0.912..  Test Loss: 0.924.. \n",
      "Epoch: 4033/5000..  Training Loss: 0.973..  Test Loss: 0.924.. \n",
      "Epoch: 4034/5000..  Training Loss: 0.942..  Test Loss: 0.924.. \n",
      "Epoch: 4035/5000..  Training Loss: 0.942..  Test Loss: 0.924.. \n",
      "Epoch: 4036/5000..  Training Loss: 0.915..  Test Loss: 0.924.. \n",
      "Epoch: 4037/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 4038/5000..  Training Loss: 0.932..  Test Loss: 0.924.. \n",
      "Epoch: 4039/5000..  Training Loss: 0.934..  Test Loss: 0.924.. \n",
      "Epoch: 4040/5000..  Training Loss: 0.932..  Test Loss: 0.924.. \n",
      "Epoch: 4041/5000..  Training Loss: 0.929..  Test Loss: 0.924.. \n",
      "Epoch: 4042/5000..  Training Loss: 0.952..  Test Loss: 0.924.. \n",
      "Epoch: 4043/5000..  Training Loss: 0.946..  Test Loss: 0.924.. \n",
      "Epoch: 4044/5000..  Training Loss: 0.920..  Test Loss: 0.924.. \n",
      "Epoch: 4045/5000..  Training Loss: 0.920..  Test Loss: 0.924.. \n",
      "Epoch: 4046/5000..  Training Loss: 0.893..  Test Loss: 0.924.. \n",
      "Epoch: 4047/5000..  Training Loss: 0.932..  Test Loss: 0.924.. \n",
      "Epoch: 4048/5000..  Training Loss: 0.884..  Test Loss: 0.924.. \n",
      "Epoch: 4049/5000..  Training Loss: 0.928..  Test Loss: 0.924.. \n",
      "Epoch: 4050/5000..  Training Loss: 0.919..  Test Loss: 0.924.. \n",
      "Epoch: 4051/5000..  Training Loss: 0.887..  Test Loss: 0.924.. \n",
      "Epoch: 4052/5000..  Training Loss: 0.931..  Test Loss: 0.924.. \n",
      "Epoch: 4053/5000..  Training Loss: 0.909..  Test Loss: 0.924.. \n",
      "Epoch: 4054/5000..  Training Loss: 0.917..  Test Loss: 0.924.. \n",
      "Epoch: 4055/5000..  Training Loss: 0.944..  Test Loss: 0.924.. \n",
      "Epoch: 4056/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 4057/5000..  Training Loss: 0.917..  Test Loss: 0.924.. \n",
      "Epoch: 4058/5000..  Training Loss: 0.913..  Test Loss: 0.924.. \n",
      "Epoch: 4059/5000..  Training Loss: 0.951..  Test Loss: 0.924.. \n",
      "Epoch: 4060/5000..  Training Loss: 0.909..  Test Loss: 0.924.. \n",
      "Epoch: 4061/5000..  Training Loss: 0.942..  Test Loss: 0.924.. \n",
      "Epoch: 4062/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4063/5000..  Training Loss: 0.967..  Test Loss: 0.924.. \n",
      "Epoch: 4064/5000..  Training Loss: 0.909..  Test Loss: 0.924.. \n",
      "Epoch: 4065/5000..  Training Loss: 0.938..  Test Loss: 0.924.. \n",
      "Epoch: 4066/5000..  Training Loss: 0.947..  Test Loss: 0.924.. \n",
      "Epoch: 4067/5000..  Training Loss: 0.915..  Test Loss: 0.924.. \n",
      "Epoch: 4068/5000..  Training Loss: 0.941..  Test Loss: 0.924.. \n",
      "Epoch: 4069/5000..  Training Loss: 0.935..  Test Loss: 0.924.. \n",
      "Epoch: 4070/5000..  Training Loss: 0.910..  Test Loss: 0.924.. \n",
      "Epoch: 4071/5000..  Training Loss: 0.918..  Test Loss: 0.924.. \n",
      "Epoch: 4072/5000..  Training Loss: 0.909..  Test Loss: 0.924.. \n",
      "Epoch: 4073/5000..  Training Loss: 0.931..  Test Loss: 0.924.. \n",
      "Epoch: 4074/5000..  Training Loss: 0.910..  Test Loss: 0.924.. \n",
      "Epoch: 4075/5000..  Training Loss: 0.917..  Test Loss: 0.924.. \n",
      "Epoch: 4076/5000..  Training Loss: 0.922..  Test Loss: 0.924.. \n",
      "Epoch: 4077/5000..  Training Loss: 0.924..  Test Loss: 0.924.. \n",
      "Epoch: 4078/5000..  Training Loss: 0.911..  Test Loss: 0.924.. \n",
      "Epoch: 4079/5000..  Training Loss: 0.917..  Test Loss: 0.924.. \n",
      "Epoch: 4080/5000..  Training Loss: 0.893..  Test Loss: 0.924.. \n",
      "Epoch: 4081/5000..  Training Loss: 0.932..  Test Loss: 0.924.. \n",
      "Epoch: 4082/5000..  Training Loss: 0.940..  Test Loss: 0.924.. \n",
      "Epoch: 4083/5000..  Training Loss: 0.948..  Test Loss: 0.924.. \n",
      "Epoch: 4084/5000..  Training Loss: 0.941..  Test Loss: 0.924.. \n",
      "Epoch: 4085/5000..  Training Loss: 0.950..  Test Loss: 0.924.. \n",
      "Epoch: 4086/5000..  Training Loss: 0.885..  Test Loss: 0.924.. \n",
      "Epoch: 4087/5000..  Training Loss: 0.923..  Test Loss: 0.924.. \n",
      "Epoch: 4088/5000..  Training Loss: 0.913..  Test Loss: 0.924.. \n",
      "Epoch: 4089/5000..  Training Loss: 0.912..  Test Loss: 0.924.. \n",
      "Epoch: 4090/5000..  Training Loss: 0.935..  Test Loss: 0.924.. \n",
      "Epoch: 4091/5000..  Training Loss: 0.919..  Test Loss: 0.924.. \n",
      "Epoch: 4092/5000..  Training Loss: 0.937..  Test Loss: 0.924.. \n",
      "Epoch: 4093/5000..  Training Loss: 0.909..  Test Loss: 0.924.. \n",
      "Epoch: 4094/5000..  Training Loss: 0.929..  Test Loss: 0.924.. \n",
      "Epoch: 4095/5000..  Training Loss: 0.894..  Test Loss: 0.924.. \n",
      "Epoch: 4096/5000..  Training Loss: 0.908..  Test Loss: 0.924.. \n",
      "Epoch: 4097/5000..  Training Loss: 0.908..  Test Loss: 0.923.. \n",
      "Epoch: 4098/5000..  Training Loss: 0.937..  Test Loss: 0.923.. \n",
      "Epoch: 4099/5000..  Training Loss: 0.927..  Test Loss: 0.923.. \n",
      "Epoch: 4100/5000..  Training Loss: 0.913..  Test Loss: 0.923.. \n",
      "Epoch: 4101/5000..  Training Loss: 0.914..  Test Loss: 0.923.. \n",
      "Epoch: 4102/5000..  Training Loss: 0.921..  Test Loss: 0.923.. \n",
      "Epoch: 4103/5000..  Training Loss: 0.941..  Test Loss: 0.923.. \n",
      "Epoch: 4104/5000..  Training Loss: 0.910..  Test Loss: 0.923.. \n",
      "Epoch: 4105/5000..  Training Loss: 0.949..  Test Loss: 0.923.. \n",
      "Epoch: 4106/5000..  Training Loss: 0.924..  Test Loss: 0.923.. \n",
      "Epoch: 4107/5000..  Training Loss: 0.939..  Test Loss: 0.923.. \n",
      "Epoch: 4108/5000..  Training Loss: 0.915..  Test Loss: 0.923.. \n",
      "Epoch: 4109/5000..  Training Loss: 0.921..  Test Loss: 0.923.. \n",
      "Epoch: 4110/5000..  Training Loss: 0.932..  Test Loss: 0.923.. \n",
      "Epoch: 4111/5000..  Training Loss: 0.929..  Test Loss: 0.923.. \n",
      "Epoch: 4112/5000..  Training Loss: 0.905..  Test Loss: 0.923.. \n",
      "Epoch: 4113/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4114/5000..  Training Loss: 0.903..  Test Loss: 0.923.. \n",
      "Epoch: 4115/5000..  Training Loss: 0.928..  Test Loss: 0.923.. \n",
      "Epoch: 4116/5000..  Training Loss: 0.944..  Test Loss: 0.923.. \n",
      "Epoch: 4117/5000..  Training Loss: 0.918..  Test Loss: 0.923.. \n",
      "Epoch: 4118/5000..  Training Loss: 0.920..  Test Loss: 0.923.. \n",
      "Epoch: 4119/5000..  Training Loss: 0.928..  Test Loss: 0.923.. \n",
      "Epoch: 4120/5000..  Training Loss: 0.924..  Test Loss: 0.923.. \n",
      "Epoch: 4121/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4122/5000..  Training Loss: 0.926..  Test Loss: 0.923.. \n",
      "Epoch: 4123/5000..  Training Loss: 0.920..  Test Loss: 0.923.. \n",
      "Epoch: 4124/5000..  Training Loss: 0.930..  Test Loss: 0.923.. \n",
      "Epoch: 4125/5000..  Training Loss: 0.917..  Test Loss: 0.923.. \n",
      "Epoch: 4126/5000..  Training Loss: 0.925..  Test Loss: 0.923.. \n",
      "Epoch: 4127/5000..  Training Loss: 0.928..  Test Loss: 0.923.. \n",
      "Epoch: 4128/5000..  Training Loss: 0.914..  Test Loss: 0.923.. \n",
      "Epoch: 4129/5000..  Training Loss: 0.881..  Test Loss: 0.923.. \n",
      "Epoch: 4130/5000..  Training Loss: 0.904..  Test Loss: 0.923.. \n",
      "Epoch: 4131/5000..  Training Loss: 0.931..  Test Loss: 0.923.. \n",
      "Epoch: 4132/5000..  Training Loss: 0.937..  Test Loss: 0.923.. \n",
      "Epoch: 4133/5000..  Training Loss: 0.901..  Test Loss: 0.923.. \n",
      "Epoch: 4134/5000..  Training Loss: 0.929..  Test Loss: 0.923.. \n",
      "Epoch: 4135/5000..  Training Loss: 0.925..  Test Loss: 0.923.. \n",
      "Epoch: 4136/5000..  Training Loss: 0.916..  Test Loss: 0.923.. \n",
      "Epoch: 4137/5000..  Training Loss: 0.957..  Test Loss: 0.923.. \n",
      "Epoch: 4138/5000..  Training Loss: 0.931..  Test Loss: 0.923.. \n",
      "Epoch: 4139/5000..  Training Loss: 0.903..  Test Loss: 0.923.. \n",
      "Epoch: 4140/5000..  Training Loss: 0.868..  Test Loss: 0.923.. \n",
      "Epoch: 4141/5000..  Training Loss: 0.940..  Test Loss: 0.923.. \n",
      "Epoch: 4142/5000..  Training Loss: 0.912..  Test Loss: 0.923.. \n",
      "Epoch: 4143/5000..  Training Loss: 0.917..  Test Loss: 0.923.. \n",
      "Epoch: 4144/5000..  Training Loss: 0.941..  Test Loss: 0.923.. \n",
      "Epoch: 4145/5000..  Training Loss: 0.927..  Test Loss: 0.923.. \n",
      "Epoch: 4146/5000..  Training Loss: 0.928..  Test Loss: 0.923.. \n",
      "Epoch: 4147/5000..  Training Loss: 0.926..  Test Loss: 0.923.. \n",
      "Epoch: 4148/5000..  Training Loss: 0.914..  Test Loss: 0.923.. \n",
      "Epoch: 4149/5000..  Training Loss: 0.949..  Test Loss: 0.923.. \n",
      "Epoch: 4150/5000..  Training Loss: 0.891..  Test Loss: 0.923.. \n",
      "Epoch: 4151/5000..  Training Loss: 0.933..  Test Loss: 0.923.. \n",
      "Epoch: 4152/5000..  Training Loss: 0.931..  Test Loss: 0.923.. \n",
      "Epoch: 4153/5000..  Training Loss: 0.914..  Test Loss: 0.923.. \n",
      "Epoch: 4154/5000..  Training Loss: 0.949..  Test Loss: 0.923.. \n",
      "Epoch: 4155/5000..  Training Loss: 0.891..  Test Loss: 0.923.. \n",
      "Epoch: 4156/5000..  Training Loss: 0.931..  Test Loss: 0.923.. \n",
      "Epoch: 4157/5000..  Training Loss: 0.941..  Test Loss: 0.923.. \n",
      "Epoch: 4158/5000..  Training Loss: 0.928..  Test Loss: 0.923.. \n",
      "Epoch: 4159/5000..  Training Loss: 0.923..  Test Loss: 0.923.. \n",
      "Epoch: 4160/5000..  Training Loss: 0.927..  Test Loss: 0.923.. \n",
      "Epoch: 4161/5000..  Training Loss: 0.917..  Test Loss: 0.923.. \n",
      "Epoch: 4162/5000..  Training Loss: 0.911..  Test Loss: 0.923.. \n",
      "Epoch: 4163/5000..  Training Loss: 0.904..  Test Loss: 0.923.. \n",
      "Epoch: 4164/5000..  Training Loss: 0.919..  Test Loss: 0.923.. \n",
      "Epoch: 4165/5000..  Training Loss: 0.933..  Test Loss: 0.923.. \n",
      "Epoch: 4166/5000..  Training Loss: 0.922..  Test Loss: 0.923.. \n",
      "Epoch: 4167/5000..  Training Loss: 0.897..  Test Loss: 0.923.. \n",
      "Epoch: 4168/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4169/5000..  Training Loss: 0.899..  Test Loss: 0.923.. \n",
      "Epoch: 4170/5000..  Training Loss: 0.936..  Test Loss: 0.923.. \n",
      "Epoch: 4171/5000..  Training Loss: 0.904..  Test Loss: 0.923.. \n",
      "Epoch: 4172/5000..  Training Loss: 0.918..  Test Loss: 0.923.. \n",
      "Epoch: 4173/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4174/5000..  Training Loss: 0.930..  Test Loss: 0.923.. \n",
      "Epoch: 4175/5000..  Training Loss: 0.891..  Test Loss: 0.923.. \n",
      "Epoch: 4176/5000..  Training Loss: 0.918..  Test Loss: 0.923.. \n",
      "Epoch: 4177/5000..  Training Loss: 0.920..  Test Loss: 0.923.. \n",
      "Epoch: 4178/5000..  Training Loss: 0.934..  Test Loss: 0.923.. \n",
      "Epoch: 4179/5000..  Training Loss: 0.921..  Test Loss: 0.923.. \n",
      "Epoch: 4180/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4181/5000..  Training Loss: 0.933..  Test Loss: 0.923.. \n",
      "Epoch: 4182/5000..  Training Loss: 0.916..  Test Loss: 0.923.. \n",
      "Epoch: 4183/5000..  Training Loss: 0.905..  Test Loss: 0.923.. \n",
      "Epoch: 4184/5000..  Training Loss: 0.919..  Test Loss: 0.923.. \n",
      "Epoch: 4185/5000..  Training Loss: 0.942..  Test Loss: 0.923.. \n",
      "Epoch: 4186/5000..  Training Loss: 0.911..  Test Loss: 0.923.. \n",
      "Epoch: 4187/5000..  Training Loss: 0.912..  Test Loss: 0.923.. \n",
      "Epoch: 4188/5000..  Training Loss: 0.919..  Test Loss: 0.923.. \n",
      "Epoch: 4189/5000..  Training Loss: 0.936..  Test Loss: 0.923.. \n",
      "Epoch: 4190/5000..  Training Loss: 0.907..  Test Loss: 0.923.. \n",
      "Epoch: 4191/5000..  Training Loss: 0.934..  Test Loss: 0.923.. \n",
      "Epoch: 4192/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4193/5000..  Training Loss: 0.941..  Test Loss: 0.923.. \n",
      "Epoch: 4194/5000..  Training Loss: 0.955..  Test Loss: 0.923.. \n",
      "Epoch: 4195/5000..  Training Loss: 0.971..  Test Loss: 0.923.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4196/5000..  Training Loss: 0.956..  Test Loss: 0.923.. \n",
      "Epoch: 4197/5000..  Training Loss: 0.894..  Test Loss: 0.923.. \n",
      "Epoch: 4198/5000..  Training Loss: 0.911..  Test Loss: 0.923.. \n",
      "Epoch: 4199/5000..  Training Loss: 0.953..  Test Loss: 0.923.. \n",
      "Epoch: 4200/5000..  Training Loss: 0.946..  Test Loss: 0.923.. \n",
      "Epoch: 4201/5000..  Training Loss: 0.949..  Test Loss: 0.923.. \n",
      "Epoch: 4202/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4203/5000..  Training Loss: 0.940..  Test Loss: 0.923.. \n",
      "Epoch: 4204/5000..  Training Loss: 0.941..  Test Loss: 0.923.. \n",
      "Epoch: 4205/5000..  Training Loss: 0.938..  Test Loss: 0.923.. \n",
      "Epoch: 4206/5000..  Training Loss: 0.926..  Test Loss: 0.923.. \n",
      "Epoch: 4207/5000..  Training Loss: 0.908..  Test Loss: 0.923.. \n",
      "Epoch: 4208/5000..  Training Loss: 0.911..  Test Loss: 0.923.. \n",
      "Epoch: 4209/5000..  Training Loss: 0.907..  Test Loss: 0.923.. \n",
      "Epoch: 4210/5000..  Training Loss: 0.914..  Test Loss: 0.923.. \n",
      "Epoch: 4211/5000..  Training Loss: 0.916..  Test Loss: 0.923.. \n",
      "Epoch: 4212/5000..  Training Loss: 0.911..  Test Loss: 0.923.. \n",
      "Epoch: 4213/5000..  Training Loss: 0.914..  Test Loss: 0.923.. \n",
      "Epoch: 4214/5000..  Training Loss: 0.944..  Test Loss: 0.923.. \n",
      "Epoch: 4215/5000..  Training Loss: 0.920..  Test Loss: 0.923.. \n",
      "Epoch: 4216/5000..  Training Loss: 0.937..  Test Loss: 0.923.. \n",
      "Epoch: 4217/5000..  Training Loss: 0.930..  Test Loss: 0.923.. \n",
      "Epoch: 4218/5000..  Training Loss: 0.930..  Test Loss: 0.923.. \n",
      "Epoch: 4219/5000..  Training Loss: 0.949..  Test Loss: 0.923.. \n",
      "Epoch: 4220/5000..  Training Loss: 0.910..  Test Loss: 0.923.. \n",
      "Epoch: 4221/5000..  Training Loss: 0.910..  Test Loss: 0.923.. \n",
      "Epoch: 4222/5000..  Training Loss: 0.891..  Test Loss: 0.923.. \n",
      "Epoch: 4223/5000..  Training Loss: 0.907..  Test Loss: 0.923.. \n",
      "Epoch: 4224/5000..  Training Loss: 0.919..  Test Loss: 0.923.. \n",
      "Epoch: 4225/5000..  Training Loss: 0.959..  Test Loss: 0.923.. \n",
      "Epoch: 4226/5000..  Training Loss: 0.925..  Test Loss: 0.923.. \n",
      "Epoch: 4227/5000..  Training Loss: 0.940..  Test Loss: 0.923.. \n",
      "Epoch: 4228/5000..  Training Loss: 0.922..  Test Loss: 0.923.. \n",
      "Epoch: 4229/5000..  Training Loss: 0.887..  Test Loss: 0.923.. \n",
      "Epoch: 4230/5000..  Training Loss: 0.901..  Test Loss: 0.923.. \n",
      "Epoch: 4231/5000..  Training Loss: 0.940..  Test Loss: 0.923.. \n",
      "Epoch: 4232/5000..  Training Loss: 0.916..  Test Loss: 0.923.. \n",
      "Epoch: 4233/5000..  Training Loss: 0.927..  Test Loss: 0.923.. \n",
      "Epoch: 4234/5000..  Training Loss: 0.961..  Test Loss: 0.923.. \n",
      "Epoch: 4235/5000..  Training Loss: 0.930..  Test Loss: 0.923.. \n",
      "Epoch: 4236/5000..  Training Loss: 0.905..  Test Loss: 0.923.. \n",
      "Epoch: 4237/5000..  Training Loss: 0.931..  Test Loss: 0.923.. \n",
      "Epoch: 4238/5000..  Training Loss: 0.940..  Test Loss: 0.923.. \n",
      "Epoch: 4239/5000..  Training Loss: 0.917..  Test Loss: 0.923.. \n",
      "Epoch: 4240/5000..  Training Loss: 0.923..  Test Loss: 0.922.. \n",
      "Epoch: 4241/5000..  Training Loss: 0.934..  Test Loss: 0.922.. \n",
      "Epoch: 4242/5000..  Training Loss: 0.904..  Test Loss: 0.922.. \n",
      "Epoch: 4243/5000..  Training Loss: 0.920..  Test Loss: 0.922.. \n",
      "Epoch: 4244/5000..  Training Loss: 0.944..  Test Loss: 0.922.. \n",
      "Epoch: 4245/5000..  Training Loss: 0.961..  Test Loss: 0.922.. \n",
      "Epoch: 4246/5000..  Training Loss: 0.942..  Test Loss: 0.922.. \n",
      "Epoch: 4247/5000..  Training Loss: 0.911..  Test Loss: 0.922.. \n",
      "Epoch: 4248/5000..  Training Loss: 0.933..  Test Loss: 0.922.. \n",
      "Epoch: 4249/5000..  Training Loss: 0.909..  Test Loss: 0.922.. \n",
      "Epoch: 4250/5000..  Training Loss: 0.929..  Test Loss: 0.922.. \n",
      "Epoch: 4251/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4252/5000..  Training Loss: 0.938..  Test Loss: 0.922.. \n",
      "Epoch: 4253/5000..  Training Loss: 0.940..  Test Loss: 0.922.. \n",
      "Epoch: 4254/5000..  Training Loss: 0.885..  Test Loss: 0.922.. \n",
      "Epoch: 4255/5000..  Training Loss: 0.905..  Test Loss: 0.922.. \n",
      "Epoch: 4256/5000..  Training Loss: 0.951..  Test Loss: 0.922.. \n",
      "Epoch: 4257/5000..  Training Loss: 0.931..  Test Loss: 0.922.. \n",
      "Epoch: 4258/5000..  Training Loss: 0.940..  Test Loss: 0.922.. \n",
      "Epoch: 4259/5000..  Training Loss: 0.922..  Test Loss: 0.922.. \n",
      "Epoch: 4260/5000..  Training Loss: 0.903..  Test Loss: 0.922.. \n",
      "Epoch: 4261/5000..  Training Loss: 0.917..  Test Loss: 0.922.. \n",
      "Epoch: 4262/5000..  Training Loss: 0.933..  Test Loss: 0.922.. \n",
      "Epoch: 4263/5000..  Training Loss: 0.897..  Test Loss: 0.922.. \n",
      "Epoch: 4264/5000..  Training Loss: 0.947..  Test Loss: 0.922.. \n",
      "Epoch: 4265/5000..  Training Loss: 0.942..  Test Loss: 0.922.. \n",
      "Epoch: 4266/5000..  Training Loss: 0.910..  Test Loss: 0.922.. \n",
      "Epoch: 4267/5000..  Training Loss: 0.936..  Test Loss: 0.922.. \n",
      "Epoch: 4268/5000..  Training Loss: 0.928..  Test Loss: 0.922.. \n",
      "Epoch: 4269/5000..  Training Loss: 0.921..  Test Loss: 0.922.. \n",
      "Epoch: 4270/5000..  Training Loss: 0.913..  Test Loss: 0.922.. \n",
      "Epoch: 4271/5000..  Training Loss: 0.923..  Test Loss: 0.922.. \n",
      "Epoch: 4272/5000..  Training Loss: 0.953..  Test Loss: 0.922.. \n",
      "Epoch: 4273/5000..  Training Loss: 0.930..  Test Loss: 0.922.. \n",
      "Epoch: 4274/5000..  Training Loss: 0.914..  Test Loss: 0.922.. \n",
      "Epoch: 4275/5000..  Training Loss: 0.918..  Test Loss: 0.922.. \n",
      "Epoch: 4276/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4277/5000..  Training Loss: 0.914..  Test Loss: 0.922.. \n",
      "Epoch: 4278/5000..  Training Loss: 0.950..  Test Loss: 0.922.. \n",
      "Epoch: 4279/5000..  Training Loss: 0.932..  Test Loss: 0.922.. \n",
      "Epoch: 4280/5000..  Training Loss: 0.939..  Test Loss: 0.922.. \n",
      "Epoch: 4281/5000..  Training Loss: 0.930..  Test Loss: 0.922.. \n",
      "Epoch: 4282/5000..  Training Loss: 0.918..  Test Loss: 0.922.. \n",
      "Epoch: 4283/5000..  Training Loss: 0.942..  Test Loss: 0.922.. \n",
      "Epoch: 4284/5000..  Training Loss: 0.925..  Test Loss: 0.922.. \n",
      "Epoch: 4285/5000..  Training Loss: 0.942..  Test Loss: 0.922.. \n",
      "Epoch: 4286/5000..  Training Loss: 0.939..  Test Loss: 0.922.. \n",
      "Epoch: 4287/5000..  Training Loss: 0.933..  Test Loss: 0.922.. \n",
      "Epoch: 4288/5000..  Training Loss: 0.913..  Test Loss: 0.922.. \n",
      "Epoch: 4289/5000..  Training Loss: 0.935..  Test Loss: 0.922.. \n",
      "Epoch: 4290/5000..  Training Loss: 0.946..  Test Loss: 0.922.. \n",
      "Epoch: 4291/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4292/5000..  Training Loss: 0.917..  Test Loss: 0.922.. \n",
      "Epoch: 4293/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4294/5000..  Training Loss: 0.952..  Test Loss: 0.922.. \n",
      "Epoch: 4295/5000..  Training Loss: 0.897..  Test Loss: 0.922.. \n",
      "Epoch: 4296/5000..  Training Loss: 0.924..  Test Loss: 0.922.. \n",
      "Epoch: 4297/5000..  Training Loss: 0.930..  Test Loss: 0.922.. \n",
      "Epoch: 4298/5000..  Training Loss: 0.917..  Test Loss: 0.922.. \n",
      "Epoch: 4299/5000..  Training Loss: 0.956..  Test Loss: 0.922.. \n",
      "Epoch: 4300/5000..  Training Loss: 0.928..  Test Loss: 0.922.. \n",
      "Epoch: 4301/5000..  Training Loss: 0.906..  Test Loss: 0.922.. \n",
      "Epoch: 4302/5000..  Training Loss: 0.889..  Test Loss: 0.922.. \n",
      "Epoch: 4303/5000..  Training Loss: 0.909..  Test Loss: 0.922.. \n",
      "Epoch: 4304/5000..  Training Loss: 0.877..  Test Loss: 0.922.. \n",
      "Epoch: 4305/5000..  Training Loss: 0.955..  Test Loss: 0.922.. \n",
      "Epoch: 4306/5000..  Training Loss: 0.925..  Test Loss: 0.922.. \n",
      "Epoch: 4307/5000..  Training Loss: 0.954..  Test Loss: 0.922.. \n",
      "Epoch: 4308/5000..  Training Loss: 0.896..  Test Loss: 0.922.. \n",
      "Epoch: 4309/5000..  Training Loss: 0.892..  Test Loss: 0.922.. \n",
      "Epoch: 4310/5000..  Training Loss: 0.912..  Test Loss: 0.922.. \n",
      "Epoch: 4311/5000..  Training Loss: 0.931..  Test Loss: 0.922.. \n",
      "Epoch: 4312/5000..  Training Loss: 0.903..  Test Loss: 0.922.. \n",
      "Epoch: 4313/5000..  Training Loss: 0.923..  Test Loss: 0.922.. \n",
      "Epoch: 4314/5000..  Training Loss: 0.918..  Test Loss: 0.922.. \n",
      "Epoch: 4315/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4316/5000..  Training Loss: 0.915..  Test Loss: 0.922.. \n",
      "Epoch: 4317/5000..  Training Loss: 0.950..  Test Loss: 0.922.. \n",
      "Epoch: 4318/5000..  Training Loss: 0.898..  Test Loss: 0.922.. \n",
      "Epoch: 4319/5000..  Training Loss: 0.930..  Test Loss: 0.922.. \n",
      "Epoch: 4320/5000..  Training Loss: 0.911..  Test Loss: 0.922.. \n",
      "Epoch: 4321/5000..  Training Loss: 0.879..  Test Loss: 0.922.. \n",
      "Epoch: 4322/5000..  Training Loss: 0.961..  Test Loss: 0.922.. \n",
      "Epoch: 4323/5000..  Training Loss: 0.916..  Test Loss: 0.922.. \n",
      "Epoch: 4324/5000..  Training Loss: 0.917..  Test Loss: 0.922.. \n",
      "Epoch: 4325/5000..  Training Loss: 0.932..  Test Loss: 0.922.. \n",
      "Epoch: 4326/5000..  Training Loss: 0.915..  Test Loss: 0.922.. \n",
      "Epoch: 4327/5000..  Training Loss: 0.943..  Test Loss: 0.922.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4328/5000..  Training Loss: 0.908..  Test Loss: 0.922.. \n",
      "Epoch: 4329/5000..  Training Loss: 0.897..  Test Loss: 0.922.. \n",
      "Epoch: 4330/5000..  Training Loss: 0.935..  Test Loss: 0.922.. \n",
      "Epoch: 4331/5000..  Training Loss: 0.917..  Test Loss: 0.922.. \n",
      "Epoch: 4332/5000..  Training Loss: 0.935..  Test Loss: 0.922.. \n",
      "Epoch: 4333/5000..  Training Loss: 0.957..  Test Loss: 0.922.. \n",
      "Epoch: 4334/5000..  Training Loss: 0.884..  Test Loss: 0.922.. \n",
      "Epoch: 4335/5000..  Training Loss: 0.923..  Test Loss: 0.922.. \n",
      "Epoch: 4336/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4337/5000..  Training Loss: 0.917..  Test Loss: 0.922.. \n",
      "Epoch: 4338/5000..  Training Loss: 0.898..  Test Loss: 0.922.. \n",
      "Epoch: 4339/5000..  Training Loss: 0.903..  Test Loss: 0.922.. \n",
      "Epoch: 4340/5000..  Training Loss: 0.908..  Test Loss: 0.922.. \n",
      "Epoch: 4341/5000..  Training Loss: 0.926..  Test Loss: 0.922.. \n",
      "Epoch: 4342/5000..  Training Loss: 0.958..  Test Loss: 0.922.. \n",
      "Epoch: 4343/5000..  Training Loss: 0.939..  Test Loss: 0.922.. \n",
      "Epoch: 4344/5000..  Training Loss: 0.923..  Test Loss: 0.922.. \n",
      "Epoch: 4345/5000..  Training Loss: 0.926..  Test Loss: 0.922.. \n",
      "Epoch: 4346/5000..  Training Loss: 0.930..  Test Loss: 0.922.. \n",
      "Epoch: 4347/5000..  Training Loss: 0.918..  Test Loss: 0.922.. \n",
      "Epoch: 4348/5000..  Training Loss: 0.904..  Test Loss: 0.922.. \n",
      "Epoch: 4349/5000..  Training Loss: 0.931..  Test Loss: 0.922.. \n",
      "Epoch: 4350/5000..  Training Loss: 0.925..  Test Loss: 0.922.. \n",
      "Epoch: 4351/5000..  Training Loss: 0.944..  Test Loss: 0.922.. \n",
      "Epoch: 4352/5000..  Training Loss: 0.906..  Test Loss: 0.922.. \n",
      "Epoch: 4353/5000..  Training Loss: 0.964..  Test Loss: 0.922.. \n",
      "Epoch: 4354/5000..  Training Loss: 0.909..  Test Loss: 0.922.. \n",
      "Epoch: 4355/5000..  Training Loss: 0.925..  Test Loss: 0.922.. \n",
      "Epoch: 4356/5000..  Training Loss: 0.904..  Test Loss: 0.922.. \n",
      "Epoch: 4357/5000..  Training Loss: 0.932..  Test Loss: 0.922.. \n",
      "Epoch: 4358/5000..  Training Loss: 0.934..  Test Loss: 0.922.. \n",
      "Epoch: 4359/5000..  Training Loss: 0.884..  Test Loss: 0.922.. \n",
      "Epoch: 4360/5000..  Training Loss: 0.920..  Test Loss: 0.922.. \n",
      "Epoch: 4361/5000..  Training Loss: 0.944..  Test Loss: 0.922.. \n",
      "Epoch: 4362/5000..  Training Loss: 0.886..  Test Loss: 0.922.. \n",
      "Epoch: 4363/5000..  Training Loss: 0.931..  Test Loss: 0.922.. \n",
      "Epoch: 4364/5000..  Training Loss: 0.918..  Test Loss: 0.922.. \n",
      "Epoch: 4365/5000..  Training Loss: 0.931..  Test Loss: 0.922.. \n",
      "Epoch: 4366/5000..  Training Loss: 0.914..  Test Loss: 0.922.. \n",
      "Epoch: 4367/5000..  Training Loss: 0.926..  Test Loss: 0.922.. \n",
      "Epoch: 4368/5000..  Training Loss: 0.913..  Test Loss: 0.922.. \n",
      "Epoch: 4369/5000..  Training Loss: 0.921..  Test Loss: 0.922.. \n",
      "Epoch: 4370/5000..  Training Loss: 0.896..  Test Loss: 0.922.. \n",
      "Epoch: 4371/5000..  Training Loss: 0.968..  Test Loss: 0.922.. \n",
      "Epoch: 4372/5000..  Training Loss: 0.896..  Test Loss: 0.922.. \n",
      "Epoch: 4373/5000..  Training Loss: 0.970..  Test Loss: 0.922.. \n",
      "Epoch: 4374/5000..  Training Loss: 0.929..  Test Loss: 0.922.. \n",
      "Epoch: 4375/5000..  Training Loss: 0.909..  Test Loss: 0.922.. \n",
      "Epoch: 4376/5000..  Training Loss: 0.936..  Test Loss: 0.922.. \n",
      "Epoch: 4377/5000..  Training Loss: 0.909..  Test Loss: 0.922.. \n",
      "Epoch: 4378/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4379/5000..  Training Loss: 0.901..  Test Loss: 0.922.. \n",
      "Epoch: 4380/5000..  Training Loss: 0.923..  Test Loss: 0.922.. \n",
      "Epoch: 4381/5000..  Training Loss: 0.926..  Test Loss: 0.922.. \n",
      "Epoch: 4382/5000..  Training Loss: 0.918..  Test Loss: 0.922.. \n",
      "Epoch: 4383/5000..  Training Loss: 0.942..  Test Loss: 0.922.. \n",
      "Epoch: 4384/5000..  Training Loss: 0.894..  Test Loss: 0.922.. \n",
      "Epoch: 4385/5000..  Training Loss: 0.904..  Test Loss: 0.922.. \n",
      "Epoch: 4386/5000..  Training Loss: 0.944..  Test Loss: 0.922.. \n",
      "Epoch: 4387/5000..  Training Loss: 0.928..  Test Loss: 0.922.. \n",
      "Epoch: 4388/5000..  Training Loss: 0.922..  Test Loss: 0.922.. \n",
      "Epoch: 4389/5000..  Training Loss: 0.891..  Test Loss: 0.922.. \n",
      "Epoch: 4390/5000..  Training Loss: 0.901..  Test Loss: 0.922.. \n",
      "Epoch: 4391/5000..  Training Loss: 0.944..  Test Loss: 0.922.. \n",
      "Epoch: 4392/5000..  Training Loss: 0.919..  Test Loss: 0.922.. \n",
      "Epoch: 4393/5000..  Training Loss: 0.923..  Test Loss: 0.922.. \n",
      "Epoch: 4394/5000..  Training Loss: 0.901..  Test Loss: 0.922.. \n",
      "Epoch: 4395/5000..  Training Loss: 0.906..  Test Loss: 0.922.. \n",
      "Epoch: 4396/5000..  Training Loss: 0.946..  Test Loss: 0.922.. \n",
      "Epoch: 4397/5000..  Training Loss: 0.954..  Test Loss: 0.922.. \n",
      "Epoch: 4398/5000..  Training Loss: 0.908..  Test Loss: 0.922.. \n",
      "Epoch: 4399/5000..  Training Loss: 0.939..  Test Loss: 0.922.. \n",
      "Epoch: 4400/5000..  Training Loss: 0.937..  Test Loss: 0.922.. \n",
      "Epoch: 4401/5000..  Training Loss: 0.927..  Test Loss: 0.922.. \n",
      "Epoch: 4402/5000..  Training Loss: 0.918..  Test Loss: 0.922.. \n",
      "Epoch: 4403/5000..  Training Loss: 0.904..  Test Loss: 0.922.. \n",
      "Epoch: 4404/5000..  Training Loss: 0.926..  Test Loss: 0.922.. \n",
      "Epoch: 4405/5000..  Training Loss: 0.895..  Test Loss: 0.921.. \n",
      "Epoch: 4406/5000..  Training Loss: 0.896..  Test Loss: 0.921.. \n",
      "Epoch: 4407/5000..  Training Loss: 0.904..  Test Loss: 0.921.. \n",
      "Epoch: 4408/5000..  Training Loss: 0.879..  Test Loss: 0.921.. \n",
      "Epoch: 4409/5000..  Training Loss: 0.906..  Test Loss: 0.921.. \n",
      "Epoch: 4410/5000..  Training Loss: 0.973..  Test Loss: 0.921.. \n",
      "Epoch: 4411/5000..  Training Loss: 0.947..  Test Loss: 0.921.. \n",
      "Epoch: 4412/5000..  Training Loss: 0.919..  Test Loss: 0.921.. \n",
      "Epoch: 4413/5000..  Training Loss: 0.900..  Test Loss: 0.921.. \n",
      "Epoch: 4414/5000..  Training Loss: 0.875..  Test Loss: 0.921.. \n",
      "Epoch: 4415/5000..  Training Loss: 0.942..  Test Loss: 0.921.. \n",
      "Epoch: 4416/5000..  Training Loss: 0.897..  Test Loss: 0.921.. \n",
      "Epoch: 4417/5000..  Training Loss: 0.917..  Test Loss: 0.921.. \n",
      "Epoch: 4418/5000..  Training Loss: 0.943..  Test Loss: 0.921.. \n",
      "Epoch: 4419/5000..  Training Loss: 0.899..  Test Loss: 0.921.. \n",
      "Epoch: 4420/5000..  Training Loss: 0.907..  Test Loss: 0.921.. \n",
      "Epoch: 4421/5000..  Training Loss: 0.908..  Test Loss: 0.921.. \n",
      "Epoch: 4422/5000..  Training Loss: 0.905..  Test Loss: 0.921.. \n",
      "Epoch: 4423/5000..  Training Loss: 0.917..  Test Loss: 0.921.. \n",
      "Epoch: 4424/5000..  Training Loss: 0.925..  Test Loss: 0.921.. \n",
      "Epoch: 4425/5000..  Training Loss: 0.924..  Test Loss: 0.921.. \n",
      "Epoch: 4426/5000..  Training Loss: 0.902..  Test Loss: 0.921.. \n",
      "Epoch: 4427/5000..  Training Loss: 0.952..  Test Loss: 0.921.. \n",
      "Epoch: 4428/5000..  Training Loss: 0.874..  Test Loss: 0.921.. \n",
      "Epoch: 4429/5000..  Training Loss: 0.893..  Test Loss: 0.921.. \n",
      "Epoch: 4430/5000..  Training Loss: 0.911..  Test Loss: 0.921.. \n",
      "Epoch: 4431/5000..  Training Loss: 0.927..  Test Loss: 0.921.. \n",
      "Epoch: 4432/5000..  Training Loss: 0.911..  Test Loss: 0.921.. \n",
      "Epoch: 4433/5000..  Training Loss: 0.941..  Test Loss: 0.921.. \n",
      "Epoch: 4434/5000..  Training Loss: 0.920..  Test Loss: 0.921.. \n",
      "Epoch: 4435/5000..  Training Loss: 0.898..  Test Loss: 0.921.. \n",
      "Epoch: 4436/5000..  Training Loss: 0.923..  Test Loss: 0.921.. \n",
      "Epoch: 4437/5000..  Training Loss: 0.892..  Test Loss: 0.921.. \n",
      "Epoch: 4438/5000..  Training Loss: 0.936..  Test Loss: 0.921.. \n",
      "Epoch: 4439/5000..  Training Loss: 0.939..  Test Loss: 0.921.. \n",
      "Epoch: 4440/5000..  Training Loss: 0.906..  Test Loss: 0.921.. \n",
      "Epoch: 4441/5000..  Training Loss: 0.917..  Test Loss: 0.921.. \n",
      "Epoch: 4442/5000..  Training Loss: 0.895..  Test Loss: 0.921.. \n",
      "Epoch: 4443/5000..  Training Loss: 0.945..  Test Loss: 0.921.. \n",
      "Epoch: 4444/5000..  Training Loss: 0.917..  Test Loss: 0.921.. \n",
      "Epoch: 4445/5000..  Training Loss: 0.933..  Test Loss: 0.921.. \n",
      "Epoch: 4446/5000..  Training Loss: 0.947..  Test Loss: 0.921.. \n",
      "Epoch: 4447/5000..  Training Loss: 0.914..  Test Loss: 0.921.. \n",
      "Epoch: 4448/5000..  Training Loss: 0.934..  Test Loss: 0.921.. \n",
      "Epoch: 4449/5000..  Training Loss: 0.944..  Test Loss: 0.921.. \n",
      "Epoch: 4450/5000..  Training Loss: 0.936..  Test Loss: 0.921.. \n",
      "Epoch: 4451/5000..  Training Loss: 0.941..  Test Loss: 0.921.. \n",
      "Epoch: 4452/5000..  Training Loss: 0.935..  Test Loss: 0.921.. \n",
      "Epoch: 4453/5000..  Training Loss: 0.921..  Test Loss: 0.921.. \n",
      "Epoch: 4454/5000..  Training Loss: 0.913..  Test Loss: 0.921.. \n",
      "Epoch: 4455/5000..  Training Loss: 0.953..  Test Loss: 0.921.. \n",
      "Epoch: 4456/5000..  Training Loss: 0.933..  Test Loss: 0.921.. \n",
      "Epoch: 4457/5000..  Training Loss: 0.893..  Test Loss: 0.921.. \n",
      "Epoch: 4458/5000..  Training Loss: 0.931..  Test Loss: 0.921.. \n",
      "Epoch: 4459/5000..  Training Loss: 0.938..  Test Loss: 0.921.. \n",
      "Epoch: 4460/5000..  Training Loss: 0.923..  Test Loss: 0.921.. \n",
      "Epoch: 4461/5000..  Training Loss: 0.951..  Test Loss: 0.921.. \n",
      "Epoch: 4462/5000..  Training Loss: 0.932..  Test Loss: 0.921.. \n",
      "Epoch: 4463/5000..  Training Loss: 0.908..  Test Loss: 0.921.. \n",
      "Epoch: 4464/5000..  Training Loss: 0.909..  Test Loss: 0.921.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4465/5000..  Training Loss: 0.942..  Test Loss: 0.921.. \n",
      "Epoch: 4466/5000..  Training Loss: 0.900..  Test Loss: 0.921.. \n",
      "Epoch: 4467/5000..  Training Loss: 0.910..  Test Loss: 0.921.. \n",
      "Epoch: 4468/5000..  Training Loss: 0.949..  Test Loss: 0.921.. \n",
      "Epoch: 4469/5000..  Training Loss: 0.942..  Test Loss: 0.921.. \n",
      "Epoch: 4470/5000..  Training Loss: 0.929..  Test Loss: 0.921.. \n",
      "Epoch: 4471/5000..  Training Loss: 0.914..  Test Loss: 0.921.. \n",
      "Epoch: 4472/5000..  Training Loss: 0.912..  Test Loss: 0.921.. \n",
      "Epoch: 4473/5000..  Training Loss: 0.925..  Test Loss: 0.921.. \n",
      "Epoch: 4474/5000..  Training Loss: 0.942..  Test Loss: 0.921.. \n",
      "Epoch: 4475/5000..  Training Loss: 0.927..  Test Loss: 0.921.. \n",
      "Epoch: 4476/5000..  Training Loss: 0.904..  Test Loss: 0.921.. \n",
      "Epoch: 4477/5000..  Training Loss: 0.894..  Test Loss: 0.921.. \n",
      "Epoch: 4478/5000..  Training Loss: 0.901..  Test Loss: 0.921.. \n",
      "Epoch: 4479/5000..  Training Loss: 0.932..  Test Loss: 0.921.. \n",
      "Epoch: 4480/5000..  Training Loss: 0.927..  Test Loss: 0.921.. \n",
      "Epoch: 4481/5000..  Training Loss: 0.935..  Test Loss: 0.921.. \n",
      "Epoch: 4482/5000..  Training Loss: 0.932..  Test Loss: 0.921.. \n",
      "Epoch: 4483/5000..  Training Loss: 0.931..  Test Loss: 0.921.. \n",
      "Epoch: 4484/5000..  Training Loss: 0.936..  Test Loss: 0.921.. \n",
      "Epoch: 4485/5000..  Training Loss: 0.939..  Test Loss: 0.921.. \n",
      "Epoch: 4486/5000..  Training Loss: 0.943..  Test Loss: 0.921.. \n",
      "Epoch: 4487/5000..  Training Loss: 0.928..  Test Loss: 0.921.. \n",
      "Epoch: 4488/5000..  Training Loss: 0.954..  Test Loss: 0.921.. \n",
      "Epoch: 4489/5000..  Training Loss: 0.908..  Test Loss: 0.921.. \n",
      "Epoch: 4490/5000..  Training Loss: 0.944..  Test Loss: 0.921.. \n",
      "Epoch: 4491/5000..  Training Loss: 0.889..  Test Loss: 0.921.. \n",
      "Epoch: 4492/5000..  Training Loss: 0.921..  Test Loss: 0.921.. \n",
      "Epoch: 4493/5000..  Training Loss: 0.913..  Test Loss: 0.921.. \n",
      "Epoch: 4494/5000..  Training Loss: 0.914..  Test Loss: 0.921.. \n",
      "Epoch: 4495/5000..  Training Loss: 0.910..  Test Loss: 0.921.. \n",
      "Epoch: 4496/5000..  Training Loss: 0.921..  Test Loss: 0.921.. \n",
      "Epoch: 4497/5000..  Training Loss: 0.910..  Test Loss: 0.921.. \n",
      "Epoch: 4498/5000..  Training Loss: 0.890..  Test Loss: 0.921.. \n",
      "Epoch: 4499/5000..  Training Loss: 0.912..  Test Loss: 0.921.. \n",
      "Epoch: 4500/5000..  Training Loss: 0.918..  Test Loss: 0.921.. \n",
      "Epoch: 4501/5000..  Training Loss: 0.903..  Test Loss: 0.921.. \n",
      "Epoch: 4502/5000..  Training Loss: 0.931..  Test Loss: 0.921.. \n",
      "Epoch: 4503/5000..  Training Loss: 0.964..  Test Loss: 0.921.. \n",
      "Epoch: 4504/5000..  Training Loss: 0.928..  Test Loss: 0.921.. \n",
      "Epoch: 4505/5000..  Training Loss: 0.900..  Test Loss: 0.921.. \n",
      "Epoch: 4506/5000..  Training Loss: 0.928..  Test Loss: 0.921.. \n",
      "Epoch: 4507/5000..  Training Loss: 0.926..  Test Loss: 0.921.. \n",
      "Epoch: 4508/5000..  Training Loss: 0.942..  Test Loss: 0.921.. \n",
      "Epoch: 4509/5000..  Training Loss: 0.929..  Test Loss: 0.921.. \n",
      "Epoch: 4510/5000..  Training Loss: 0.928..  Test Loss: 0.921.. \n",
      "Epoch: 4511/5000..  Training Loss: 0.902..  Test Loss: 0.921.. \n",
      "Epoch: 4512/5000..  Training Loss: 0.906..  Test Loss: 0.921.. \n",
      "Epoch: 4513/5000..  Training Loss: 0.907..  Test Loss: 0.921.. \n",
      "Epoch: 4514/5000..  Training Loss: 0.907..  Test Loss: 0.921.. \n",
      "Epoch: 4515/5000..  Training Loss: 0.939..  Test Loss: 0.921.. \n",
      "Epoch: 4516/5000..  Training Loss: 0.950..  Test Loss: 0.921.. \n",
      "Epoch: 4517/5000..  Training Loss: 0.909..  Test Loss: 0.921.. \n",
      "Epoch: 4518/5000..  Training Loss: 0.923..  Test Loss: 0.921.. \n",
      "Epoch: 4519/5000..  Training Loss: 0.922..  Test Loss: 0.921.. \n",
      "Epoch: 4520/5000..  Training Loss: 0.906..  Test Loss: 0.921.. \n",
      "Epoch: 4521/5000..  Training Loss: 0.917..  Test Loss: 0.921.. \n",
      "Epoch: 4522/5000..  Training Loss: 0.901..  Test Loss: 0.921.. \n",
      "Epoch: 4523/5000..  Training Loss: 0.932..  Test Loss: 0.921.. \n",
      "Epoch: 4524/5000..  Training Loss: 0.905..  Test Loss: 0.921.. \n",
      "Epoch: 4525/5000..  Training Loss: 0.934..  Test Loss: 0.921.. \n",
      "Epoch: 4526/5000..  Training Loss: 0.956..  Test Loss: 0.921.. \n",
      "Epoch: 4527/5000..  Training Loss: 0.943..  Test Loss: 0.921.. \n",
      "Epoch: 4528/5000..  Training Loss: 0.934..  Test Loss: 0.921.. \n",
      "Epoch: 4529/5000..  Training Loss: 0.914..  Test Loss: 0.921.. \n",
      "Epoch: 4530/5000..  Training Loss: 0.914..  Test Loss: 0.921.. \n",
      "Epoch: 4531/5000..  Training Loss: 0.906..  Test Loss: 0.921.. \n",
      "Epoch: 4532/5000..  Training Loss: 0.881..  Test Loss: 0.921.. \n",
      "Epoch: 4533/5000..  Training Loss: 0.934..  Test Loss: 0.921.. \n",
      "Epoch: 4534/5000..  Training Loss: 0.930..  Test Loss: 0.921.. \n",
      "Epoch: 4535/5000..  Training Loss: 0.932..  Test Loss: 0.921.. \n",
      "Epoch: 4536/5000..  Training Loss: 0.901..  Test Loss: 0.921.. \n",
      "Epoch: 4537/5000..  Training Loss: 0.921..  Test Loss: 0.921.. \n",
      "Epoch: 4538/5000..  Training Loss: 0.928..  Test Loss: 0.921.. \n",
      "Epoch: 4539/5000..  Training Loss: 0.884..  Test Loss: 0.921.. \n",
      "Epoch: 4540/5000..  Training Loss: 0.962..  Test Loss: 0.921.. \n",
      "Epoch: 4541/5000..  Training Loss: 0.899..  Test Loss: 0.921.. \n",
      "Epoch: 4542/5000..  Training Loss: 0.939..  Test Loss: 0.921.. \n",
      "Epoch: 4543/5000..  Training Loss: 0.892..  Test Loss: 0.921.. \n",
      "Epoch: 4544/5000..  Training Loss: 0.912..  Test Loss: 0.921.. \n",
      "Epoch: 4545/5000..  Training Loss: 0.946..  Test Loss: 0.921.. \n",
      "Epoch: 4546/5000..  Training Loss: 0.939..  Test Loss: 0.921.. \n",
      "Epoch: 4547/5000..  Training Loss: 0.938..  Test Loss: 0.921.. \n",
      "Epoch: 4548/5000..  Training Loss: 0.932..  Test Loss: 0.921.. \n",
      "Epoch: 4549/5000..  Training Loss: 0.918..  Test Loss: 0.921.. \n",
      "Epoch: 4550/5000..  Training Loss: 0.920..  Test Loss: 0.921.. \n",
      "Epoch: 4551/5000..  Training Loss: 0.940..  Test Loss: 0.921.. \n",
      "Epoch: 4552/5000..  Training Loss: 0.937..  Test Loss: 0.921.. \n",
      "Epoch: 4553/5000..  Training Loss: 0.907..  Test Loss: 0.921.. \n",
      "Epoch: 4554/5000..  Training Loss: 0.944..  Test Loss: 0.921.. \n",
      "Epoch: 4555/5000..  Training Loss: 0.904..  Test Loss: 0.921.. \n",
      "Epoch: 4556/5000..  Training Loss: 0.862..  Test Loss: 0.921.. \n",
      "Epoch: 4557/5000..  Training Loss: 0.938..  Test Loss: 0.921.. \n",
      "Epoch: 4558/5000..  Training Loss: 0.873..  Test Loss: 0.921.. \n",
      "Epoch: 4559/5000..  Training Loss: 0.886..  Test Loss: 0.921.. \n",
      "Epoch: 4560/5000..  Training Loss: 0.901..  Test Loss: 0.921.. \n",
      "Epoch: 4561/5000..  Training Loss: 0.933..  Test Loss: 0.921.. \n",
      "Epoch: 4562/5000..  Training Loss: 0.890..  Test Loss: 0.921.. \n",
      "Epoch: 4563/5000..  Training Loss: 0.906..  Test Loss: 0.921.. \n",
      "Epoch: 4564/5000..  Training Loss: 0.953..  Test Loss: 0.921.. \n",
      "Epoch: 4565/5000..  Training Loss: 0.922..  Test Loss: 0.921.. \n",
      "Epoch: 4566/5000..  Training Loss: 0.929..  Test Loss: 0.921.. \n",
      "Epoch: 4567/5000..  Training Loss: 0.919..  Test Loss: 0.921.. \n",
      "Epoch: 4568/5000..  Training Loss: 0.902..  Test Loss: 0.921.. \n",
      "Epoch: 4569/5000..  Training Loss: 0.896..  Test Loss: 0.920.. \n",
      "Epoch: 4570/5000..  Training Loss: 0.942..  Test Loss: 0.920.. \n",
      "Epoch: 4571/5000..  Training Loss: 0.911..  Test Loss: 0.920.. \n",
      "Epoch: 4572/5000..  Training Loss: 0.958..  Test Loss: 0.920.. \n",
      "Epoch: 4573/5000..  Training Loss: 0.923..  Test Loss: 0.920.. \n",
      "Epoch: 4574/5000..  Training Loss: 0.922..  Test Loss: 0.920.. \n",
      "Epoch: 4575/5000..  Training Loss: 0.900..  Test Loss: 0.920.. \n",
      "Epoch: 4576/5000..  Training Loss: 0.943..  Test Loss: 0.920.. \n",
      "Epoch: 4577/5000..  Training Loss: 0.926..  Test Loss: 0.920.. \n",
      "Epoch: 4578/5000..  Training Loss: 0.942..  Test Loss: 0.920.. \n",
      "Epoch: 4579/5000..  Training Loss: 0.908..  Test Loss: 0.920.. \n",
      "Epoch: 4580/5000..  Training Loss: 0.930..  Test Loss: 0.920.. \n",
      "Epoch: 4581/5000..  Training Loss: 0.923..  Test Loss: 0.920.. \n",
      "Epoch: 4582/5000..  Training Loss: 0.940..  Test Loss: 0.920.. \n",
      "Epoch: 4583/5000..  Training Loss: 0.912..  Test Loss: 0.920.. \n",
      "Epoch: 4584/5000..  Training Loss: 0.911..  Test Loss: 0.920.. \n",
      "Epoch: 4585/5000..  Training Loss: 0.914..  Test Loss: 0.920.. \n",
      "Epoch: 4586/5000..  Training Loss: 0.918..  Test Loss: 0.920.. \n",
      "Epoch: 4587/5000..  Training Loss: 0.926..  Test Loss: 0.920.. \n",
      "Epoch: 4588/5000..  Training Loss: 0.907..  Test Loss: 0.920.. \n",
      "Epoch: 4589/5000..  Training Loss: 0.900..  Test Loss: 0.920.. \n",
      "Epoch: 4590/5000..  Training Loss: 0.922..  Test Loss: 0.920.. \n",
      "Epoch: 4591/5000..  Training Loss: 0.926..  Test Loss: 0.920.. \n",
      "Epoch: 4592/5000..  Training Loss: 0.926..  Test Loss: 0.920.. \n",
      "Epoch: 4593/5000..  Training Loss: 0.916..  Test Loss: 0.920.. \n",
      "Epoch: 4594/5000..  Training Loss: 0.898..  Test Loss: 0.920.. \n",
      "Epoch: 4595/5000..  Training Loss: 0.896..  Test Loss: 0.920.. \n",
      "Epoch: 4596/5000..  Training Loss: 0.932..  Test Loss: 0.920.. \n",
      "Epoch: 4597/5000..  Training Loss: 0.952..  Test Loss: 0.920.. \n",
      "Epoch: 4598/5000..  Training Loss: 0.912..  Test Loss: 0.920.. \n",
      "Epoch: 4599/5000..  Training Loss: 0.921..  Test Loss: 0.920.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4600/5000..  Training Loss: 0.905..  Test Loss: 0.920.. \n",
      "Epoch: 4601/5000..  Training Loss: 0.914..  Test Loss: 0.920.. \n",
      "Epoch: 4602/5000..  Training Loss: 0.912..  Test Loss: 0.920.. \n",
      "Epoch: 4603/5000..  Training Loss: 0.942..  Test Loss: 0.920.. \n",
      "Epoch: 4604/5000..  Training Loss: 0.925..  Test Loss: 0.920.. \n",
      "Epoch: 4605/5000..  Training Loss: 0.919..  Test Loss: 0.920.. \n",
      "Epoch: 4606/5000..  Training Loss: 0.890..  Test Loss: 0.920.. \n",
      "Epoch: 4607/5000..  Training Loss: 0.919..  Test Loss: 0.920.. \n",
      "Epoch: 4608/5000..  Training Loss: 0.911..  Test Loss: 0.920.. \n",
      "Epoch: 4609/5000..  Training Loss: 0.946..  Test Loss: 0.920.. \n",
      "Epoch: 4610/5000..  Training Loss: 0.942..  Test Loss: 0.920.. \n",
      "Epoch: 4611/5000..  Training Loss: 0.936..  Test Loss: 0.920.. \n",
      "Epoch: 4612/5000..  Training Loss: 0.899..  Test Loss: 0.920.. \n",
      "Epoch: 4613/5000..  Training Loss: 0.894..  Test Loss: 0.920.. \n",
      "Epoch: 4614/5000..  Training Loss: 0.927..  Test Loss: 0.920.. \n",
      "Epoch: 4615/5000..  Training Loss: 0.943..  Test Loss: 0.920.. \n",
      "Epoch: 4616/5000..  Training Loss: 0.927..  Test Loss: 0.920.. \n",
      "Epoch: 4617/5000..  Training Loss: 0.909..  Test Loss: 0.920.. \n",
      "Epoch: 4618/5000..  Training Loss: 0.922..  Test Loss: 0.920.. \n",
      "Epoch: 4619/5000..  Training Loss: 0.960..  Test Loss: 0.920.. \n",
      "Epoch: 4620/5000..  Training Loss: 0.916..  Test Loss: 0.920.. \n",
      "Epoch: 4621/5000..  Training Loss: 0.929..  Test Loss: 0.920.. \n",
      "Epoch: 4622/5000..  Training Loss: 0.913..  Test Loss: 0.920.. \n",
      "Epoch: 4623/5000..  Training Loss: 0.896..  Test Loss: 0.920.. \n",
      "Epoch: 4624/5000..  Training Loss: 0.923..  Test Loss: 0.920.. \n",
      "Epoch: 4625/5000..  Training Loss: 0.957..  Test Loss: 0.920.. \n",
      "Epoch: 4626/5000..  Training Loss: 0.923..  Test Loss: 0.920.. \n",
      "Epoch: 4627/5000..  Training Loss: 0.907..  Test Loss: 0.920.. \n",
      "Epoch: 4628/5000..  Training Loss: 0.919..  Test Loss: 0.920.. \n",
      "Epoch: 4629/5000..  Training Loss: 0.920..  Test Loss: 0.920.. \n",
      "Epoch: 4630/5000..  Training Loss: 0.937..  Test Loss: 0.920.. \n",
      "Epoch: 4631/5000..  Training Loss: 0.904..  Test Loss: 0.920.. \n",
      "Epoch: 4632/5000..  Training Loss: 0.914..  Test Loss: 0.920.. \n",
      "Epoch: 4633/5000..  Training Loss: 0.930..  Test Loss: 0.920.. \n",
      "Epoch: 4634/5000..  Training Loss: 0.898..  Test Loss: 0.920.. \n",
      "Epoch: 4635/5000..  Training Loss: 0.908..  Test Loss: 0.920.. \n",
      "Epoch: 4636/5000..  Training Loss: 0.935..  Test Loss: 0.920.. \n",
      "Epoch: 4637/5000..  Training Loss: 0.919..  Test Loss: 0.920.. \n",
      "Epoch: 4638/5000..  Training Loss: 0.897..  Test Loss: 0.920.. \n",
      "Epoch: 4639/5000..  Training Loss: 0.938..  Test Loss: 0.920.. \n",
      "Epoch: 4640/5000..  Training Loss: 0.900..  Test Loss: 0.920.. \n",
      "Epoch: 4641/5000..  Training Loss: 0.927..  Test Loss: 0.920.. \n",
      "Epoch: 4642/5000..  Training Loss: 0.912..  Test Loss: 0.920.. \n",
      "Epoch: 4643/5000..  Training Loss: 0.905..  Test Loss: 0.920.. \n",
      "Epoch: 4644/5000..  Training Loss: 0.929..  Test Loss: 0.920.. \n",
      "Epoch: 4645/5000..  Training Loss: 0.942..  Test Loss: 0.920.. \n",
      "Epoch: 4646/5000..  Training Loss: 0.920..  Test Loss: 0.920.. \n",
      "Epoch: 4647/5000..  Training Loss: 0.913..  Test Loss: 0.920.. \n",
      "Epoch: 4648/5000..  Training Loss: 0.932..  Test Loss: 0.920.. \n",
      "Epoch: 4649/5000..  Training Loss: 0.925..  Test Loss: 0.920.. \n",
      "Epoch: 4650/5000..  Training Loss: 0.954..  Test Loss: 0.920.. \n",
      "Epoch: 4651/5000..  Training Loss: 0.910..  Test Loss: 0.920.. \n",
      "Epoch: 4652/5000..  Training Loss: 0.909..  Test Loss: 0.920.. \n",
      "Epoch: 4653/5000..  Training Loss: 0.876..  Test Loss: 0.920.. \n",
      "Epoch: 4654/5000..  Training Loss: 0.942..  Test Loss: 0.920.. \n",
      "Epoch: 4655/5000..  Training Loss: 0.919..  Test Loss: 0.920.. \n",
      "Epoch: 4656/5000..  Training Loss: 0.932..  Test Loss: 0.920.. \n",
      "Epoch: 4657/5000..  Training Loss: 0.910..  Test Loss: 0.920.. \n",
      "Epoch: 4658/5000..  Training Loss: 0.899..  Test Loss: 0.920.. \n",
      "Epoch: 4659/5000..  Training Loss: 0.901..  Test Loss: 0.920.. \n",
      "Epoch: 4660/5000..  Training Loss: 0.948..  Test Loss: 0.920.. \n",
      "Epoch: 4661/5000..  Training Loss: 0.949..  Test Loss: 0.920.. \n",
      "Epoch: 4662/5000..  Training Loss: 0.922..  Test Loss: 0.920.. \n",
      "Epoch: 4663/5000..  Training Loss: 0.891..  Test Loss: 0.920.. \n",
      "Epoch: 4664/5000..  Training Loss: 0.932..  Test Loss: 0.920.. \n",
      "Epoch: 4665/5000..  Training Loss: 0.889..  Test Loss: 0.920.. \n",
      "Epoch: 4666/5000..  Training Loss: 0.909..  Test Loss: 0.920.. \n",
      "Epoch: 4667/5000..  Training Loss: 0.937..  Test Loss: 0.920.. \n",
      "Epoch: 4668/5000..  Training Loss: 0.925..  Test Loss: 0.920.. \n",
      "Epoch: 4669/5000..  Training Loss: 0.916..  Test Loss: 0.920.. \n",
      "Epoch: 4670/5000..  Training Loss: 0.927..  Test Loss: 0.920.. \n",
      "Epoch: 4671/5000..  Training Loss: 0.916..  Test Loss: 0.920.. \n",
      "Epoch: 4672/5000..  Training Loss: 0.952..  Test Loss: 0.920.. \n",
      "Epoch: 4673/5000..  Training Loss: 0.896..  Test Loss: 0.920.. \n",
      "Epoch: 4674/5000..  Training Loss: 0.940..  Test Loss: 0.920.. \n",
      "Epoch: 4675/5000..  Training Loss: 0.923..  Test Loss: 0.920.. \n",
      "Epoch: 4676/5000..  Training Loss: 0.929..  Test Loss: 0.920.. \n",
      "Epoch: 4677/5000..  Training Loss: 0.919..  Test Loss: 0.920.. \n",
      "Epoch: 4678/5000..  Training Loss: 0.966..  Test Loss: 0.920.. \n",
      "Epoch: 4679/5000..  Training Loss: 0.882..  Test Loss: 0.920.. \n",
      "Epoch: 4680/5000..  Training Loss: 0.923..  Test Loss: 0.920.. \n",
      "Epoch: 4681/5000..  Training Loss: 0.928..  Test Loss: 0.920.. \n",
      "Epoch: 4682/5000..  Training Loss: 0.873..  Test Loss: 0.920.. \n",
      "Epoch: 4683/5000..  Training Loss: 0.961..  Test Loss: 0.920.. \n",
      "Epoch: 4684/5000..  Training Loss: 0.939..  Test Loss: 0.920.. \n",
      "Epoch: 4685/5000..  Training Loss: 0.928..  Test Loss: 0.920.. \n",
      "Epoch: 4686/5000..  Training Loss: 0.909..  Test Loss: 0.920.. \n",
      "Epoch: 4687/5000..  Training Loss: 0.948..  Test Loss: 0.920.. \n",
      "Epoch: 4688/5000..  Training Loss: 0.917..  Test Loss: 0.920.. \n",
      "Epoch: 4689/5000..  Training Loss: 0.910..  Test Loss: 0.920.. \n",
      "Epoch: 4690/5000..  Training Loss: 0.939..  Test Loss: 0.920.. \n",
      "Epoch: 4691/5000..  Training Loss: 0.900..  Test Loss: 0.920.. \n",
      "Epoch: 4692/5000..  Training Loss: 0.895..  Test Loss: 0.920.. \n",
      "Epoch: 4693/5000..  Training Loss: 0.943..  Test Loss: 0.920.. \n",
      "Epoch: 4694/5000..  Training Loss: 0.941..  Test Loss: 0.920.. \n",
      "Epoch: 4695/5000..  Training Loss: 0.896..  Test Loss: 0.920.. \n",
      "Epoch: 4696/5000..  Training Loss: 0.930..  Test Loss: 0.920.. \n",
      "Epoch: 4697/5000..  Training Loss: 0.878..  Test Loss: 0.920.. \n",
      "Epoch: 4698/5000..  Training Loss: 0.924..  Test Loss: 0.920.. \n",
      "Epoch: 4699/5000..  Training Loss: 0.933..  Test Loss: 0.920.. \n",
      "Epoch: 4700/5000..  Training Loss: 0.933..  Test Loss: 0.920.. \n",
      "Epoch: 4701/5000..  Training Loss: 0.919..  Test Loss: 0.920.. \n",
      "Epoch: 4702/5000..  Training Loss: 0.964..  Test Loss: 0.920.. \n",
      "Epoch: 4703/5000..  Training Loss: 0.891..  Test Loss: 0.920.. \n",
      "Epoch: 4704/5000..  Training Loss: 0.934..  Test Loss: 0.920.. \n",
      "Epoch: 4705/5000..  Training Loss: 0.942..  Test Loss: 0.920.. \n",
      "Epoch: 4706/5000..  Training Loss: 0.927..  Test Loss: 0.920.. \n",
      "Epoch: 4707/5000..  Training Loss: 0.925..  Test Loss: 0.920.. \n",
      "Epoch: 4708/5000..  Training Loss: 0.902..  Test Loss: 0.920.. \n",
      "Epoch: 4709/5000..  Training Loss: 0.923..  Test Loss: 0.920.. \n",
      "Epoch: 4710/5000..  Training Loss: 0.935..  Test Loss: 0.920.. \n",
      "Epoch: 4711/5000..  Training Loss: 0.904..  Test Loss: 0.920.. \n",
      "Epoch: 4712/5000..  Training Loss: 0.875..  Test Loss: 0.920.. \n",
      "Epoch: 4713/5000..  Training Loss: 0.924..  Test Loss: 0.920.. \n",
      "Epoch: 4714/5000..  Training Loss: 0.917..  Test Loss: 0.920.. \n",
      "Epoch: 4715/5000..  Training Loss: 0.917..  Test Loss: 0.920.. \n",
      "Epoch: 4716/5000..  Training Loss: 0.911..  Test Loss: 0.920.. \n",
      "Epoch: 4717/5000..  Training Loss: 0.911..  Test Loss: 0.920.. \n",
      "Epoch: 4718/5000..  Training Loss: 0.944..  Test Loss: 0.920.. \n",
      "Epoch: 4719/5000..  Training Loss: 0.906..  Test Loss: 0.920.. \n",
      "Epoch: 4720/5000..  Training Loss: 0.887..  Test Loss: 0.920.. \n",
      "Epoch: 4721/5000..  Training Loss: 0.899..  Test Loss: 0.920.. \n",
      "Epoch: 4722/5000..  Training Loss: 0.916..  Test Loss: 0.920.. \n",
      "Epoch: 4723/5000..  Training Loss: 0.947..  Test Loss: 0.920.. \n",
      "Epoch: 4724/5000..  Training Loss: 0.888..  Test Loss: 0.920.. \n",
      "Epoch: 4725/5000..  Training Loss: 0.928..  Test Loss: 0.920.. \n",
      "Epoch: 4726/5000..  Training Loss: 0.901..  Test Loss: 0.920.. \n",
      "Epoch: 4727/5000..  Training Loss: 0.913..  Test Loss: 0.920.. \n",
      "Epoch: 4728/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4729/5000..  Training Loss: 0.921..  Test Loss: 0.919.. \n",
      "Epoch: 4730/5000..  Training Loss: 0.898..  Test Loss: 0.919.. \n",
      "Epoch: 4731/5000..  Training Loss: 0.895..  Test Loss: 0.919.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4732/5000..  Training Loss: 0.921..  Test Loss: 0.919.. \n",
      "Epoch: 4733/5000..  Training Loss: 0.898..  Test Loss: 0.919.. \n",
      "Epoch: 4734/5000..  Training Loss: 0.941..  Test Loss: 0.919.. \n",
      "Epoch: 4735/5000..  Training Loss: 0.894..  Test Loss: 0.919.. \n",
      "Epoch: 4736/5000..  Training Loss: 0.892..  Test Loss: 0.919.. \n",
      "Epoch: 4737/5000..  Training Loss: 0.925..  Test Loss: 0.919.. \n",
      "Epoch: 4738/5000..  Training Loss: 0.915..  Test Loss: 0.919.. \n",
      "Epoch: 4739/5000..  Training Loss: 0.900..  Test Loss: 0.919.. \n",
      "Epoch: 4740/5000..  Training Loss: 0.913..  Test Loss: 0.919.. \n",
      "Epoch: 4741/5000..  Training Loss: 0.931..  Test Loss: 0.919.. \n",
      "Epoch: 4742/5000..  Training Loss: 0.927..  Test Loss: 0.919.. \n",
      "Epoch: 4743/5000..  Training Loss: 0.937..  Test Loss: 0.919.. \n",
      "Epoch: 4744/5000..  Training Loss: 0.917..  Test Loss: 0.919.. \n",
      "Epoch: 4745/5000..  Training Loss: 0.898..  Test Loss: 0.919.. \n",
      "Epoch: 4746/5000..  Training Loss: 0.933..  Test Loss: 0.919.. \n",
      "Epoch: 4747/5000..  Training Loss: 0.909..  Test Loss: 0.919.. \n",
      "Epoch: 4748/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4749/5000..  Training Loss: 0.930..  Test Loss: 0.919.. \n",
      "Epoch: 4750/5000..  Training Loss: 0.907..  Test Loss: 0.919.. \n",
      "Epoch: 4751/5000..  Training Loss: 0.919..  Test Loss: 0.919.. \n",
      "Epoch: 4752/5000..  Training Loss: 0.913..  Test Loss: 0.919.. \n",
      "Epoch: 4753/5000..  Training Loss: 0.936..  Test Loss: 0.919.. \n",
      "Epoch: 4754/5000..  Training Loss: 0.899..  Test Loss: 0.919.. \n",
      "Epoch: 4755/5000..  Training Loss: 0.926..  Test Loss: 0.919.. \n",
      "Epoch: 4756/5000..  Training Loss: 0.909..  Test Loss: 0.919.. \n",
      "Epoch: 4757/5000..  Training Loss: 0.927..  Test Loss: 0.919.. \n",
      "Epoch: 4758/5000..  Training Loss: 0.905..  Test Loss: 0.919.. \n",
      "Epoch: 4759/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4760/5000..  Training Loss: 0.924..  Test Loss: 0.919.. \n",
      "Epoch: 4761/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4762/5000..  Training Loss: 0.912..  Test Loss: 0.919.. \n",
      "Epoch: 4763/5000..  Training Loss: 0.925..  Test Loss: 0.919.. \n",
      "Epoch: 4764/5000..  Training Loss: 0.918..  Test Loss: 0.919.. \n",
      "Epoch: 4765/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4766/5000..  Training Loss: 0.886..  Test Loss: 0.919.. \n",
      "Epoch: 4767/5000..  Training Loss: 0.940..  Test Loss: 0.919.. \n",
      "Epoch: 4768/5000..  Training Loss: 0.930..  Test Loss: 0.919.. \n",
      "Epoch: 4769/5000..  Training Loss: 0.898..  Test Loss: 0.919.. \n",
      "Epoch: 4770/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4771/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4772/5000..  Training Loss: 0.943..  Test Loss: 0.919.. \n",
      "Epoch: 4773/5000..  Training Loss: 0.920..  Test Loss: 0.919.. \n",
      "Epoch: 4774/5000..  Training Loss: 0.935..  Test Loss: 0.919.. \n",
      "Epoch: 4775/5000..  Training Loss: 0.932..  Test Loss: 0.919.. \n",
      "Epoch: 4776/5000..  Training Loss: 0.922..  Test Loss: 0.919.. \n",
      "Epoch: 4777/5000..  Training Loss: 0.927..  Test Loss: 0.919.. \n",
      "Epoch: 4778/5000..  Training Loss: 0.922..  Test Loss: 0.919.. \n",
      "Epoch: 4779/5000..  Training Loss: 0.869..  Test Loss: 0.919.. \n",
      "Epoch: 4780/5000..  Training Loss: 0.920..  Test Loss: 0.919.. \n",
      "Epoch: 4781/5000..  Training Loss: 0.924..  Test Loss: 0.919.. \n",
      "Epoch: 4782/5000..  Training Loss: 0.931..  Test Loss: 0.919.. \n",
      "Epoch: 4783/5000..  Training Loss: 0.894..  Test Loss: 0.919.. \n",
      "Epoch: 4784/5000..  Training Loss: 0.922..  Test Loss: 0.919.. \n",
      "Epoch: 4785/5000..  Training Loss: 0.903..  Test Loss: 0.919.. \n",
      "Epoch: 4786/5000..  Training Loss: 0.949..  Test Loss: 0.919.. \n",
      "Epoch: 4787/5000..  Training Loss: 0.929..  Test Loss: 0.919.. \n",
      "Epoch: 4788/5000..  Training Loss: 0.925..  Test Loss: 0.919.. \n",
      "Epoch: 4789/5000..  Training Loss: 0.955..  Test Loss: 0.919.. \n",
      "Epoch: 4790/5000..  Training Loss: 0.926..  Test Loss: 0.919.. \n",
      "Epoch: 4791/5000..  Training Loss: 0.903..  Test Loss: 0.919.. \n",
      "Epoch: 4792/5000..  Training Loss: 0.965..  Test Loss: 0.919.. \n",
      "Epoch: 4793/5000..  Training Loss: 0.905..  Test Loss: 0.919.. \n",
      "Epoch: 4794/5000..  Training Loss: 0.930..  Test Loss: 0.919.. \n",
      "Epoch: 4795/5000..  Training Loss: 0.894..  Test Loss: 0.919.. \n",
      "Epoch: 4796/5000..  Training Loss: 0.904..  Test Loss: 0.919.. \n",
      "Epoch: 4797/5000..  Training Loss: 0.915..  Test Loss: 0.919.. \n",
      "Epoch: 4798/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4799/5000..  Training Loss: 0.902..  Test Loss: 0.919.. \n",
      "Epoch: 4800/5000..  Training Loss: 0.932..  Test Loss: 0.919.. \n",
      "Epoch: 4801/5000..  Training Loss: 0.947..  Test Loss: 0.919.. \n",
      "Epoch: 4802/5000..  Training Loss: 0.893..  Test Loss: 0.919.. \n",
      "Epoch: 4803/5000..  Training Loss: 0.922..  Test Loss: 0.919.. \n",
      "Epoch: 4804/5000..  Training Loss: 0.889..  Test Loss: 0.919.. \n",
      "Epoch: 4805/5000..  Training Loss: 0.913..  Test Loss: 0.919.. \n",
      "Epoch: 4806/5000..  Training Loss: 0.919..  Test Loss: 0.919.. \n",
      "Epoch: 4807/5000..  Training Loss: 0.927..  Test Loss: 0.919.. \n",
      "Epoch: 4808/5000..  Training Loss: 0.920..  Test Loss: 0.919.. \n",
      "Epoch: 4809/5000..  Training Loss: 0.937..  Test Loss: 0.919.. \n",
      "Epoch: 4810/5000..  Training Loss: 0.942..  Test Loss: 0.919.. \n",
      "Epoch: 4811/5000..  Training Loss: 0.880..  Test Loss: 0.919.. \n",
      "Epoch: 4812/5000..  Training Loss: 0.949..  Test Loss: 0.919.. \n",
      "Epoch: 4813/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4814/5000..  Training Loss: 0.934..  Test Loss: 0.919.. \n",
      "Epoch: 4815/5000..  Training Loss: 0.956..  Test Loss: 0.919.. \n",
      "Epoch: 4816/5000..  Training Loss: 0.913..  Test Loss: 0.919.. \n",
      "Epoch: 4817/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4818/5000..  Training Loss: 0.941..  Test Loss: 0.919.. \n",
      "Epoch: 4819/5000..  Training Loss: 0.881..  Test Loss: 0.919.. \n",
      "Epoch: 4820/5000..  Training Loss: 0.947..  Test Loss: 0.919.. \n",
      "Epoch: 4821/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4822/5000..  Training Loss: 0.959..  Test Loss: 0.919.. \n",
      "Epoch: 4823/5000..  Training Loss: 0.934..  Test Loss: 0.919.. \n",
      "Epoch: 4824/5000..  Training Loss: 0.920..  Test Loss: 0.919.. \n",
      "Epoch: 4825/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4826/5000..  Training Loss: 0.932..  Test Loss: 0.919.. \n",
      "Epoch: 4827/5000..  Training Loss: 0.918..  Test Loss: 0.919.. \n",
      "Epoch: 4828/5000..  Training Loss: 0.962..  Test Loss: 0.919.. \n",
      "Epoch: 4829/5000..  Training Loss: 0.925..  Test Loss: 0.919.. \n",
      "Epoch: 4830/5000..  Training Loss: 0.922..  Test Loss: 0.919.. \n",
      "Epoch: 4831/5000..  Training Loss: 0.926..  Test Loss: 0.919.. \n",
      "Epoch: 4832/5000..  Training Loss: 0.919..  Test Loss: 0.919.. \n",
      "Epoch: 4833/5000..  Training Loss: 0.912..  Test Loss: 0.919.. \n",
      "Epoch: 4834/5000..  Training Loss: 0.909..  Test Loss: 0.919.. \n",
      "Epoch: 4835/5000..  Training Loss: 0.904..  Test Loss: 0.919.. \n",
      "Epoch: 4836/5000..  Training Loss: 0.890..  Test Loss: 0.919.. \n",
      "Epoch: 4837/5000..  Training Loss: 0.910..  Test Loss: 0.919.. \n",
      "Epoch: 4838/5000..  Training Loss: 0.928..  Test Loss: 0.919.. \n",
      "Epoch: 4839/5000..  Training Loss: 0.920..  Test Loss: 0.919.. \n",
      "Epoch: 4840/5000..  Training Loss: 0.936..  Test Loss: 0.919.. \n",
      "Epoch: 4841/5000..  Training Loss: 0.926..  Test Loss: 0.919.. \n",
      "Epoch: 4842/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4843/5000..  Training Loss: 0.910..  Test Loss: 0.919.. \n",
      "Epoch: 4844/5000..  Training Loss: 0.899..  Test Loss: 0.919.. \n",
      "Epoch: 4845/5000..  Training Loss: 0.888..  Test Loss: 0.919.. \n",
      "Epoch: 4846/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4847/5000..  Training Loss: 0.892..  Test Loss: 0.919.. \n",
      "Epoch: 4848/5000..  Training Loss: 0.921..  Test Loss: 0.919.. \n",
      "Epoch: 4849/5000..  Training Loss: 0.903..  Test Loss: 0.919.. \n",
      "Epoch: 4850/5000..  Training Loss: 0.902..  Test Loss: 0.919.. \n",
      "Epoch: 4851/5000..  Training Loss: 0.911..  Test Loss: 0.919.. \n",
      "Epoch: 4852/5000..  Training Loss: 0.927..  Test Loss: 0.919.. \n",
      "Epoch: 4853/5000..  Training Loss: 0.904..  Test Loss: 0.919.. \n",
      "Epoch: 4854/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4855/5000..  Training Loss: 0.913..  Test Loss: 0.919.. \n",
      "Epoch: 4856/5000..  Training Loss: 0.929..  Test Loss: 0.919.. \n",
      "Epoch: 4857/5000..  Training Loss: 0.910..  Test Loss: 0.919.. \n",
      "Epoch: 4858/5000..  Training Loss: 0.953..  Test Loss: 0.919.. \n",
      "Epoch: 4859/5000..  Training Loss: 0.943..  Test Loss: 0.919.. \n",
      "Epoch: 4860/5000..  Training Loss: 0.935..  Test Loss: 0.919.. \n",
      "Epoch: 4861/5000..  Training Loss: 0.892..  Test Loss: 0.919.. \n",
      "Epoch: 4862/5000..  Training Loss: 0.906..  Test Loss: 0.919.. \n",
      "Epoch: 4863/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4864/5000..  Training Loss: 0.897..  Test Loss: 0.919.. \n",
      "Epoch: 4865/5000..  Training Loss: 0.913..  Test Loss: 0.919.. \n",
      "Epoch: 4866/5000..  Training Loss: 0.915..  Test Loss: 0.919.. \n",
      "Epoch: 4867/5000..  Training Loss: 0.912..  Test Loss: 0.919.. \n",
      "Epoch: 4868/5000..  Training Loss: 0.900..  Test Loss: 0.919.. \n",
      "Epoch: 4869/5000..  Training Loss: 0.944..  Test Loss: 0.919.. \n",
      "Epoch: 4870/5000..  Training Loss: 0.924..  Test Loss: 0.919.. \n",
      "Epoch: 4871/5000..  Training Loss: 0.924..  Test Loss: 0.919.. \n",
      "Epoch: 4872/5000..  Training Loss: 0.941..  Test Loss: 0.919.. \n",
      "Epoch: 4873/5000..  Training Loss: 0.926..  Test Loss: 0.919.. \n",
      "Epoch: 4874/5000..  Training Loss: 0.907..  Test Loss: 0.919.. \n",
      "Epoch: 4875/5000..  Training Loss: 0.936..  Test Loss: 0.919.. \n",
      "Epoch: 4876/5000..  Training Loss: 0.897..  Test Loss: 0.919.. \n",
      "Epoch: 4877/5000..  Training Loss: 0.921..  Test Loss: 0.919.. \n",
      "Epoch: 4878/5000..  Training Loss: 0.907..  Test Loss: 0.919.. \n",
      "Epoch: 4879/5000..  Training Loss: 0.908..  Test Loss: 0.919.. \n",
      "Epoch: 4880/5000..  Training Loss: 0.914..  Test Loss: 0.919.. \n",
      "Epoch: 4881/5000..  Training Loss: 0.918..  Test Loss: 0.919.. \n",
      "Epoch: 4882/5000..  Training Loss: 0.924..  Test Loss: 0.919.. \n",
      "Epoch: 4883/5000..  Training Loss: 0.932..  Test Loss: 0.919.. \n",
      "Epoch: 4884/5000..  Training Loss: 0.933..  Test Loss: 0.919.. \n",
      "Epoch: 4885/5000..  Training Loss: 0.937..  Test Loss: 0.919.. \n",
      "Epoch: 4886/5000..  Training Loss: 0.877..  Test Loss: 0.919.. \n",
      "Epoch: 4887/5000..  Training Loss: 0.916..  Test Loss: 0.919.. \n",
      "Epoch: 4888/5000..  Training Loss: 0.905..  Test Loss: 0.919.. \n",
      "Epoch: 4889/5000..  Training Loss: 0.880..  Test Loss: 0.918.. \n",
      "Epoch: 4890/5000..  Training Loss: 0.880..  Test Loss: 0.918.. \n",
      "Epoch: 4891/5000..  Training Loss: 0.914..  Test Loss: 0.918.. \n",
      "Epoch: 4892/5000..  Training Loss: 0.917..  Test Loss: 0.918.. \n",
      "Epoch: 4893/5000..  Training Loss: 0.919..  Test Loss: 0.918.. \n",
      "Epoch: 4894/5000..  Training Loss: 0.945..  Test Loss: 0.918.. \n",
      "Epoch: 4895/5000..  Training Loss: 0.916..  Test Loss: 0.918.. \n",
      "Epoch: 4896/5000..  Training Loss: 0.935..  Test Loss: 0.918.. \n",
      "Epoch: 4897/5000..  Training Loss: 0.887..  Test Loss: 0.918.. \n",
      "Epoch: 4898/5000..  Training Loss: 0.896..  Test Loss: 0.918.. \n",
      "Epoch: 4899/5000..  Training Loss: 0.920..  Test Loss: 0.918.. \n",
      "Epoch: 4900/5000..  Training Loss: 0.918..  Test Loss: 0.918.. \n",
      "Epoch: 4901/5000..  Training Loss: 0.885..  Test Loss: 0.918.. \n",
      "Epoch: 4902/5000..  Training Loss: 0.929..  Test Loss: 0.918.. \n",
      "Epoch: 4903/5000..  Training Loss: 0.889..  Test Loss: 0.918.. \n",
      "Epoch: 4904/5000..  Training Loss: 0.904..  Test Loss: 0.918.. \n",
      "Epoch: 4905/5000..  Training Loss: 0.927..  Test Loss: 0.918.. \n",
      "Epoch: 4906/5000..  Training Loss: 0.892..  Test Loss: 0.918.. \n",
      "Epoch: 4907/5000..  Training Loss: 0.935..  Test Loss: 0.918.. \n",
      "Epoch: 4908/5000..  Training Loss: 0.918..  Test Loss: 0.918.. \n",
      "Epoch: 4909/5000..  Training Loss: 0.901..  Test Loss: 0.918.. \n",
      "Epoch: 4910/5000..  Training Loss: 0.874..  Test Loss: 0.918.. \n",
      "Epoch: 4911/5000..  Training Loss: 0.929..  Test Loss: 0.918.. \n",
      "Epoch: 4912/5000..  Training Loss: 0.928..  Test Loss: 0.918.. \n",
      "Epoch: 4913/5000..  Training Loss: 0.934..  Test Loss: 0.918.. \n",
      "Epoch: 4914/5000..  Training Loss: 0.951..  Test Loss: 0.918.. \n",
      "Epoch: 4915/5000..  Training Loss: 0.936..  Test Loss: 0.918.. \n",
      "Epoch: 4916/5000..  Training Loss: 0.942..  Test Loss: 0.918.. \n",
      "Epoch: 4917/5000..  Training Loss: 0.917..  Test Loss: 0.918.. \n",
      "Epoch: 4918/5000..  Training Loss: 0.920..  Test Loss: 0.918.. \n",
      "Epoch: 4919/5000..  Training Loss: 0.910..  Test Loss: 0.918.. \n",
      "Epoch: 4920/5000..  Training Loss: 0.902..  Test Loss: 0.918.. \n",
      "Epoch: 4921/5000..  Training Loss: 0.935..  Test Loss: 0.918.. \n",
      "Epoch: 4922/5000..  Training Loss: 0.930..  Test Loss: 0.918.. \n",
      "Epoch: 4923/5000..  Training Loss: 0.924..  Test Loss: 0.918.. \n",
      "Epoch: 4924/5000..  Training Loss: 0.948..  Test Loss: 0.918.. \n",
      "Epoch: 4925/5000..  Training Loss: 0.920..  Test Loss: 0.918.. \n",
      "Epoch: 4926/5000..  Training Loss: 0.908..  Test Loss: 0.918.. \n",
      "Epoch: 4927/5000..  Training Loss: 0.939..  Test Loss: 0.918.. \n",
      "Epoch: 4928/5000..  Training Loss: 0.906..  Test Loss: 0.918.. \n",
      "Epoch: 4929/5000..  Training Loss: 0.905..  Test Loss: 0.918.. \n",
      "Epoch: 4930/5000..  Training Loss: 0.927..  Test Loss: 0.918.. \n",
      "Epoch: 4931/5000..  Training Loss: 0.949..  Test Loss: 0.918.. \n",
      "Epoch: 4932/5000..  Training Loss: 0.926..  Test Loss: 0.918.. \n",
      "Epoch: 4933/5000..  Training Loss: 0.918..  Test Loss: 0.918.. \n",
      "Epoch: 4934/5000..  Training Loss: 0.923..  Test Loss: 0.918.. \n",
      "Epoch: 4935/5000..  Training Loss: 0.946..  Test Loss: 0.918.. \n",
      "Epoch: 4936/5000..  Training Loss: 0.938..  Test Loss: 0.918.. \n",
      "Epoch: 4937/5000..  Training Loss: 0.919..  Test Loss: 0.918.. \n",
      "Epoch: 4938/5000..  Training Loss: 0.905..  Test Loss: 0.918.. \n",
      "Epoch: 4939/5000..  Training Loss: 0.953..  Test Loss: 0.918.. \n",
      "Epoch: 4940/5000..  Training Loss: 0.937..  Test Loss: 0.918.. \n",
      "Epoch: 4941/5000..  Training Loss: 0.931..  Test Loss: 0.918.. \n",
      "Epoch: 4942/5000..  Training Loss: 0.919..  Test Loss: 0.918.. \n",
      "Epoch: 4943/5000..  Training Loss: 0.902..  Test Loss: 0.918.. \n",
      "Epoch: 4944/5000..  Training Loss: 0.892..  Test Loss: 0.918.. \n",
      "Epoch: 4945/5000..  Training Loss: 0.919..  Test Loss: 0.918.. \n",
      "Epoch: 4946/5000..  Training Loss: 0.896..  Test Loss: 0.918.. \n",
      "Epoch: 4947/5000..  Training Loss: 0.915..  Test Loss: 0.918.. \n",
      "Epoch: 4948/5000..  Training Loss: 0.923..  Test Loss: 0.918.. \n",
      "Epoch: 4949/5000..  Training Loss: 0.912..  Test Loss: 0.918.. \n",
      "Epoch: 4950/5000..  Training Loss: 0.940..  Test Loss: 0.918.. \n",
      "Epoch: 4951/5000..  Training Loss: 0.932..  Test Loss: 0.918.. \n",
      "Epoch: 4952/5000..  Training Loss: 0.903..  Test Loss: 0.918.. \n",
      "Epoch: 4953/5000..  Training Loss: 0.942..  Test Loss: 0.918.. \n",
      "Epoch: 4954/5000..  Training Loss: 0.919..  Test Loss: 0.918.. \n",
      "Epoch: 4955/5000..  Training Loss: 0.911..  Test Loss: 0.918.. \n",
      "Epoch: 4956/5000..  Training Loss: 0.915..  Test Loss: 0.918.. \n",
      "Epoch: 4957/5000..  Training Loss: 0.895..  Test Loss: 0.918.. \n",
      "Epoch: 4958/5000..  Training Loss: 0.922..  Test Loss: 0.918.. \n",
      "Epoch: 4959/5000..  Training Loss: 0.923..  Test Loss: 0.918.. \n",
      "Epoch: 4960/5000..  Training Loss: 0.926..  Test Loss: 0.918.. \n",
      "Epoch: 4961/5000..  Training Loss: 0.923..  Test Loss: 0.918.. \n",
      "Epoch: 4962/5000..  Training Loss: 0.916..  Test Loss: 0.918.. \n",
      "Epoch: 4963/5000..  Training Loss: 0.913..  Test Loss: 0.918.. \n",
      "Epoch: 4964/5000..  Training Loss: 0.926..  Test Loss: 0.918.. \n",
      "Epoch: 4965/5000..  Training Loss: 0.930..  Test Loss: 0.918.. \n",
      "Epoch: 4966/5000..  Training Loss: 0.912..  Test Loss: 0.918.. \n",
      "Epoch: 4967/5000..  Training Loss: 0.913..  Test Loss: 0.918.. \n",
      "Epoch: 4968/5000..  Training Loss: 0.936..  Test Loss: 0.918.. \n",
      "Epoch: 4969/5000..  Training Loss: 0.907..  Test Loss: 0.918.. \n",
      "Epoch: 4970/5000..  Training Loss: 0.934..  Test Loss: 0.918.. \n",
      "Epoch: 4971/5000..  Training Loss: 0.904..  Test Loss: 0.918.. \n",
      "Epoch: 4972/5000..  Training Loss: 0.922..  Test Loss: 0.918.. \n",
      "Epoch: 4973/5000..  Training Loss: 0.909..  Test Loss: 0.918.. \n",
      "Epoch: 4974/5000..  Training Loss: 0.919..  Test Loss: 0.918.. \n",
      "Epoch: 4975/5000..  Training Loss: 0.889..  Test Loss: 0.918.. \n",
      "Epoch: 4976/5000..  Training Loss: 0.880..  Test Loss: 0.918.. \n",
      "Epoch: 4977/5000..  Training Loss: 0.925..  Test Loss: 0.918.. \n",
      "Epoch: 4978/5000..  Training Loss: 0.909..  Test Loss: 0.918.. \n",
      "Epoch: 4979/5000..  Training Loss: 0.905..  Test Loss: 0.918.. \n",
      "Epoch: 4980/5000..  Training Loss: 0.915..  Test Loss: 0.918.. \n",
      "Epoch: 4981/5000..  Training Loss: 0.943..  Test Loss: 0.918.. \n",
      "Epoch: 4982/5000..  Training Loss: 0.912..  Test Loss: 0.918.. \n",
      "Epoch: 4983/5000..  Training Loss: 0.912..  Test Loss: 0.918.. \n",
      "Epoch: 4984/5000..  Training Loss: 0.911..  Test Loss: 0.918.. \n",
      "Epoch: 4985/5000..  Training Loss: 0.915..  Test Loss: 0.918.. \n",
      "Epoch: 4986/5000..  Training Loss: 0.896..  Test Loss: 0.918.. \n",
      "Epoch: 4987/5000..  Training Loss: 0.953..  Test Loss: 0.918.. \n",
      "Epoch: 4988/5000..  Training Loss: 0.933..  Test Loss: 0.918.. \n",
      "Epoch: 4989/5000..  Training Loss: 0.923..  Test Loss: 0.918.. \n",
      "Epoch: 4990/5000..  Training Loss: 0.917..  Test Loss: 0.918.. \n",
      "Epoch: 4991/5000..  Training Loss: 0.918..  Test Loss: 0.918.. \n",
      "Epoch: 4992/5000..  Training Loss: 0.887..  Test Loss: 0.918.. \n",
      "Epoch: 4993/5000..  Training Loss: 0.927..  Test Loss: 0.918.. \n",
      "Epoch: 4994/5000..  Training Loss: 0.903..  Test Loss: 0.918.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4995/5000..  Training Loss: 0.880..  Test Loss: 0.918.. \n",
      "Epoch: 4996/5000..  Training Loss: 0.938..  Test Loss: 0.918.. \n",
      "Epoch: 4997/5000..  Training Loss: 0.899..  Test Loss: 0.918.. \n",
      "Epoch: 4998/5000..  Training Loss: 0.903..  Test Loss: 0.918.. \n",
      "Epoch: 4999/5000..  Training Loss: 0.928..  Test Loss: 0.918.. \n",
      "Epoch: 5000/5000..  Training Loss: 0.933..  Test Loss: 0.918.. \n"
     ]
    }
   ],
   "source": [
    "model = Regressor()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.000001)\n",
    "\n",
    "epochs = 5000\n",
    "cc=0\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = (criterion(output, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cc=cc+1\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(xtest)\n",
    "            test_loss += (criterion(predictions, ytest))\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "params = {'legend.fontsize': 18,\n",
    "          'legend.handlelength': 2}\n",
    "plt.rcParams.update(params)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(loc=3, prop={'size': 30})\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"Epochs\",fontsize=20)\n",
    "plt.ylabel(\"Loss\",fontsize=20)\n",
    "plt.title(\"Training and Validation Loss\",fontsize=25)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "fig1.savefig('image3.pdf', dpi=100,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
